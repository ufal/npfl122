title: NPFL122, Lecture 6
class: title, langtech, cc-by-sa
# Policy Gradient Methods

## Milan Straka

### November 07, 2022

---
section: Policy Gradient Methods
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_{→θ} v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.svgz)

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{s∼h} v_π(s)$.

~~~
Then
$$∇_{→θ} v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ)$$
and
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, … steps. The $γ$ parameter should
be treated as a form of termination, i.e., $P(s → … → s'|π) ∝ ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π)$.

---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + π(a|s; →θ) ∇ \big(∑\nolimits_{s', r} p(s', r|s, a)(r + γv_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + γπ(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ q_π(s, a) ∇ π(a|s; →θ) + γπ(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad ∑\nolimits_{a'} \Big[ q_π(s', a') ∇ π(a'|s'; →θ) + γπ(a'|s'; →θ) \big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, recall that
$$∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∝ P(s → … → s'|π).$$

~~~
For the second part, we know that
$$∇_{→θ} J(→θ) = 𝔼_{s ∼ h} ∇_{→θ} v_π(s) ∝ 𝔼_{s ∼ h} ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ),$$
~~~
therefore using the fact that $μ(s') = 𝔼_{s ∼ h} P(s → … → s'|π)$ we get
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

---
section: REINFORCE
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(→θ) ≝ -𝔼_{s∼h} v_π(s)$. The loss gradient is then
$$∇_{→θ} -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_{→θ} -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_{→θ} \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_{→θ} π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss $-J(→θ)$ with gradient
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
where we estimate the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
# REINFORCE Algorithm Example Performance

![w=30%,v=middle](stochastic_policy_example.svgz)![w=69%,v=middle](reinforce_performance.svgz)

---
# On-policy Distribution in REINFORCE

In the proof, we assumed $γ$ is used as a form of termination in the definition
of the on-policy distribution.

~~~
However, even when discounting is used during training (to guarantee convergence
even for very long episodes), evaluation is often performed without discounting.

~~~
Consequently, the distribution $μ$ used in the REINFORCE algorithm is almost
always the unterminated (undiscounted) on-policy distribution (I am not
aware of any implementation or paper that would use it), so that we learn even
in states that are far from the beginning of an episode.

~~~
Note that this is actually true even for DQN and its variants. Therefore,
the discounting parameter $γ$ is used mostly as a variance-reduction technique.

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_{→θ} π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_{→θ} π(a | s; →θ) = b(s) ∑_a ∇_{→θ} π(a | s; →θ) = b(s) ∇_{→θ} ∑_a π(a | s; →θ) = b(s) ∇_{→θ} 1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize the
variance of the gradient estimate (in limit $γ → 1$; see L. Weaver and N. Tao,
[The Optimal Reward Baseline for Gradient-Based Reinforcement Learning](https://arxiv.org/abs/1301.2315) 
for the proof). Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called the **advantage** function
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

In REINFORCE with baseline, we train:
1. the _policy network_ using the REINFORCE algorithm, and
~~~
2. the _value network_ by minimizing the mean squared value error
   $\overline{VE}$.

![w=85%,h=center](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline Example Performance


![w=40%,h=center,mh=48%](stochastic_policy_example.svgz)

![w=48%](reinforce_performance.svgz)![w=52%](reinforce_with_baseline_comparison.svgz)

---
section: OP-REINFORCE
# Operator View of Policy Gradient Methods

In the middle of 2020, _Dibya Ghosh et al._ introduced the operator view of
policy gradient methods in their paper
[An operator view of policy gradient methods](https://arxiv.org/abs/2006.11266).

~~~
## Trajectory Formulation

Let $τ = (S_0, A_0, S_1, A_1, …)$ be a specific trajectory with return
$G(τ) = ∑_{k=0}^∞ γ^k R_{k+1}(τ)$.
~~~
The probability of $τ$ under a policy $π$ is $π(τ) = h(S_0) ∏_i π(A_i | S_i)
p(S_{i+1} | S_i, A_i)$.

~~~
Our goal is then to find
$$→θ^* = \argmax_{→θ} 𝔼_{τ ∼ π_{→θ}} \big[G(τ)\big] = \argmax_{→θ} ∫_τ π(τ) G(τ) \d τ,$$
~~~
and the REINFORCE algorithm at each step sets the weights $→θ_{t+1}$ to
~~~
$$→θ_t + α𝔼_{τ ∼ π_{→θ_t}}\bigg[G(τ) \frac{∂\log π_{→θ}(τ)}{∂→θ}\Big|_{→θ=→θ_t} \bigg]
= →θ_t + α∫_τ π_{→θ_t}(τ) G(τ) \frac{∂\log π_{→θ}(τ)}{∂→θ}\Big|_{→θ=→θ_t} \d τ.$$

---
# Trajector Formulation of OP-REINFORCE

In the operator view, policy improvement is achieved by a successive application
of a **policy improvement operator** $𝓘$ and a **projection operator** $𝓟$. For
tabular methods, the projection operator is identity, but it is needed for
functional approximation methods.

~~~
The operator version of REINFORCE is then the iterative application of $𝓟∘𝓘$
with
$$(𝓘 π)(τ) ∝ G(τ) π(τ),$$
~~~
$$𝓟 ν = \argmin_{→θ} D_\textrm{KL}\big(ν \| π_{→θ}\big).$$

~~~
As formulated, the operator version of REINFORCE computes the projection
perfectly in each step, while the REINFORCE performs just one step
of gradient descent in the direction of $𝓟$. However, it is easy to show that
the fixed points of both algorithms are the same.

---
# Trajector Formulation of OP-REINFORCE

The proposition is actually not difficult to prove, we just need to expand the
definitions.

~~~
Denoting $ν$ the distribution over trajectories such that
$ν(τ) ∝ G(τ) π(τ)$, we get

~~~
$$D_\textrm{KL}\big(ν \| π_{→θ}) = ∫_τ ν(τ) \log \frac{ν(τ)}{π_{→θ}(τ)} \d τ.$$

~~~
Therefore, the gradient is
$$\frac{∂ D_\textrm{KL}\big(ν \| π_{→θ})}{∂ →θ}
 = - ∫_τ ν(τ) ∇_{→θ} \log π_{→θ}(τ) \d τ
 ∝ - ∫_τ π_{→θ}(τ) G(τ) ∇_{→θ} \log π_{→θ}(τ) \d τ.$$

---
# State-Action Formulation of OP-REINFORCE

We can formulate the operator view also employing the action-value function $q$
and the on-policy distribution $μ_π$; however, the policy improvement operator
needs to return not just a policy, but a joint distribution over the states
and actions.

~~~
The REINFORCE algorithm can be seen as performing one gradient step to minimize
the composition $𝓟∘𝓘$, where
$$(𝓘 π)(s, a) ∝ μ_π(s) q_π(s, a) π(a | s),$$
~~~
$$𝓟 ν = \argmin_{→θ} 𝔼_{s∼ν(s)} \Big[D_\textrm{KL}\big(ν(⋅|s) \| π_{→θ}(⋅|s)\big)\Big].$$

---
class: dbend
# State-Action Formulation of OP-REINFORCE

Just for completeness, we can explicitly express the joint distribution
$(𝓘 π)(s, a)$ as a product of $(𝓘 π)(s) ⋅ (𝓘 π)(a|s)$, where
- the distribution over the states is
  $$(𝓘 π)(s) = \frac{μ_π(s) v_π(s)}{∑_{s'} μ_π(s') v_π(s')} = \frac{μ_π(s) v_π(s)}{𝔼_{s'∼μ_π} [v_π(s')]},$$

~~~
- the conditional distribution over the actions is
  $$(𝓘 π)(a|s)
    = \frac{q_π(s, a) π(a|s)}{∑_{a'} q_π(s, a') π(a'|s)}
    = \frac{q_π(s, a) π(a|s)}{𝔼_{a'∼π(s)}[q_π(s, a')]}
    = \frac{q_π(s, a) π(a|s)}{v_π(s)}.$$

---
section: Actor-Critic
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called the **actor-critic** methods.

~~~
The idea is straightforward – similarly to the REINFORCE with baseline, we train
the policy network together with the value network. However, instead of
estimating the episode return using the whole episode rewards, we use $n$-step
return TD estimate in both the policy gradient and the mean squared
value error $\overline{VE}$.

---
# Actor-Critic

![w=85%,h=center](actor_critic.svgz)

---
section: A3C
# Asynchronous Methods for Deep RL

The A3C was introduced in a 2016 paper from Volodymyr Mnih et al. (the same group as DQN)
[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783).

~~~
The authors propose an asynchronous framework, where multiple workers share one
neural network, each training using either an off-line or on-line RL algorithm.

~~~
They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C
(an _asynchronous advantage actor-critic_ method). For A3C, they compare
a version with and without LSTM.

~~~
The authors also introduce _entropy regularization term_ $-β H(π(s; →θ))$ to the
loss to support exploration and discourage premature convergence (they use
$β=0.01$).

~~~
- The entropy regularization has since become the standard way of encouraging
  exploration with a policy network.

---
# Asynchronous Methods for Deep RL

![w=45%,h=center](asynchronous_q_learning.svgz)

---
# Asynchronous Methods for Deep RL

![w=69%,h=center](asynchronous_q_learning_nstep.svgz)

---
# Asynchronous Methods for Deep RL

![w=86.5%,h=center](a3c.svgz)

---
# Asynchronous Methods for Deep RL

All methods performed updates every 5 actions
($t_\textrm{max}=I_\textrm{AsyncUpdate}=5$), updating the target
network each $40\,000$ frames.

~~~
The Atari inputs were processed as in DQN, using also action repeat 4.

~~~
The network architecture is: 16 filters $8×8$ stride 4, 32
filters $4×4$ stride 2, followed by a fully connected layer with 256 units.
All hidden layers apply a ReLU nonlinearity. Values and/or action values
were then generated from the (same) last hidden layer.

~~~
The LSTM methods utilized a 256-unit LSTM cell after the dense hidden layer.

~~~
All experiments used a discount factor of $γ=0.99$ and used RMSProp with
momentum decay factor of $0.99$.

---
# Asynchronous Methods for Deep RL

![w=100%](a3c_performance.svgz)
![w=85%,mw=50%,h=center](a3c_performance_table.svgz)![w=50%](a3c_speedup.svgz)

---
# Asynchronous Methods for Deep RL

![w=92%,h=center](a3c_data_efficiency_episodes.svgz)

---
# Asynchronous Methods for Deep RL

![w=92%,h=center](a3c_data_efficiency_time.svgz)

---
# Asynchronous Methods for Deep RL

![w=100%,v=middle](a3c_learning_rates.svgz)
