title: NPFL122, Lecture 6
class: title, langtech, cc-by-sa
# Policy Gradient Methods

## Milan Straka

### November 09, 2020

---
section: Policy Gradient Methods
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.svgz)

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{s∼h} v_π(s)$.

~~~
Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, … steps. The $γ$ parameter should
be treated as a form of termination, i.e., $P(s → … → s'|π) ∝ ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π)$.

---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + γv_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + γπ(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + γπ(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + γπ(a'|s'; →θ) \big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, recall that
$$∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∝ P(s → … → s'|π).$$

~~~
For the second part, we know that
$$∇_→θ J(→θ) = 𝔼_{s ∼ h} ∇_→θ v_π(s) ∝ 𝔼_{s ∼ h} ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ),$$
~~~
therefore using the fact that $μ(s') = 𝔼_{s ∼ h} P(s → … → s'|π)$ we get
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

---
section: REINFORCE
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(→θ) ≝ -𝔼_{s∼h} v_π(s)$. The loss gradient is then
$$∇_→θ -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_→θ -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_→θ \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
# On-policy Distribution in REINFORCE

In the proof, we assumed $γ$ is used as a form of termination in the definition
of the on-policy distribution.

~~~
However, even when discounting is used during training (to guarantee convergence
even for very long episodes), evaluation is often performed without discounting.

~~~
Consequently, the distribution $μ$ used in the REINFORCE algorithm is almost
always the unterminated (undiscounted) on-policy distribution (I am not
aware of any implementation or paper that would use it), so that we learn even
in states that are far from the beginning of an episode.

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇_→θ ∑_a π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called an **advantage** function
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.svgz)

---
section: Actor-Critic
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called _actor-critic_ methods.

~~~
The idea is straightforward – instead of estimating the episode return using the
whole episode rewards, we can use $n$-step temporal difference estimation.

---
# Actor-Critic

![w=85%,h=center](actor_critic.svgz)

---
section: A3C
# Asynchronous Methods for Deep RL

A 2015 paper from Volodymyr Mnih et al., the same group as DQN.

~~~
The authors propose an asynchronous framework, where multiple workers share one
neural network, each training using either an off-line or on-line RL algorithm.

~~~
They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C
(an _asynchronous advantage actor-critic_ method). For A3C, they compare
a version with and without LSTM.

~~~
The authors also introduce _entropy regularization term_ $-β H(π(s; →θ))$ to the
loss to support exploration and discourage premature convergence (they use
$β=0.01$).

---
# Asynchronous Methods for Deep RL

![w=45%,h=center](asynchronous_q_learning.svgz)

---
# Asynchronous Methods for Deep RL

![w=68%,h=center](asynchronous_q_learning_nstep.svgz)

---
# Asynchronous Methods for Deep RL

![w=68%,h=center](a3c.svgz)

---
# Asynchronous Methods for Deep RL

All methods performed updates every 5 actions
($t_\textrm{max}=I_\textrm{AsyncUpdate}=5$), updating the target
network each $40\,000$ frames.

~~~
The Atari inputs were processed as in DQN, using also action repeat 4.

~~~
The network architecture is: 16 filters $8×8$ stride 4, 32
filters $4×4$ stride 2, followed by a fully connected layer with 256 units.
All hidden layers apply a ReLU non-linearity. Values and/or action values
were then generated from the (same) last hidden layer.

~~~
The LSTM methods utilized a 256-unit LSTM cell after the dense hidden layer.

~~~
All experiments used a discount factor of $γ=0.99$ and used RMSProp with
momentum decay factor of $0.99$.

---
# Asynchronous Methods for Deep RL

![w=100%](a3c_performance.svgz)
![w=85%,mw=50%,h=center](a3c_performance_table.svgz)![w=50%](a3c_speedup.svgz)

---
# Asynchronous Methods for Deep RL

![w=85%,h=center](a3c_data_efficiency_episodes.svgz)

---
# Asynchronous Methods for Deep RL

![w=85%,h=center](a3c_data_efficiency_time.svgz)

---
# Asynchronous Methods for Deep RL

![w=100%,v=middle](a3c_learning_rates.svgz)
