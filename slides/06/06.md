title: NPFL122, Lecture 6
class: title, langtech, cc-by-nc-sa
# Rainbow,<br>Policy Gradient Methods

## Milan Straka

### November 19, 2018

---
section: Refresh
# Function Approximation

We will approximate value function $v$ and/or state-value function $q$, choosing
from a family of functions parametrized by a weight vector $â†’wâˆˆâ„^d$.

We denote the approximations as
$$\begin{gathered}
  \hat v(s, â†’w),\\
  \hat q(s, a, â†’w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(â†’w) â‰ âˆ‘_{sâˆˆğ“¢} Î¼(s) \left[v_Ï€(s) - \hat v(s, â†’w)\right]^2,$$
where the state distribution $Î¼(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $â†’w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \frac{1}{2} Î± âˆ‡ \left[v_Ï€(S_t) - \hat v(S_t, â†’w_t)\right]^2\\
           &â† â†’w_t - Î±\left[v_Ï€(S_t) - \hat v(S_t, â†’w_t)\right] âˆ‡ \hat v(S_t, â†’w_t).\\
\end{aligned}$$

As usual, the $v_Ï€(S_t)$ is estimated by a suitable sample. For example in Monte
Carlo methods, we use episodic return $G_t$, and in temporal difference methods,
we employ bootstrapping and use $R_{t+1} + Î³\hat v(S_{t+1}, â†’w).$

---
section: DQN
# Deep Q Network

Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

Training can be extremely brittle (and can even diverge as shown earlier).

![w=65%,h=center](../05/dqn_architecture.pdf)

---
# Deep Q Networks

- Preprocessing: $210Ã—160$ 128-color images are converted to grayscale and
  then resized to $84Ã—84$.
~~~
- Frame skipping technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- Input to the network are last $4$ frames (considering only the frames kept by
  frame skipping), i.e., an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8Ã—8$ with stride 4 and ReLU,
  - 64 filters of size $4Ã—4$ with stride 2 and ReLU,
  - 64 filters of size $3Ã—3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ğ“› â‰ ğ”¼_{(s, a, r, s')âˆ¼\mathit{data}}\left[(r + Î³ \max_{a'} Q(s', a'; \bar Î¸) - Q(s, a; Î¸))^2\right].$$
~~~
- An $Îµ$-greedy behavior policy is utilized.

~~~
Important improvements:
~~~
- experience replay: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly;
~~~
- separate target network $\bar Î¸$: to prevent instabilities, a separate target
  network is used to estimate state-value function. The weights are not trained,
  but copied from the trained network once in a while;
~~~
- reward clipping of $(r + Î³ \max_{a'} Q(s', a'; \bar Î¸) - Q(s, a; Î¸))$ to $[-1, 1]$.

---
class: tablefull
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
| replay buffer size | 1M |
| target network update frequency | 10k |
| discount factor | 0.99 |
| training frames | 50M |
| RMSProp learning rate and momentum | 0.00025, 0.95 |
| initial $Îµ$, final $Îµ$ and frame of final $Îµ$ | 1.0, 0.1, 1M |
| replay start size | 50k |
| no-op max | 30 |

---
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 7 of them into a single architecture they call _Rainbow_.

~~~
![w=38%,h=center](rainbow_results.pdf)

---
section: DDQN
# Rainbow DQN Extensions

## Double Q-learning

Similarly to double Q-learning, instead of
$$r + Î³ \max_{a'} Q(s', a'; \bar Î¸) - Q(s, a; Î¸),$$
we minimize
$$r + Î³ Q(s', \argmax_{a'}Q(s', a'; Î¸); \bar Î¸) - Q(s, a; Î¸).$$

~~~
![w=30%,h=center](ddqn_errors.pdf)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=100%,h=center](ddqn_errors_analysis.pdf)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=60%,h=center](ddqn_analysis.pdf)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=40%,h=center,mh=40%,v=middle](ddqn_results_5min.pdf)

![w=55%,h=center,mh=40%,v=middle](ddqn_results_30min.pdf)

---
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t âˆ \Big|r + Î³ \max_{a'} Q(s', a'; \bar Î¸) - Q(s, a; Î¸)\Big|^Ï‰,$$
where $Ï‰$ controls the shape of the distribution (which is uniform for $Ï‰=0$
and corresponds to TD error for $Ï‰=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

---
section: PriRep
# Rainbow DQN Extensions

## Prioritized Replay

Because we now sample transitions according to $p_t$ instead of uniformly,
on-policy distribution and sampling distribution differ. To compensate, we
therefore utilize importance sampling with ratio
$$Ï_t = \left( \frac{1/N}{p_t} \right) ^Î².$$

~~~
The authors utilize in fact â€œfor stability reasonsâ€
$$Ï_t / \max_i Ï_i.$$

---
# Rainbow DQN Extensions

## Prioritized Replay

![w=75%,h=center](prioritized_dqn_algorithm.pdf)

---
section: Duelling
# Rainbow DQN Extensions

## Duelling Networks

Instead of computing directly $Q(s, a; Î¸)$, we compose it from the following quantities:
- value function for a given state $s$,
- advantage function computing an _advantage_ of using action $a$ in state $s$.

$$Q(s, a) â‰ V(f(s; Î¶); Î·) + A(f(s; Î¶), a; Ïˆ) + \frac{\sum_{a' âˆˆ ğ“} A(f(s; Î¶), a'; Ïˆ)}{|ğ“|}$$

![w=25%,h=center](dqn_dueling_architecture.pdf)

---
# Rainbow DQN Extensions

## Duelling Networks

![w=100%,h=center](dqn_dueling_corridor.pdf)

---
# Rainbow DQN Extensions

## Duelling Networks

![w=32%,h=center](dqn_dueling_visualization.pdf)

---
# Rainbow DQN Extensions

## Duelling Networks

![w=70%,h=center,mh=80%,v=middle](dqn_dueling_results.pdf)

---
# Rainbow DQN Extensions
## Multi-step Learning

Instead of Q-learning, we use $n$-step variant Q-learning (to be exact, we use
$n$-step Expected Sarsa) to maximize
$$âˆ‘_{i=1}^n Î³^{i-1} r_i + Î³^n \max_{a'} Q(s', a'; \bar Î¸) - Q(s, a; Î¸),$$

~~~
This changes the off-policy algorithm to on-policy, but it is not discussed in
any way by the authors.

---
section: NoisyNets
# Rainbow DQN Extensions

## Noisy Nets

Noisy Nets are neural networks whose weights and biases are perturbed by
a parametric function of a noise.

~~~
The parameters $â†’Î¸$ are represented as
$$â†’Î¸ â‰ â†’Î¼ + â†’Ïƒ âŠ™ â†’Îµ,$$
where $â†’Îµ$ is zero-mean noise with fixed statistics. We therefore learn the
parameters $â†’Î¶ â‰ (â†’Î¼, â†’Ïƒ)$.

~~~
Therefore, a fully connected layer
$$â†’y = â†’w â†’x + â†’b$$
is represented in the following way in Noisy Nets:
$$â†’y = (â†’Î¼_w + â†’Ïƒ_w âŠ™ â†’Îµ_w) â†’x + (â†’Î¼_b + â†’Ïƒ_b âŠ™ â†’Îµ_b).$$

---
# Rainbow DQN Extensions

## Noisy Nets

The noise $Îµ$ can be for example independent Gaussian noise. However, for
performance reasons, factorized Gaussian noise is used to generate a matrix of
noise. If $Îµ_{i, j}$ is noise corresponding to a layer with $i$ inputs and $j$
outputs, we generate independent noise $Îµ_i$ for input neurons, independent
noise $Îµ_j$ for output neurons, and set
$$Îµ_{i,j} = f(Îµ_i) f(Îµ_j)$$
for $f(x) = \operatorname{sign}(x) \sqrt{|x|}$.

~~~
The authors generate noise samples for every batch, sharing the noise for all
batch instances.

~~~
### Deep Q Networks
When training a DQN, $Îµ$-greedy is no longer used and all policies are greedy,
and all fully connected layers are parametrized as noisy nets.

---
# Rainbow DQN Extensions

## Noisy Nets

![w=50%,h=center](dqn_noisynets_results.pdf)

![w=65%,h=center](dqn_noisynets_curves.pdf)

---
# Rainbow DQN Extensions

## Noisy Nets

![w=100%](dqn_noisynets_noise_study.pdf)

---
section: DistRL
# Rainbow DQN Extensions

## Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate distribution of
expected returns $Z(s, a)$.

These distributions satisfy a distributional Bellman equation:
$$Z(s, a) = R(s, a) + Î³ Z(s', a').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric (Wasserstein metric).

---
# Rainbow DQN Extensions

## Distributional RL

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N âˆˆ â„•$ and by $V_\textrm{MIN}, V_\textrm{MAX} âˆˆ â„$.
Support of the distribution are atoms
$$\{z_i â‰ V_\textrm{MIN} + i Î”z : 0 â‰¤ i < N\}\textrm{~~~for~}Î”z â‰ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_â†’Î¸(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a)}}{âˆ‘_j e^{f_j(s, a)}}\right\}.$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=30%,f=right](dqn_distributional_operator.pdf)

After the Bellman update, the support of the distribution $R(s, a) + Î³Z(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Î¦\big(R(s, a) + Î³Z(s', a')\big)_i â‰
  âˆ‘_{j=1}^N \left[ 1 - \frac{\left|[r + Î³z_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Î”z} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\big(Î¦(R + \max_{a'} Z(s', a') || Z(s, a)\big).$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=50%,h=center](dqn_distributional_algorithm.pdf)


---
# Rainbow DQN Extensions

## Distributional RL

![w=40%,h=center](dqn_distributional_results.pdf)

![w=40%,h=center](dqn_distributional_example_distribution.pdf)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_example_distributions.pdf)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_atoms_ablation.pdf)

---
section: Rainbow
# Rainbow Architecture

Rainbow combines all described DQN extensions. Instead of $1$-step updates,
$n$-step updates are utilized, and KL divergence of the current and target
return distribution is minimized:
$$D_\textrm{KL}\big(Î¦(G_{t:t+n} + Î³^n \max_{a'} Z(s', a')) || Z(s, a)\big).$$

~~~
The prioritized replay chooses transitions according to the probability
$$p_t âˆ \Big(D_\textrm{KL}\big(Î¦(G_{t:t+n} + Î³^n \max_{a'} Z(s', a')) || Z(s, a)\big)\Big)^Ï‰.$$

~~~
Network utilizes duelling architecture feeding the shared representation $f(s; Î¶)$
into value computation $V(f(s; Î¶); Î·)$ and advantage computation $A_i(f(s; Î¶), a; Ïˆ)$ for atom $z_i$,
and the final probability of atom $z_i$ in state $s$ and action $a$ is computed as
$$p_i(s, a) â‰
  \frac{e^{V(f(s; Î¶); Î·) + A_i(f(s; Î¶), a; Ïˆ) - \sum_{a' âˆˆ ğ“} A_i(f(s; Î¶), a'; Ïˆ)/|ğ“|}}
  {\sum_j e^{V(f(s; Î¶); Î·) + A_j(f(s; Î¶), a; Ïˆ) - \sum_{a' âˆˆ ğ“} A_j(f(s; Î¶), a'; Ïˆ)/|ğ“|}}.$$

---
# Rainbow Hyperparameters

![w=70%,h=center](rainbow_hyperparameters.pdf)

---
# Rainbow Results

![w=50%](rainbow_results.pdf)![w=50%](rainbow_table.pdf)

---
# Rainbow Results

![w=50%](rainbow_results.pdf)![w=50%](rainbow_results_ablations.pdf)

---
# Rainbow Ablations

![w=90%,h=center](rainbow_ablations.pdf)

---
# Rainbow Ablations

![w=90%,h=center](rainbow_ablations_per_game.pdf)

---
section: Policy Gradient
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$Ï€(a | s; â†’Î¸).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $Ï€$ instead of just $Îµ$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_Ï€(s)$ and to
that account we need to compute its _gradient_ $âˆ‡_â†’Î¸ v_Ï€(s)$.

---
# Policy Gradient Theorem

Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.
Let also $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$.

~~~
Then
$$âˆ‡_â†’Î¸ v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ s') âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸)$$
and
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

---
# Proof of Policy Gradient Theorem

$\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \Big[ âˆ‘\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ \big(âˆ‘\nolimits_{s'} p(s'|s, a)(r + v_Ï€(s'))\big) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \big(âˆ‘\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\big) \Big]$

~~~
_We now expand $v_Ï€(s')$._

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \Big(âˆ‘\nolimits_{s'} p(s'|s, a)\Big(\\
          \qquad\qquad\qquad\qquad âˆ‘\nolimits_{a'} \Big[ âˆ‡ Ï€(a'|s'; â†’Î¸) q_Ï€(s', a') + Ï€(a'|s'; â†’Î¸) \Big(âˆ‘\nolimits_{s''} p(s''|s', a')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_Ï€(s')$, we obtain the following:_

$\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} P(s â†’ s') âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸).$
---
# Policy Gradient Methods

In addition to discarding $Îµ$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$. To compute the gradient
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$
REINFORCE algorithm estimates the $q_Ï€(s, a)$ by a single sample.

~~~
![w=75%,h=center](reinforce.pdf)
