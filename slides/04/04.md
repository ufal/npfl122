title: NPFL122, Lecture 4
class: title, langtech, cc-by-nc-sa
# Function Approximation,<br>Deep Q Network

## Milan Straka

### October 26, 2020

---
# Where Are We

- Until now, we have solved the tasks by explicitly calculating expected return,
either as $v(s)$ or as $q(s, a)$.
~~~

  - Finite number of states and actions.
~~~
  - We do not share information between different states or actions.
~~~
  - We use $q(s, a)$ if we do not have the environment model
    (a _model-free_ method); if we do, it is usually better to
    estimate $v(s)$ and choose actions as $\argmax\nolimits_a ùîº[R + v(s')]$.
~~~
- The methods we know differ in several aspects:

  - Whether they compute return by simulating whole episode (Monte Carlo
    methods), or by using bootstrapping (temporal difference, i.e.,
    $G_t ‚âà R_t + v(S_t)$, possibly $n$-step).
~~~
    - TD methods more noisy and unstable, but can learn immediately and
      explicitly assume Markovian property of value function.
~~~
  - Whether they estimate the value function of the same policy they use to
    generate episodes (on-policy) or not (off-policy).
~~~
    - The off-policy methods are more noisy and unstable, but more flexible.

---
section: Function Approximation
# Function Approximation

We will approximate value function $v$ and/or state-value function $q$,
selecting it from a family of functions parametrized by a weight vector $‚Üíw ‚àà ‚Ñù^d$.

~~~
We will denote the approximations as
$$\begin{gathered}
  vÃÇ(s; ‚Üíw),\\
  qÃÇ(s, a; ‚Üíw).
\end{gathered}$$

~~~
Weights are usually shared among states. Therefore, we need to define state
distribution $Œº(s)$ to allow an objective for finding the best function approximation
(if we give preference to some states, improving their estimates might worsen
estimates in other states).

~~~
The state distribution $Œº(s)$ gives rise to a natural objective function called
_Mean Squared Value Error_, denoted $\overline{VE}$:
$$\overline{VE}(‚Üíw) ‚âù ‚àë_{s‚ààùì¢} Œº(s) \left[v_œÄ(s) - vÃÇ(s; ‚Üíw)\right]^2.$$

---
# Function Approximation

For on-policy algorithms, $Œº(s)$ is often the on-policy distribution (fraction of
time spent in $s$).

~~~
- For **continuing tasks**, $Œº$ is the stationary distribution under $œÄ$, if it
  exists (i.e., a distribution which does not change after one step):
  $$Œº(s) = ‚àë\nolimits_{s'} Œº(s') ‚àë\nolimits_a œÄ(a|s') p(s|s', a).$$

~~~
- For **episodic tasks**, let $h(s)$ be a probability that an episodes starts in state $s$,
  and let $Œ∑(s)$ denote the number of time steps spent, on average, in state $s$
  in a single episode:
  $$Œ∑(s) = h(s) + ‚àë\nolimits_{s'}Œ∑(s')‚àë\nolimits_a œÄ(a|s') p(s|s', a).$$

~~~
  The on-policy distribution is then obtained by normalizing: $Œº(s) ‚âù \frac{Œ∑(s)}{‚àë_{s'} Œ∑(s')}.$

~~~
  If there is discounting ($Œ≥<1$), it should be treated as a form of
  termination, by including a factor $Œ≥$ to the second term of the $Œ∑(s)$ equation.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $‚Üíw$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  ‚Üíw_{t+1} &‚Üê ‚Üíw_t - \tfrac{1}{2} Œ± ‚àá_{‚Üíw_t} \left[v_œÄ(S_t) - vÃÇ(S_t; ‚Üíw_t)\right]^2\\
           &‚Üê ‚Üíw_t + Œ±\left[v_œÄ(S_t) - vÃÇ(S_t; ‚Üíw_t)\right] ‚àá_{‚Üíw_t} vÃÇ(S_t; ‚Üíw_t).\\
\end{aligned}$$

~~~
As usual, the $v_œÄ(S_t)$ is estimated by a suitable sample. In Monte Carlo
methods, we use episodic return $G_t$, and in temporal difference methods, we
employ bootstrapping and use $R_{t+1} + Œ≥vÃÇ(S_{t+1}; ‚Üíw).$

---
# Monte Carlo Gradient Policy Evaluation
![w=100%,v=middle](grad_mc_estimation.svgz)

---
# Linear Methods

A simple special case of function approximation are linear methods, where
$$vÃÇ\big(‚Üíx(s); ‚Üíw\big) ‚âù ‚Üíx(s)^T ‚Üíw = ‚àëx(s)_i w_i.$$

~~~
The $‚Üíx(s)$ is a representation of state $s$, which is a vector of the same size
as $‚Üíw$. It is sometimes called a _feature vector_.

~~~
The SGD update rule then becomes
$$‚Üíw_{t+1} ‚Üê ‚Üíw_t + Œ±\left[v_œÄ(S_t) - vÃÇ(‚Üíx(S_t); ‚Üíw_t)\right] ‚Üíx(S_t).$$

~~~
This rule is the same as in tabular methods, if $‚Üíx(s)$ is one-hot
representation of state $s$.

---
# State Aggregation

Simple way of generating a feature vector is **state aggregation**, where several
neighboring states are grouped together.

~~~
For example, consider a 1000-state random walk, where transitions lead uniformly
randomly to any of 100 neighboring states on the left or on the right. Using
state aggregation, we can partition the 1000 states into 10 groups of 100
states. Monte Carlo policy evaluation then computes the following:

![w=60%,h=center](grad_mc_estimation_example.svgz)

---
# Feature Construction for Linear Methods

Many methods developed in the past:
~~~
- polynomials,

~~~
- Fourier bases,
~~~
- radial basis functions,
~~~
- tile coding, ‚Ä¶

~~~
But of course, nowadays we use deep neural networks, which construct a suitable
feature vector automatically as a latent variable (the last hidden layer).

---
section: Tile Coding
# Tile Coding

![w=100%,mh=90%,v=middle](tile_coding.svgz)

~~~
If $t$ overlapping tiles are used, the learning rate is usually normalized as $Œ±/t$.

---
# Tile Coding

For example, on the 1000-state random walk example, the performance of tile
coding surpasses state aggregation:

![w=60%,h=center](tile_coding_performance.svgz)

---
# Asymmetrical Tile Coding

In higher dimensions, the tiles should have asymmetrical offsets, with
a sequence of $(1, 3, 5, ‚Ä¶, 2d-1)$ proposed as a good choice.

![w=50%,h=center](tile_coding_asymmetrical.svgz)

---
section: Semi-Gradient TD
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again use bootstrapping to estimate
$v_œÄ(S_t)$ as $R_{t+1} + Œ≥vÃÇ(S_{t+1}; ‚Üíw).$

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
section: Semi-Gradient TD
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $vÃÇ(S_{t+1}; ‚Üíw)$:
$$‚Üíw ‚Üê ‚Üíw + Œ±\big[R_{t+1} + Œ≥vÃÇ(S_{t+1}; ‚Üíw) - vÃÇ(S_t; ‚Üíw)\big] ‚àá_{‚Üíw_t} vÃÇ(S_t; ‚Üíw).$$

~~~
In other words, the above rule is in fact not a SGD update, because there does
not exist a function $J(‚Üíw)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $vÃÇ(S_t; ‚Üíw) = ‚àë_i x(S_t)_i w_i$ and assume such a $J(‚Üíw)$ exists.
Then
$$\tfrac{‚àÇ}{‚àÇw_i}J(‚Üíw) = \big[R_{t+1} + Œ≥vÃÇ(S_{t+1}; ‚Üíw) - vÃÇ(S_t; ‚Üíw)\big] x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction.
$$\begin{aligned}
  \tfrac{‚àÇ}{‚àÇw_i}\tfrac{‚àÇ}{‚àÇw_j}J(‚Üíw) &= \big[Œ≥x(S_{t+1})_i - x(S_t)_i\big] x(S_t)_j = Œ≥x(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{‚àÇ}{‚àÇw_j}\tfrac{‚àÇ}{‚àÇw_i}J(‚Üíw) &= \big[Œ≥x(S_{t+1})_j - x(S_t)_j\big] x(S_t)_i = Œ≥x(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $‚Üíw_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(‚Üíw_\mathrm{TD}) ‚â§ \frac{1}{1-Œ≥} \min_‚Üíw \overline{VE}(‚Üíw).$$

~~~
However, when $Œ≥$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=60%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

On the left, the results of one-step TD(0) algorithm is presented.
The effect of increasing $n$ in an $n$-step variant is displayed on the right.

![w=100%](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($Œª$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policy Divergence
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, TD error will be also 10 (or nearly 10 if $Œ≥<1$).
~~~
- If for example $Œ±=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be realized for instance by the following _Baird's
counterexample_:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $Œ±=0.01$.

---
# Off-policy Divergence With Function Approximation

However, for off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update, where we compute expectation over all following
states and actions, the weights diverge to $+‚àû$ (while using on-policy
distribution converges fine).

$$‚Üíw ‚Üê ‚Üíw + \frac{Œ±}{|ùì¢|} ‚àë_s \Big(ùîº_œÄ \big[R_{t+1} + Œ≥vÃÇ(S_{t+1}; ‚Üíw) | S_t=s\big] - vÃÇ(s; ‚Üíw)\Big) ‚àávÃÇ(s; ‚Üíw)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

- bootstrapping;

- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
In Feb 2015 accepted in Nature, as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210√ó160$ 128-color images are converted to grayscale and
  then resized to $84√ó84$.
~~~
- Frame skipping technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- Input to the network are last $4$ frames (considering only the frames kept by
  frame skipping), i.e., an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8√ó8$ with stride 4 and ReLU,
  - 64 filters of size $4√ó4$ with stride 2 and ReLU,
  - 64 filters of size $3√ó3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ùìõ ‚âù ùîº_{(s, a, r, s')‚àº\mathrm{data}}\left[(r + \left[s'\textrm{~not~terminal}\right] ‚ãÖ Œ≥ \max\nolimits_{a'} Q(s', a'; ‚ÜíŒ∏ÃÑ) - Q(s, a; ‚ÜíŒ∏))^2\right].$$
~~~
- An $Œµ$-greedy behavior policy is utilized (starts at $Œµ=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- experience replay: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate target network $‚ÜíŒ∏ÃÑ$: to prevent instabilities, a separate target
  network is used to estimate state-value function. The weights are not trained,
  but copied from the trained network once in a while;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as end of episode.
~~~
  - furthermore, $(r + \left[s'\textrm{~not~terminal}\right] ‚ãÖ Œ≥ \max_{a'} Q(s', a'; ‚ÜíŒ∏ÃÑ) - Q(s, a; ‚ÜíŒ∏))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablefull
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
| replay buffer size | 1M |
| target network update frequency | 10k |
| discount factor | 0.99 |
| training frames | 50M |
| RMSProp learning rate and momentum | 0.00025, 0.95 |
| initial $Œµ$, final $Œµ$ (linear decay) and frame of final $Œµ$ | 1.0, 0.1, 1M |
| replay start size | 50k |
| no-op max | 30 |
