title: NPFL122, Lecture 4
class: title, langtech, cc-by-nc-sa
# N-step Methods, Eligibility Traces, Function Approximation

## Milan Straka

### November 05, 2018

---
section: Refresh
# Sarsa and Q-learning

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\right].$$

~~~
Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} +  Î³ \max_a q(S_{t+1}, a) -q(S_t, A_t)\right].$$

---
# Refresh â€“ Q-learning versus Sarsa

![w=70%,h=center](../03/cliff_walking.pdf)

~~~ ~
# Refresh â€“ Q-learning versus Sarsa
![w=40%,h=center](../03/cliff_walking.pdf)
![w=45%,h=center](../03/cliff_walking_learning.pdf)

---
# Refresh â€“ Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, â€¦, S_T$,
the probability of this episode under a policy $Ï€$ is
$$âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
behaviour policies is
$$Ï_t â‰ \frac{âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k)}{âˆ_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = âˆ_{k=t}^{T-1} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_Ï€(S_t) = ğ”¼_b[Ï_t G_t].$$

---
# Refresh â€“ Off-policy Monte Carlo Prediction

Let $ğ“£(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{|ğ“£(s)|}.$$

~~~
Such simple average is called _ordinary importance sampling_. It is unbiased, but
can have very high variance.

~~~
An alternative is _weighted importance sampling_, where we compute weighted
average as
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{âˆ‘_{tâˆˆğ“£(s)} Ï_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero), but usually has smaller variance.

---
# Refresh â€“ Off-policy Monte Carlo Prediction

![w=80%,h=center](../03/importance_sampling.pdf)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with sum of player's cards 13 and a usable ace, we estimate target
policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.

---
# Refresh â€“ Expected Sarsa

The action $A_{t+1}$ is a source of variance, moving only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ ğ”¼_Ï€ q(S_{t+1}, a) - q(S_t, A_t)\right]\\
              &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ âˆ‘\nolimits_a Ï€(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\right].
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|ğ“|$.

~~~
Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $Ï€$ to differ.

~~~
Especially, if $Ï€$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
section: Double Q
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $Îµ$-greedy variant of the target
policy, the same samples (up to $Îµ$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](double_q_learning_example.pdf)

---
section: Double Q
# Double Q-learning

![w=80%,h=center](double_q_learning.pdf)

---
section: $n$-step Methods
# $n$-step Methods

![w=40%,f=right](nstep_td.pdf)

Full return is
$$G_t = âˆ‘_{k=t}^âˆ R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + Î³ V_t(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n V_{t+n-1}(S_{t+n}).$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$.

---
# $n$-step Methods

A natural update rule is
$$V_{t+n}(S_t) â‰ V_{t+n-1}(S_t) + Î±\left[G_{t:t+n} - V_{t+n-1}(S_t)\right].$$

~~~
![w=55%,h=center](nstep_td_prediction.pdf)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../03/td_mc_comparison_example.pdf)

we obtain the following comparison of different values of $n$:
![w=50%,h=center](nstep_td_performance.pdf)

---
section: $n$-step Sarsa
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n Q_{t+n-1}(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$,
~~~
we get the following straightforward
algorithm:
$$Q_{t+n}(S_t, A_t) â‰ Q_{t+n-1}(S_t, A_t) + Î±\left[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\right].$$

~~~
![w=70%,h=center](nstep_sarsa_example.pdf)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.pdf)

---
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$Ï_{t:t+n} â‰ âˆ_{k=t}^{\min(t+n, T-1)} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD can be computed as
$$V_{t+n}(S_t) â‰ V_{t+n-1}(S_t) + Î±Ï_{t:t+n-1}\left[G_{t:t+n} - V_{t+n-1}(S_t)\right].$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q_{t+n}(S_t, A_t) â‰ Q_{t+n-1}(S_t, A_t) + Î±Ï_{t+1:t+n}\left[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\right].$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.pdf)

---
section: Tree Backup
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.pdf)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.pdf)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} â‰ R_{t+1} + âˆ‘\nolimits_a Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.pdf)
