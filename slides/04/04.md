title: NPFL122, Lecture 4
class: title, langtech, cc-by-nc-sa
# N-step Temporal Difference Methods

## Self-Study

### November 4, 2019

---
section: $n$-step Methods
# $n$-step Methods

![w=40%,f=right](nstep_td.pdf)

Full return is
$$G_t = ∑_{k=t}^∞ R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + γ V(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n V(S_{t+n}).$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$.

---
# $n$-step Methods

A natural update rule is
$$V(S_t) ← V(S_t) + α\left[G_{t:t+n} - V(S_t)\right].$$

~~~
![w=55%,h=center](nstep_td_prediction.pdf)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../03/td_mc_comparison_example.pdf)

we obtain the following comparison of different values of $n$:
![w=50%,h=center](nstep_td_performance.pdf)

---
section: $n$-step Sarsa
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n Q(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$,
~~~
we get the following straightforward
algorithm:
$$Q(S_t, A_t) ← Q(S_t, A_t) + α\left[G_{t:t+n} - Q(S_t, A_t)\right].$$

~~~
![w=70%,h=center](nstep_sarsa_example.pdf)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.pdf)

---
section: Off-policy $n$-step Sarsa
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$ρ_{t:t+n} ≝ ∏_{k=t}^{\min(t+n, T-1)} \frac{π(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD can be computed as
$$V(S_t) ← V(S_t) + αρ_{t:t+n-1}\left[G_{t:t+n} - V(S_t)\right].$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q(S_t, A_t) ← Q(S_t, A_t) + αρ_{\boldsymbol{t+1}:\boldsymbol{t+n}}\left[G_{t:t+n} - Q(S_t, A_t)\right].$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.pdf)

---
section: TreeBackup
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.pdf)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.pdf)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} ≝ R_{t+1} + ∑\nolimits_a π(a|S_{t+1}) Q(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$

~~~
The resulting algorithm is $n$-step **Tree backup** and it is an off-policy
$n$-step temporal difference method not requiring importance sampling.

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.pdf)
