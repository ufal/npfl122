title: NPFL122, Lecture 3
class: title, langtech, cc-by-nc-sa
# Temporal Difference Methods, Off-Policy Methods

## Milan Straka

### October 19, 2020

---
section: Refresh
# MDPs and Partially Observable MDPs

Recall that a **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
- $ğ“$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a **discount factor**.

~~~
**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(ğ“¢, ğ“, p, Î³, ğ“, o)$, where in addition to an MDP
- $ğ“$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model, which is used as agent input
  instead of $S_t$.

~~~
Although planning in general POMDP is undecidable, several approaches are used to
handle POMDPs in robotics (to model uncertainty, imprecise mechanisms and inaccurate
sensors, â€¦). In deep RL, partially observable MDPs are usually handled using recurrent
networks, which model the latent states $S_t$.

---
# Refresh â€“ Policies and Value Functions

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Optimal state-value function is defined as $v_*(s) â‰ \max_Ï€ v_Ï€(s),$
analogously optimal action-value function is defined as $q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$

Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**.

---
# Refresh â€“ Value Iteration

Optimal value function can be computed by repetitive application of Bellman
optimality equation:
$$\begin{aligned}
v_0(s) &â† 0 \\
v_{k+1}(s) &â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right] = B v_k.
\end{aligned}$$

~~~
Converges for finite-horizon tasks or when discount factor $Î³<1$.

---
# Refresh â€“ Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$Ï€_0 \stackrel{E}{\longrightarrow} v_{Ï€_0} \stackrel{I}{\longrightarrow}
  Ï€_1 \stackrel{E}{\longrightarrow} v_{Ï€_1} \stackrel{I}{\longrightarrow}
  Ï€_2 \stackrel{E}{\longrightarrow} v_{Ï€_2} \stackrel{I}{\longrightarrow}
  â€¦ \stackrel{I}{\longrightarrow} Ï€_* \stackrel{E}{\longrightarrow} v_{Ï€_*}.$$

~~~
The result is a sequence of monotonically improving policies $Ï€_i$. Note that
when $Ï€' = Ï€$, also $v_{Ï€'} = v_Ï€$, which means Bellman optimality equation is
fulfilled and both $v_Ï€$ and $Ï€$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $Ï€_{k+1}$, we usually start with $v_{Ï€_k}$,
which is assumed to be a good approximation to $v_{Ï€_{k+1}}$.

---
# Refresh â€“ Generalized Policy Iteration

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

![w=30%,mw=50%,h=center](../02/gpi.svgz)![w=80%,mw=50%,h=center](../02/gpi_convergence.svgz)

If both processes stabilize, we know we have obtained optimal policy.

---
# Refresh â€“ Monte Carlo Methods

Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework. Keeping estimated returns for the action-value function, we perform
policy evaluation by sampling one episode according to current policy. We then
update the action-value function by averaging over the observed returns,
including the currently sampled episode.

~~~

We considered two variants of exploration:
- exploring starts
~~~
- $Îµ$-soft policies

---
# Refresh â€“ Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{T+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: Afterstates
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  Ï€(s) &â‰ \argmax_a q_Ï€(s, a) \\
       &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.svgz)
However, if the environment is known, it is often better to estimate returns only
for states, because there can be substantially less states than state-action pairs.

---
section: TD
# TD Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $Î±$, which performs
$$v(S_t) â† v(S_t) + Î±\left[G_t - v(S_t)\right],$$
the simplest temporal-difference method computes the following:
$$v(S_t) â† v(S_t) + Î±\big[R_{t+1} + Î³v(S_{t+1}) - v(S_t)\big],$$

---
# TD Methods

![w=70%,h=center](td_example.svgz)

~~~
![w=70%,h=center](td_example_update.svgz)

---
# TD and MC Comparison

As with Monte Carlo methods, for a fixed policy $Ï€$, TD methods converge to
$v_Ï€$.

~~~
On stochastic tasks, TD methods usually converge to $v_Ï€$ faster than constant-$Î±$ MC
methods.

~~~
![w=70%,h=center](td_mc_comparison_example.svgz)

~~~
![w=75%,h=center](td_mc_comparison.svgz)

---
# Optimality of MC and TD Methods

![w=70%,mw=50%,h=center](td_mc_optimality_example.svgz)![w=90%,mw=50%,h=center](td_mc_optimality_data.svgz)

~~~
For state B, 6 out of 8 times return from B was 1 and 0 otherwise.
Therefore, $v(B) = 3/4$.

~~~
- [TD] For state A, in all cases it transfered to B. Therefore, $v(A)$ could be $3/4$.
~~~
- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.

~~~
MC minimizes error on training data, TD minimizes MLE error for the Markov
process.

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) â† q(S_t, A_t) + Î±\big[R_{t+1} + Î³ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\big].$$

~~~
![w=75%,h=center](sarsa.svgz)

---
# Sarsa

![w=65%,h=center](sarsa_example.svgz)

~~~
MC methods cannot be easily used, because an episode might not terminate if
current policy caused the agent to stay in the same state.

---
section: Q-learning
# Q-learning

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} +  Î³ \max_a q(S_{t+1}, a) -q(S_t, A_t)\right].$$

~~~
![w=80%,h=center](q_learning.svgz)

---
# Q-learning versus Sarsa

![w=70%,h=center](cliff_walking.svgz)

~~~ ~
# Q-learning versus Sarsa
![w=40%,h=center](cliff_walking.svgz)
![w=45%,h=center](cliff_walking_learning.svgz)

---
section: Double Q
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $Îµ$-greedy variant of the target
policy, the same samples (up to $Îµ$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](double_q_learning_example.svgz)

---
section: Double Q
# Double Q-learning

![w=80%,h=center](double_q_learning.svgz)

---
section: Off-policy
# On-policy and Off-policy Methods

So far, all methods were **on-policy**. The same policy was used both for
generating episodes and as a target of value function.

~~~
However, while the policy for generating episodes needs to be more exploratory,
the target policy should capture optimal behaviour.

~~~
Generally, we can consider two policies:
- **behaviour** policy, usually $b$, is used to generate behaviour and can be more
  exploratory;
~~~
- **target** policy, usually $Ï€$, is the policy being learned (ideally the optimal
  one).

~~~
When the behaviour and target policies differ, we talk about **off-policy**
learning.

---
# On-policy and Off-policy Methods

The off-policy methods are usually more complicated and slower to converge, but
are able to process data generated by different policy than the target one.

~~~
The advantages are:
- can compute optimal non-stochastic (non-exploratory) policies;

~~~
- more exploratory behaviour;

~~~
- ability to process _expert trajectories_.

---
# Off-policy Prediction

Consider prediction problem for off-policy case.

~~~
In order to use episodes from $b$ to estimate values for $Ï€$, we require that
every action taken by $Ï€$ is also taken by $b$, i.e.,
$$Ï€(a|s) > 0 â‡’ b(a|s) > 0.$$

~~~
Many off-policy methods utilize **importance sampling**, a general technique for
estimating expected values of one distribution given samples from another
distribution.

---
# Importance Sampling

Assume that $b$ and $Ï€$ are two distributions and let $x_i$ be $N$ samples of $b$.
We can then estimate $ğ”¼_{xâˆ¼b}[f(x)]$ as
$$ğ”¼_{xâˆ¼b}[f(x)] âˆ¼ \frac{1}{N} âˆ‘_i f(x_i).$$

~~~
In order to estimate $ğ”¼_{xâˆ¼Ï€}[f(x)]$ using the samples $x_i$, we need to account
for different probabilities of $x_i$ under the two distributions by
$$ğ”¼_{xâˆ¼Ï€}[f(x)] âˆ¼ \frac{1}{N} âˆ‘_i \frac{Ï€(x_i)}{b(x_i)} f(x_i)$$
with $Ï€(x)/b(x)$ being a **relative probability** of $x$ under the two
distributions.

---
# Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, â€¦, S_T$,
the probability of this episode under a policy $Ï€$ is
$$âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
behaviour policies is
$$Ï_t â‰ \frac{âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k)}{âˆ_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = âˆ_{k=t}^{T-1} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_Ï€(S_t) = ğ”¼_b[Ï_t G_t].$$

---
# Off-policy Monte Carlo Prediction

Let $ğ“£(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{|ğ“£(s)|}.$$

~~~
Such simple average is called **ordinary importance sampling**. It is unbiased, but
can have very high variance.

~~~
An alternative is **weighted importance sampling**, where we compute weighted
average as
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{âˆ‘_{tâˆˆğ“£(s)} Ï_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero), but has smaller variance.

---
# Off-policy Multi-armed Bandits

![w=30%,f=right](../01/k-armed_bandits.svgz)

As a simple example, consider the 10-armed bandits from the first lecture, with
single-step episodes.

~~~
Let the _behaviour policy_ be a uniform policy, so the return is a reward of
a randomly selected arm.

~~~
Let the _target policy_ be a greedy policy always using action 3.

~~~
Assume that the first sample from the behaviour policy produced action 3 with
reward R. Then
- Ordinary importance sampling estimate the return for the target policy as
  $$\frac{Ï€(a)}{b(a)} R = \frac{1}{1/10} R = 10â‹…R.$$
  The factor $10$ is present, because the behaviour policy returns action 3
  in 10% cases.

~~~
- Weighted importance sampling estimate the return for target policy as average
  of rewards for action 3.

---
# Off-policy Monte Carlo Policy Evaluation

![w=80%,h=center](importance_sampling.svgz)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with sum of player's cards 13 and a usable ace, we estimate target
policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.

---
# Off-policy Monte Carlo Policy Evaluation

We can compute weighted importance sampling similarly to the incremental
implementation of Monte Carlo averaging.

![w=75%,h=center](off_policy_mc_prediction.svgz)

---
# Off-policy Monte Carlo

![w=80%,h=center](off_policy_mc.svgz)

---
section: Expected Sarsa
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, providing correct estimate only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ ğ”¼_Ï€ q(S_{t+1}, a) - q(S_t, A_t)\right]\\
              &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ âˆ‘\nolimits_a Ï€(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\right].
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|ğ“|$.

---
# Expected Sarsa as an Off-policy Algorithm

Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $Ï€$ to differ.

~~~
Especially, if $Ï€$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
# Expected Sarsa Example

![w=25%](cliff_walking.svgz)![w=90%,mw=75%,h=center](expected_sarsa.svgz)

Asymptotic performance is averaged over 100k episodes, interim performance
over the first 100.

---
section: $n$-step
# $n$-step Methods

![w=40%,f=right](nstep_td.svgz)

Full return is
$$G_t = âˆ‘_{k=t}^âˆ Î³^{k-t} R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + Î³ V(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n V(S_{t+n}).$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$ (episode length).

---
# $n$-step Methods

A natural update rule is
$$V(S_t) â† V(S_t) + Î±\big[G_{t:t+n} - V(S_t)\big].$$

~~~
![w=55%,h=center](nstep_td_prediction.svgz)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../03/td_mc_comparison_example.svgz)

we obtain the following comparison of different values of $n$:
![w=50%,h=center](nstep_td_performance.svgz)

---
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n Q(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$,
~~~
we get the following straightforward
algorithm:
$$Q(S_t, A_t) â† Q(S_t, A_t) + Î±\big[G_{t:t+n} - Q(S_t, A_t)\big].$$

~~~
![w=70%,h=center](nstep_sarsa_example.svgz)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.svgz)

---
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$Ï_{t:t+n} â‰ âˆ_{k=t}^{\min(t+n, T-1)} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD policy evaluation can be computed as
$$V(S_t) â† V(S_t) + Î±Ï_{t:t+n-1}\big[G_{t:t+n} - V(S_t)\big].$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q(S_t, A_t) â† Q(S_t, A_t) + Î±Ï_{\boldsymbol{t+1}:\boldsymbol{t+n}}\big[G_{t:t+n} - Q(S_t, A_t)\big].$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.svgz)

---
section: TB
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.svgz)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.svgz)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} â‰ R_{t+1} + Î³âˆ‘\nolimits_a Ï€(a|S_{t+1}) Q(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$

~~~
The resulting algorithm is $n$-step **Tree backup** and it is an off-policy
$n$-step temporal difference method not requiring importance sampling.

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.svgz)
