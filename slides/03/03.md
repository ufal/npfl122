title: NPFL122, Lecture 3
class: title, langtech, cc-by-nc-sa
# Temporal Difference Methods, Off-Policy Methods

## Milan Straka

### October 22, 2018

---
section: Refresh
# Refresh â€“ Policies and Value Functions

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_Ï€(s)$, or
_state-value function_, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Optimal state-value function is defined as $v_*(s) â‰ \max_Ï€ v_Ï€(s),$
analogously optimal action-value function is defined as $q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$

Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an _optimal policy_.

---
# Refresh â€“ Value Iteration

Optimal value function can be computed by repetitive application of Bellman
optimality equation:
$$\begin{aligned}
v_0(s) &â† 0 \\
v_{k+1}(s) &â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right] = B v_k.
\end{aligned}$$

---
# Refresh â€“ Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$Ï€_0 \stackrel{E}{\longrightarrow} v_{Ï€_0} \stackrel{I}{\longrightarrow}
  Ï€_1 \stackrel{E}{\longrightarrow} v_{Ï€_1} \stackrel{I}{\longrightarrow}
  Ï€_2 \stackrel{E}{\longrightarrow} v_{Ï€_2} \stackrel{I}{\longrightarrow}
  â€¦ \stackrel{I}{\longrightarrow} Ï€_* \stackrel{E}{\longrightarrow} v_{Ï€_*}.$$

~~~
The result is a sequence of monotonically improving policies $Ï€_i$. Note that
when $Ï€' = Ï€$, also $v_{Ï€'} = v_Ï€$, which means Bellman optimality equation is
fulfilled and both $v_Ï€$ and $Ï€$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluation policy $Ï€_{k+1}$, we usually start with $v_{Ï€_k}$,
which is assumed to be a good approximation to $v_{Ï€_{k+1}}$.

---
# Refresh â€“ Generalized Policy Iteration

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

![w=30%,mw=50%,h=center](../02/gpi.pdf)![w=80%,mw=50%,h=center](../02/gpi_convergence.pdf)

If both processes stabilize, we know we have obtained optimal policy.

---
# Refresh â€“ $Îµ$-soft Policies

A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$

~~~
We call a policy $Îµ$-greedy, if one action has maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for class of $Îµ$-soft
policies, and using<br>$Îµ$-greedy policy in policy improvement step, policy
iteration has same convergence properties. (We can embed the $Îµ$-soft behaviour
â€œinsideâ€ the environment and prove equivalence.)

---
# Refresh â€“ Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{T+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: Afterstates
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  Ï€(s) &â‰ \argmax_a q_Ï€(s, a) \\
       &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.pdf)
However, if the environment is known, it might be better to estimate returns only
for states, and there can be substantially less states than state-action pairs.

---
section: TD
# TD Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $Î±$, which performs
$$v(S_t) â† v(S_t) + Î±\left[G_t - v(S_t)\right],$$
the simplest temporal-difference method computes the following:
$$v(S_t) â† v(S_t) + Î±\left[R_{t+1} + Î³v(S_{t+1}) - v(S_t)\right],$$

---
# TD Methods

![w=70%,h=center](td_example.pdf)

~~~
![w=70%,h=center](td_example_update.pdf)

---
# TD and MC Comparison

As with Monte Carlo methods, for a fixed policy $Ï€$, TD methods converge to
$v_Ï€$.

~~~
On stochastic tasks, TD methods usually converge to $v_Ï€$ faster than constant-$Î±$ MC
methods.

~~~
![w=70%,h=center](td_mc_comparison_example.pdf)

~~~
![w=75%,h=center](td_mc_comparison.pdf)

---
# Optimality of MC and TD Methods

![w=70%,mw=50%,h=center](td_mc_optimality_example.pdf)![w=90%,mw=50%,h=center](td_mc_optimality_data.pdf)

~~~
For state B, 6 out of 8 times return from B was 1 and 0 otherwise.
Therefore, $v(B) = 3/4$.

~~~
- [TD] For state A, in all cases it transfered to B. Therefore, $v(A)$ could be $3/4$.
~~~
- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.

~~~
MC minimizes error on training data, TD minimizes MLE error for the Markov
process.

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\right].$$

~~~
![w=75%,h=center](sarsa.pdf)

---
# Sarsa

![w=65%,h=center](sarsa_example.pdf)

~~~
MC methods cannot be easily used, because an episode might not terminate if
current policy caused the agent to stay in the same state.

---
section: Q-learning
# Q-learning

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} +  Î³ \max_a q(S_{t+1}, a) -q(S_t, A_t)\right].$$

~~~
![w=80%,h=center](q_learning.pdf)

---
# Q-learning versus Sarsa

![w=70%,h=center](cliff_walking.pdf)

~~~ ~
# Q-learning versus Sarsa
![w=40%,h=center](cliff_walking.pdf)
![w=45%,h=center](cliff_walking_learning.pdf)

---
section: Off-policy
# On-policy and Off-policy Methods

So far, all methods were _on-policy_. The same policy was used both for
generating episodes and as a target of value function.

~~~
However, while the policy for generating episodes needs to be more exploratory,
the target policy should capture optimal behaviour.

~~~
Generally, we can consider two policies:
- _behaviour_ policy, usually $b$, is used to generate behaviour and can be more
  exploratory
~~~
- _target_ policy, usually $Ï€$, is the policy being learned (ideally the optimal
  one)

~~~
When the behaviour and target policies differ, we talk about _off-policy_
learning.

---
# On-policy and Off-policy Methods

The off-policy methods are usually more complicated and slower to converge, but
are able to process data generated by different policy than the target one.

~~~
The advantages are:
- more exploratory behaviour;

~~~
- ability to process _expert trajectories_.

---
# Off-policy Prediction

Consider prediction problem for off-policy case.

~~~
In order to use episodes from $b$ to estimate values for $Ï€$, we require that
every action taken by $Ï€$ is also taken by $b$, i.e.,
$$Ï€(a|s) > 0 â‡’ b(a|s) > 0.$$

~~~
Many off-policy methods utilize _importance sampling_, a general technique for
estimating expected values of one distribution given samples from another
distribution.

---
# Importance Sampling

Assume that $b$ and $Ï€$ are two distributions.

Let $x_i$ be the samples of $b$ and $y_i$ the corresponding samples of
$$ğ”¼_{xâˆ¼b}[f(x)].$$

~~~
Our goal is to estimate
$$ğ”¼_{xâˆ¼Ï€}[f(x)] = âˆ‘_x Ï€(x) f(x).$$

~~~
We can therefore compute
$$âˆ‘_{x_i} \frac{Ï€(x_i)}{b(x_i)} f(x_i)$$
with $Ï€(x)/b(x)$ being a _relative probability_ of $x$ under the two
distributions.

---
# Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, â€¦, S_T$,
the probability of this episode under a policy $Ï€$ is
$$âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
behaviour policies is
$$Ï_t â‰ \frac{âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k)}{âˆ_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = âˆ_{k=t}^{T-1} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_Ï€(S_t) = ğ”¼_b[Ï_t G_t].$$

---
# Off-policy Monte Carlo Prediction

Let $ğ“£(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{|ğ“£(s)|}.$$

~~~
Such simple average is called _ordinary importance sampling_. It is unbiased, but
can have very high variance.

~~~
An alternative is _weighted importance sampling_, where we compute weighted
average as
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{âˆ‘_{tâˆˆğ“£(s)} Ï_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero), but usually has smaller variance.

---
# Off-policy Monte Carlo Prediction

![w=80%,h=center](importance_sampling.pdf)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with sum of player's cards 13 and a usable ace, we estimate target
policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.

---
# Off-policy Monte Carlo Prediction

We can compute weighted importance sampling similarly to the incremental
implementation of Monte Carlo averaging.

![w=75%,h=center](off_policy_mc_prediction.pdf)

---
# Off-policy Monte Carlo

![w=80%,h=center](off_policy_mc.pdf)

---
section: Expected Sarsa
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, moving only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ ğ”¼_Ï€ q(S_{t+1}, a) - q(S_t, A_t)\right]\\
              &â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ âˆ‘\nolimits_a Ï€(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\right].
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|ğ“|$.

---
# Expected Sarsa as Off-policy Algorithm

Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $Ï€$ to differ.

~~~
Especially, if $Ï€$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
# Expected Sarsa Example

![w=25%](cliff_walking.pdf)![w=90%,mw=75%,h=center](expected_sarsa.pdf)

Asymptotic performance is averaged over 100k episodes, interim performance
over the first 100.
