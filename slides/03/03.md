title: NPFL122, Lecture 3
class: title, langtech, cc-by-sa
# Temporal Difference Methods, Off-Policy Methods

## Milan Straka

### October 17, 2022

---
section: Refresh
# MDPs and Partially Observable MDPs

Recall that a **Markov decision process** (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:

![w=40%,f=right](../01/mdp.svgz)

- $𝓢$ is a set of states,
- $𝓐$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a **reward** $r ∈ ℝ$,
- $γ ∈ [0, 1]$ is a **discount factor**.

~~~
**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(𝓢, 𝓐, p, γ, 𝓞, o)$, where in addition to an MDP,

![w=40%,f=right](../01/pomdp.svgz)

- $𝓞$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$ is used as agent input
  instead of the state $S_t$.

---
# Refresh – Policies and Value Functions

A **policy** $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_π(s)$, or
**state-value function**, as
$$v_π(s) ≝ 𝔼_π\left[G_t \middle| S_t = s\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An **action-value function** for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\left[G_t \middle| S_t = s, A_t = a\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Optimal state-value function is defined as $v_*(s) ≝ \max_π v_π(s),$
analogously optimal action-value function is defined as $q_*(s, a) ≝ \max_π q_π(s, a).$

Any policy $π_*$ with $v_{π_*} = v_*$ is called an **optimal policy**.

---
# Refresh – Value Iteration

Optimal value function can be computed by repetitive application of Bellman
optimality equation:
$$\begin{aligned}
v_0(s) &← 0 \\
v_{k+1}(s) &← \max_a 𝔼\left[R_{t+1} + γ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right] = B v_k.
\end{aligned}$$

~~~
Converges for finite-horizon tasks or when discount factor $γ<1$.

---
# Refresh – Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$π_0 \stackrel{E}{\longrightarrow} v_{π_0} \stackrel{I}{\longrightarrow}
  π_1 \stackrel{E}{\longrightarrow} v_{π_1} \stackrel{I}{\longrightarrow}
  π_2 \stackrel{E}{\longrightarrow} v_{π_2} \stackrel{I}{\longrightarrow}
  … \stackrel{I}{\longrightarrow} π_* \stackrel{E}{\longrightarrow} v_{π_*}.$$

~~~
The result is a sequence of monotonically improving policies $π_i$. Note that
when $π' = π$, also $v_{π'} = v_π$, which means Bellman optimality equation is
fulfilled and both $v_π$ and $π$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $π_{k+1}$, we usually start with $v_{π_k}$,
which is assumed to be a good approximation to $v_{π_{k+1}}$.

---
# Refresh – Generalized Policy Iteration

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

![w=30%,mw=50%,h=center](../02/gpi.svgz)![w=80%,mw=50%,h=center](../02/gpi_convergence.svgz)

If both processes stabilize, we know we have obtained optimal policy.

---
# Refresh – Monte Carlo Methods

Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework. Keeping estimated returns for the action-value function, we perform
policy evaluation by sampling one episode according to current policy. We then
update the action-value function by averaging over the observed returns,
including the currently sampled episode.

~~~

We considered two variants of exploration:
- exploring starts
~~~
- $ε$-soft policies

---
# Refresh – Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{T+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: Afterstates
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  π(s) &≝ \argmax_a q_π(s, a) \\
       &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \left[r + γ v_π(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.svgz)
However, if the environment is known, it is often better to estimate returns only
for states, because there can be substantially less states than state-action pairs.

---
section: TD
# TD Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $α$, which performs
$$v(S_t) ← v(S_t) + α\big(G_t - v(S_t)\big),$$
the simplest temporal-difference method computes the following:
$$v(S_t) ← v(S_t) + α\big(R_{t+1} + [¬\textrm{done}]⋅γv(S_{t+1}) - v(S_t)\big),$$
where $[¬\textrm{done}]$ has a value of 1 if the episode continues in
the state $S_{t+1}$, and 0 otherwise.

~~~
We say TD methods are **bootstraping**, because they base their update on an
existing (action-)value function estimate.

---
# TD Methods

![w=70%,h=center](td_example.svgz)

~~~
![w=70%,h=center](td_example_update.svgz)

---
# TD Methods

An obvious advantage of TD methods compared to Monte Carlo is that
they are naturally implemented in _online_, _fully incremental_ fashion,
while the Monte Carlo methods must wait until an episode ends, because only then
the return is known.

~~~
The possibility of immediate learning is useful for:
- continuous environments,

~~~
- environments with extremely large episodes,
~~~
- environments ending after some nontrivial goal is reached, requiring some
  coordinated strategy from the agent (i.e., it is improbable that random
  actions will reach it).

---
# TD and MC Comparison

As with Monte Carlo methods, for a fixed policy $π$ (i.e., the policy evaluation
part of the algorithms), TD methods converge to $v_π$.

~~~
On stochastic tasks, TD methods usually converge to $v_π$ faster than constant-$α$ MC
methods.

~~~
![w=70%,h=center](td_mc_comparison_example.svgz)

~~~
![w=70%,h=center](td_mc_comparison.svgz)

---
# Optimality of MC and TD Methods

![w=58%,mw=50%,h=center](td_mc_optimality_example.svgz)![w=90%,mw=50%,h=center](td_mc_optimality_data.svgz)

~~~
For state B, 6 out of 8 times return from B was 1 and 0 otherwise.
Therefore, $v(B) = 3/4$.

~~~
- [TD] For state A, in all cases it transferred to B. Therefore, $v(A)$ could be $3/4$.
~~~
- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.

~~~
MC minimizes mean squared error on the returns from the training data, while TD
finds the estimates that would be exactly correct for a maximum-likelihood
estimate of the Markov process model (the estimated transition probability from
$s$ to $t$ is the faction of observed transitions from $s$ that went to $t$, and
the corresponding reward is the average of the rewards observed on those
transitions).

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) ← q(S_t, A_t) + α\big(R_{t+1} + [¬\textrm{done}]⋅γ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\big).$$

~~~
![w=75%,h=center](sarsa.svgz)

---
# Sarsa

![w=65%,h=center](sarsa_example.svgz)

~~~
MC methods cannot be easily used, because an episode might not terminate if
the current policy causes the agent to stay in the same state.

---
section: Q-learning
# Q-learning

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) ← q(S_t, A_t) + α\Big(R_{t+1} +  [¬\textrm{done}]⋅γ \max_a q(S_{t+1}, a) -q(S_t, A_t)\Big).$$

~~~
![w=80%,h=center](q_learning.svgz)

---
# Q-learning versus Sarsa

![w=70%,h=center](cliff_walking.svgz)

~~~ ~
# Q-learning versus Sarsa
![w=40%,h=center](cliff_walking.svgz)
![w=45%,h=center](cliff_walking_learning.svgz)

---
section: Off-policy
# On-policy and Off-policy Methods

So far, most methods were **on-policy**. The same policy was used both for
generating episodes and as a target of value function.

~~~
However, while the policy for generating episodes needs to be more exploratory,
the target policy should capture optimal behaviour.

~~~
Generally, we can consider two policies:
- **behaviour** policy, usually $b$, is used to generate behaviour and can be more
  exploratory;
~~~
- **target** policy, usually $π$, is the policy being learned (ideally the optimal
  one).

~~~
When the behaviour and target policies differ, we talk about **off-policy**
learning.

---
# On-policy and Off-policy Methods

The off-policy methods are usually more complicated and slower to converge, but
are able to process data generated by different policy than the target one.

~~~
The advantages are:
- can compute optimal non-stochastic (non-exploratory) policies;

~~~
- more exploratory behaviour;

~~~
- ability to process _expert trajectories_.

---
# Off-policy Prediction

Consider prediction problem for off-policy case.

~~~
In order to use episodes from $b$ to estimate values for $π$, we require that
every action taken by $π$ is also taken by $b$, i.e.,
$$π(a|s) > 0 ⇒ b(a|s) > 0.$$

~~~
Many off-policy methods utilize **importance sampling**, a general technique for
estimating expected values of one distribution given samples from another
distribution.

---
# Importance Sampling

Assume that $p$ and $q$ are two distributions and let $x_i$ be $N$ samples of $p$.
We can then estimate $𝔼_{⁇x∼p}\big[f(x)\big]$ as $\frac{1}{N} ∑_i f(x_i)$.

~~~
In order to estimate $𝔼_{⁇x∼q}\big[f(x)\big]$ using the samples $x_i$, we need to account
for different probabilities of $x_i$ under the two distributions. It is
straightforward to verify that
$$𝔼_{⁇x∼q}\big[f(x)\big] = 𝔼_{⁇x∼p}\left[\frac{q(x)}{p(x)}f(x)\right].$$

~~~
Therefore, we can estimate
$$𝔼_{⁇x∼q}\big[f(x)\big] ≈ \frac{1}{N} ∑_i \frac{q(x_i)}{p(x_i)} f(x_i),$$
with $q(x)/p(x)$ being the **relative probability** of $x$ under the two
distributions.

~~~
Both estimates mentioned on this slide are _unbiased_.

---
# Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, …, S_T$,
the probability of this episode under a policy $π$ is
$$∏_{k=t}^{T-1} π(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
behaviour policies is
$$ρ_t ≝ \frac{∏_{k=t}^{T-1} π(A_k | S_k) p(S_{k+1} | S_k, A_k)}{∏_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = ∏_{k=t}^{T-1} \frac{π(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
The $ρ_t$ is usually called the **importance sampling ratio** or _relative
probability_.

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_π(S_t) = 𝔼_b[ρ_t G_t].$$

---
# Off-policy Monte Carlo Prediction

Let $𝓣(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_π(s) = \frac{∑_{t∈𝓣(s)} ρ_t G_t}{|𝓣(s)|}.$$

~~~
Such simple average is called **ordinary importance sampling**. It is unbiased, but
can have very high variance.

~~~
An alternative is **weighted importance sampling**, where we compute weighted
average as
$$v_π(s) = \frac{∑_{t∈𝓣(s)} ρ_t G_t}{∑_{t∈𝓣(s)} ρ_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero, i.e., a _consistent_ estimate), but has smaller variance.

---
# Off-policy Multi-armed Bandits

![w=30%,f=right](../01/k-armed_bandits.svgz)

As a simple example, consider the 10-armed bandits from the first lecture, with
single-step episodes.

~~~
Let the _behaviour policy_ be a uniform policy, so the return is a reward of
a randomly selected arm.

~~~
Let the _target policy_ be a greedy policy always using action 3.

~~~
Assume that the first sample from the behaviour policy produced action 3 with
reward $R$. Then
- Ordinary importance sampling estimate the return for the target policy as
  $$\frac{π(a)}{b(a)} R = \frac{1}{1/10} R = 10⋅R.$$

  The factor $10$ is present, because the behaviour policy returns action 3
  in 10% cases.

~~~
- Weighted importance sampling estimate the return for target policy as average
  of rewards for action 3.

---
# Off-policy Monte Carlo Policy Evaluation

![w=80%,h=center](importance_sampling.svgz)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with sum of player's cards 13 and a usable ace, we estimate target
policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.

---
# Off-policy Monte Carlo Policy Evaluation

We can compute weighted importance sampling similarly to the incremental
implementation of Monte Carlo averaging.

![w=75%,h=center](off_policy_mc_prediction.svgz)

---
# Off-policy Monte Carlo

![w=80%,h=center](off_policy_mc.svgz)

---
section: Expected Sarsa
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, providing correct estimate only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &← q(S_t, A_t) + α\Big(R_{t+1} + [¬\textrm{done}]⋅γ 𝔼_π q(S_{t+1}, a) - q(S_t, A_t)\Big)\\
              &← q(S_t, A_t) + α\Big(R_{t+1} + [¬\textrm{done}]⋅γ ∑\nolimits_a π(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\Big).
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|𝓐|$.

---
# Expected Sarsa as an Off-policy Algorithm

Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $π$ to differ.

~~~
Especially, if $π$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
# Expected Sarsa Example

![w=25%](cliff_walking.svgz)![w=90%,mw=75%,h=center](expected_sarsa.svgz)

Asymptotic performance is an average over 100k episodes (10 runs), interim
performance over the first 100 episodes (50k runs); $ε$-greedy policy with
$ε=0.1%$ is used.

---
section: Double Q
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $ε$-greedy variant of the target
policy, the same samples (up to $ε$-greedy) determine both the maximizing action
and its value estimate.

~~~
![w=75%,h=center](double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](double_q_learning.svgz)

---
section: $n$-step
# $n$-step Methods

![w=40%,f=right](nstep_td.svgz)

Full return is
$$G_t = ∑_{k=t}^∞ γ^{k-t} R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + [¬\textrm{done}]⋅γ V(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n V(S_{t+n}).$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$ (episode length).

---
# $n$-step Methods

A natural update rule is
$$V(S_t) ← V(S_t) + α\big(G_{t:t+n} - V(S_t)\big).$$

~~~
![w=55%,h=center](nstep_td_prediction.svgz)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../03/td_mc_comparison_example.svgz)

we obtain the following comparison of different values of $n$:
![w=50%,h=center](nstep_td_performance.svgz)

---
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n Q(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$,
~~~
we get the following straightforward
algorithm:
$$Q(S_t, A_t) ← Q(S_t, A_t) + α\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

~~~
![w=70%,h=center](nstep_sarsa_example.svgz)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.svgz)

---
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$ρ_{t:t+n} ≝ ∏_{k=t}^{\min(t+n, T-1)} \frac{π(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD policy evaluation can be computed as
$$V(S_t) ← V(S_t) + αρ_{t:t+n-1}\big(G_{t:t+n} - V(S_t)\big).$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q(S_t, A_t) ← Q(S_t, A_t) + αρ_{\boldsymbol{t+1}:\boldsymbol{t+n}}\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.svgz)

---
section: TB
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.svgz)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.svgz)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} ≝ R_{t+1} + [¬\textrm{done}]⋅γ∑\nolimits_a π(a|S_{t+1}) Q(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+n},$$
with $G_{t:t+n} ≝ G_{t:T}$ if $t+n ≥ T$ (episode length).

~~~
The resulting algorithm is $n$-step **Tree backup** and it is an off-policy
$n$-step temporal difference method not requiring importance sampling.

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.svgz)
