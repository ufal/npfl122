title: NPFL122, Lecture 2
class: title, langtech, cc-by-nc-sa
# Markov Decision Process, Optimal Solutions, Monte Carlo Methods

## Milan Straka

### October 14, 2019

---
section: MDP Definition
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a _reward_ $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a _discount factor_.

~~~
Let a _return_ $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to have
separate reward distribution for every state. Such generalization is
usually called _Contextualized Bandits_ problem.
Assuming that state transitions are independent on rewards and given by
a distribution $\textit{next}(s)$, the MDP dynamics function for contextualized
bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called _episodes_, we talk about **episodic tasks**.
Each episode then ends in a special _terminal state_, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These _finite-horizont tasks_ then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_Ï€(s)$, or
_state-value function_, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_Ï€[q_Ï€(s, a)], \\
  q_Ï€(s, a) &= ğ”¼_Ï€[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an _optimal policy_. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.

~~~
## Existence
Under some mild assumptions, there always exists a unique optimal state-value function,
unique optimal action-value function, and (not necessarily unique) optimal policy.
The mild assumptions are that either termination is guaranteed from all
reachable states, or $Î³ < 1$.

---
section: Dynamic Programming
# Dynamic Programming

Dynamic programming is an approach devised by Richard Bellman in 1950s.

~~~
To apply it to MDP, we now consider finite-horizon problems with finite number
of states $ğ“¢$ and actions $ğ“$, and known MDP dynamics $p$.

~~~
The following recursion is usually called
the _Bellman equation_:
$$\begin{aligned}
  v_*(s) &= \max_a ğ”¼\left[R_{t+1} + Î³ v_*(S_{t+1}) \middle| S_t=s, A_t=a\right] \\
         &= \max_a âˆ‘_{s', r} p(s', r | s, a) \left[r + Î³ v_*(s')\right].
\end{aligned}$$

~~~
It must hold for an optimal value function in a MDP, because future decisions
does not depend on the current one. Therefore, the optimal policy can be
expressed as one action followed by optimal policy from the resulting state.

---
# Dynamic Programming

To turn the Bellman equation into an algorithm, we change the equal signs to assignments:
$$\begin{aligned}
v_0(s) &â† \begin{cases} 0&\textrm{for terminal state $s$} \\ -âˆ&\textrm{otherwise} \end{cases} \\
v_{k+1}(s) &â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].
\end{aligned}$$

~~~
In a finite-horizon task with at most $H$ steps, the optimal value function
is reached after $H$ iteration of the above assignment â€“ we can show by
induction that $v_k(s)$ is the maximum return reachable from state $s$ in $k$
steps.

---
# Relations to Graph Algorithms

Searching for optimal value functions of deterministic problems is in fact
search for the shortest path in a suitable graph.

~~~
![w=80%,mh=80%,h=center,v=middle](trellis.svg)

---
# Bellman-Ford-Moore Algorithm

$$v_{k+1}(s) â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

Bellman-Ford-Moore algorithm:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices: d[v] = 0 if v == s else +âˆ

for i in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```
---
# Bellman Equation Solutions

If we fix value of terminal states to 0, the Bellman equation has a unique
solution. Therefore, not only does the optimal value function satisfies the
Bellman equation, but the converse statement is also true: If a value function
satisfies the Bellman equation, it is optimal.

~~~
To sketch the proof of the statement, consider for a contradiction that
the value function is not optimal. Then there exists a state $s$ which has
different than optimal value.

~~~
Consider now a trajectory following some optimal policy. Such a trajectory
eventually reaches a terminal state.

~~~
Lastly focus on the last state on the trajectory with different than optimal
value â€“ the Bellman Equation cannot be fulfilled in this state.

---
section: Value Iteration
# Bellman Backup Operator

Our goal is now to handle also infinite horizon tasks, using discount factor of
$Î³ < 1$.

~~~
For any value function $vâˆˆâ„^{|ğ“¢|}$ we define _Bellman backup operator_ $B : â„^{|ğ“¢|} â†’ â„^{|ğ“¢|}$ as
$$Bv(s) â‰ \max_a ğ”¼\left[R_{t+1} + Î³ v(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

~~~
It is not difficult to show that Bellman backup operator is a _contraction_:
$$\max_s \left|Bv_1(s) - Bv_2(s)\right| â‰¤ Î³ \max_s \left|v_1(s) - v_2(s)\right|.$$

~~~
Considering a normed vector space $â„^{|ğ“¢|}$ with sup-norm $||â‹…||_âˆ$,
from Banach fixed-point theorem it follows there exist a _unique value function_
$v_*$ such that
$$Bv_* = v_*.$$

~~~
Such unique $v_*$ is the _optimal value function_, because it satistifes the
Bellman equation.

---
# Bellman Backup Operator

Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,
because
$$||Bv - v_*||_âˆ = ||Bv - Bv_*||_âˆ â‰¤ Î³||v - v_*||,$$
and therefore $B^nv â†’ v_*$.

---
# Value Iteration Algorithm

We can turn the iterative application of Bellman backup operator into an
algorithm.
$$Bv(s) â‰ \max_a ğ”¼\left[R_{t+1} + Î³ v(S_{t+1}) \middle| S_t=s, A_t=a\right]$$

![w=75%,h=center](value_iteration.pdf)

---
# Value Iteration Algorithm

Although we have described the so-called _synchronous_ implementation requiring
two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify
the value function in place (if a fixed ordering is used, usually such value
iteration is called _Gauss-Seidel_).

~~~
Even with such asynchronous update value iteration can be proven to converge,
and usually performs better in practise.

~~~
For example, the Bellman-Ford-Moore algorithm also updates the distances
in-place. In the case of dynamic programming, we can extend the invariant
from â€œ$v_k(s)$ is the maximum return reachable from state $s$ in $k$ stepsâ€
to include not only all trajectories of $k$ steps, but also some number of longer
trajectories.

---
# Bellman Backup Operator as a Contraction

To show that Bellman backup operator is a contraction, we proceed as follows:
~~~
$$\begin{aligned}
||Bv_1 - Bv_2||_âˆ &= ||\max_a ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - \max_a ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ \\
                  &â‰¤ \max_a\left( || ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ\right) \\
                  &= \max_a\left( \left|\left| âˆ‘\nolimits_{s', r} p\left(s', r \middle| s, a\right)Î³(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &= Î³ \max_a\left(\left|\left| âˆ‘\nolimits_{s'} p\left(s' \middle| s, a\right)(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &â‰¤ Î³ ||v_1 - v_2||_âˆ,
\end{aligned}$$

where the second line follows from $|\max_x f(x) - \max_x g(x)| â‰¤ \max_x |f(x) - g(x)|$
and the last line from the fact that from any given $s$ and $a$, the
$âˆ‘_{s'} p(s' | s, a)$ sums to 1.

---
# Speed of Convergence

Assuming maximum reward is $R_\textrm{max}$, we have that
$$v_*(s) â‰¤ âˆ‘_{t=0}^âˆ Î³^t R_\textrm{max} = \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Starting with $v(s) â† 0$, we have
$$||B^k v - v_*||_âˆ â‰¤ Î³^k ||v - v_*||_âˆ â‰¤ Î³^k \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Compare to finite horizon case, where $B^T v = v_*$.

---
# Value Iteration Example

Consider a simple betting game, where a gambler bets on the outcomes of
a sequence of coin flips, either losing their stake or winning the same
amount of coints that was bet. The gambler wins if they obtain 100 coins,
and lose if they run our of money.

~~~
We can formulate the problem as an undiscounted episodic MDP. The states
are the coins owned by the gambler, $\{1, â€¦, 99\}$, and actions are
stakes $\{1, â€¦, \min(s, 100-s)\}$. The reward is $+1$ when reaching 100
and 0 otherwise.

~~~
The state-value function then gives probability of winning from each state,
and policy prescribes a stake with a given capital.

---
# Value Iteration Example

![w=95%,h=center](value_iteration_example.pdf)

---
section: Policy Iteration
# Policy Iteration Algorithm

We now propose another approach of computing optimal policy. The approach,
called _policy iteration_, consists of repeatedly performing policy
_evaluation_ and policy _improvement_.

## Policy Evaluation

Given a policy $Ï€$, policy evaluation computes $v_Ï€$.

Recall that
$$\begin{aligned}
  v_Ï€(s) &â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] \\
         &= ğ”¼_Ï€\left[R_{t+1} + Î³ v_Ï€(S_{t+1}) \middle | S_t = s\right] \\
         &= âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

If the dynamics of the MDP $p$ is known, the above is a system of linear
equations, and therefore, $v_Ï€$ can be computed exactly.

---
# Policy Evaluation
The equation
$$v_Ï€(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]$$
is called _Bellman equation for $v_Ï€$_ and analogously to Bellman optimality
equation, it can be proven that
- under the same assumptions as before ($Î³<1$ or termination), $v_Ï€$ exists and
  is unique;
- $v_Ï€$ is a fixed point of the Bellman equation
  $$v_{k+1}(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_k(s')\right];$$
- iterative application of the Bellman equation to any $v$ converges to $v_Ï€$.

---
class: middle
# Policy Evaluation

![w=100%](policy_evaluation.pdf)

---
# Policy Improvement

Given $Ï€$ and computed $v_Ï€$, we would like to _improve_ the policy.
A straightforward way to do so is to define a policy using a _greedy_ action
$$\begin{aligned}
  Ï€'(s) &â‰ \argmax_a q_Ï€(s, a) \\
        &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

For such $Ï€'$, we can easily show that
$$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s).$$

---
# Policy Improvement Theorem

Let $Ï€$ and $Ï€'$ be any pair of deterministic policies, such that
$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s)$.

Then for all states $s$, $v_{Ï€'}(s) â‰¥ v_Ï€(s)$.

~~~
The proof is straightforward, we repeatedly expand $q_Ï€$ and use the
assumption of the policy improvement theorem:

~~~
$$\begin{aligned}
v_Ï€(s) &â‰¤ q_Ï€(s, Ï€'(s)) \\
       &= ğ”¼[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s, A_t = Ï€'(s)] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s] \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ q_Ï€(S_{t+1}, Ï€'(S_{t+1})) | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ ğ”¼[R_{t+2} + Î³ v_Ï€(S_{t+2}) | S_{t+1}, A_{t+1} = Ï€'(S_{t+1})] | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 v_Ï€(S_{t+2}) | S_t = s] \\
       &â€¦ \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 R_{t+3} + â€¦ | S_t = s] = v_{Ï€'}(s)
\end{aligned}$$

---
# Policy Improvement Example

![w=50%](gridworld_4x4.pdf)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.pdf)

---
# Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$Ï€_0 \stackrel{E}{\longrightarrow} v_{Ï€_0} \stackrel{I}{\longrightarrow}
  Ï€_1 \stackrel{E}{\longrightarrow} v_{Ï€_1} \stackrel{I}{\longrightarrow}
  Ï€_2 \stackrel{E}{\longrightarrow} v_{Ï€_2} \stackrel{I}{\longrightarrow}
  â€¦ \stackrel{I}{\longrightarrow} Ï€_* \stackrel{E}{\longrightarrow} v_{Ï€_*}.$$

~~~
The result is a sequence of monotonically improving policies $Ï€_i$. Note that
when $Ï€' = Ï€$, also $v_{Ï€'} = v_Ï€$, which means Bellman optimality equation is
fulfilled and both $v_Ï€$ and $Ï€$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $Ï€_{k+1}$, we usually start with $v_{Ï€_k}$,
which is assumed to be a good approximation to $v_{Ï€_{k+1}}$.

---
# Policy Iteration Algorithm
![w=70%,h=center](policy_iteration.pdf)

---
# Value Iteration as Policy Iteration

Note that value iteration is in fact a policy iteration, where policy evaluation
is performed only for one step:

$$\begin{aligned}
  Ï€'(s) &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(policy improvement)} \\
  v'(s) &= âˆ‘\nolimits_a Ï€'(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(one step of policy evaluation)}
\end{aligned}$$

Substituting the former into the latter, we get
$$v'(s) = \max_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s)\right] = Bv(s).$$

---
# Generalized Policy Iteration

Therefore, it seems that to achieve convergence, it is not necessary to perform
policy evaluation exactly.

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

~~~
![w=30%,mw=50%,h=center](gpi.pdf)![w=80%,mw=50%,h=center](gpi_convergence.pdf)

If both processes stabilize, we know we have obtained optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework.

~~~
Keeping estimated returns for the action-value function, we perform policy
evaluation by sampling one episode according to current policy. We then update
the action-value function by averaging over the observed returns, including
the currently sampled episode.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
~~~
- _first visit_: only the first occurence of a state-action pair in an episode is
  considered
- _every visit_: all occurences of a state-action pair are considered.

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.pdf)


---
# Monte Carlo and $Îµ$-soft Policies

A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$

~~~
For $Îµ$-soft policy, Monte Carlo policy evaluation also converges, without the need
of exploring starts.

~~~
We call a policy $Îµ$-greedy, if one action has maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $Îµ$-soft
policies, and using<br>$Îµ$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $Îµ$-soft behaviour
â€œinsideâ€ the environment and prove equivalence.)

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{T+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: Remarks
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  Ï€(s) &â‰ \argmax_a q_Ï€(s, a) \\
       &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.pdf)
However, if the environment is known, it might be better to estimate returns only
for states, and there can be substantially less states than state-action pairs.

---
# Partially Observable MDPs

Recall that a _Markov decision process_ (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
- $ğ“$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a _reward_ $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a _discount factor_.

~~~
_Partially observable Markov decision process_ extends the Markov decision
process to a sextuple $(ğ“¢, ğ“, p, Î³, ğ“, o)$, where in addition to an MDP
- $ğ“$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model.

~~~
Although planning in general POMDP is undecidable, several approaches are used to
handle POMDPs in robotics (to model uncertainty, imprecise mechanisms and inaccurate
sensors, â€¦). In deep RL, partially observable MDPs are usually handled using recurrent
networks, which model the latent states $S_t$.

---
section: TD
# TD Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $Î±$, which performs
$$v(S_t) â† v(S_t) + Î±\left[G_t - v(S_t)\right],$$
the simplest temporal-difference method computes the following:
$$v(S_t) â† v(S_t) + Î±\left[R_{t+1} + Î³v(S_{t+1}) - v(S_t)\right],$$

---
# TD Methods

![w=70%,h=center](td_example.pdf)

~~~
![w=70%,h=center](td_example_update.pdf)

---
# TD and MC Comparison

As with Monte Carlo methods, for a fixed policy $Ï€$, TD methods converge to
$v_Ï€$.

~~~
On stochastic tasks, TD methods usually converge to $v_Ï€$ faster than constant-$Î±$ MC
methods.

~~~
![w=70%,h=center](td_mc_comparison_example.pdf)

~~~
![w=75%,h=center](td_mc_comparison.pdf)

---
# Optimality of MC and TD Methods

![w=70%,mw=50%,h=center](td_mc_optimality_example.pdf)![w=90%,mw=50%,h=center](td_mc_optimality_data.pdf)

~~~
For state B, 6 out of 8 times return from B was 1 and 0 otherwise.
Therefore, $v(B) = 3/4$.

~~~
- [TD] For state A, in all cases it transfered to B. Therefore, $v(A)$ could be $3/4$.
~~~
- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.

~~~
MC minimizes error on training data, TD minimizes MLE error for the Markov
process.

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) â† q(S_t, A_t) + Î±\left[R_{t+1} + Î³ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\right].$$

~~~
![w=75%,h=center](sarsa.pdf)

---
# Sarsa

![w=65%,h=center](sarsa_example.pdf)

~~~
MC methods cannot be easily used, because an episode might not terminate if
current policy caused the agent to stay in the same state.

