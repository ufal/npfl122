title: NPFL122, Lecture 2
class: title, langtech, cc-by-nc-sa
# Markov Decision Process, Optimal Solutions, Monte Carlo Methods

## Milan Straka

### October 12, 2020

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](../01/diagram.svgz)

~~~~
# Markov Decision Process

![w=55%,h=center](../01/diagram.svgz)

A **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizont tasks** then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_Ï€(s) & â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right] \\
         & = ğ”¼_{A_t âˆ¼ Ï€(s)} ğ”¼_{S_{t+1},R_{t+1} âˆ¼ p(s,A_t)} \big[R_{t+1}
           + Î³ ğ”¼_{A_{t+1} âˆ¼ Ï€(S_{t+1})} ğ”¼_{S_{t+2},R_{t+2} âˆ¼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + â€¦ \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and state-value function can be of course expressed using one another:
$$v_Ï€(s) = ğ”¼_Ï€[q_Ï€(s, a)],~~~~~~~q_Ï€(s, a) = ğ”¼[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizont tasks or if $Î³ < 1$, there always exists a unique optimal
state-value function, aÂ unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: Dynamic Programming
# Dynamic Programming

Dynamic programming is an approach devised by Richard Bellman in 1950s.

~~~
To apply it to MDP, we now consider finite-horizon problems with finite number
of states $ğ“¢$, finite number of actions $ğ“$, and known MDP dynamics $p$.
Note that without loss of generality, we can assume that every episode takes
exactly $H$ steps (by introducing a suitable absorbing state, if necessary).

~~~
The following recursion is usually called
the _Bellman equation_:
$$\begin{aligned}
  v_*(s) &= \max_a ğ”¼\big[R_{t+1} + Î³ v_*(S_{t+1}) \big| S_t=s, A_t=a\big] \\
         &= \max_a âˆ‘_{s', r} p(s', r | s, a) \big[r + Î³ v_*(s')\big].
\end{aligned}$$

~~~
It must hold for an optimal value function in a MDP, because future decisions
do not depend on the current one. Therefore, the optimal policy can be
expressed as one action followed by optimal policy from the resulting state.

---
# Dynamic Programming

To turn the Bellman equation into an algorithm, we change the equal signs to assignments:
$$\begin{aligned}
v_0(s) &â† \begin{cases} 0&\textrm{for the terminal state $s$} \\ -âˆ&\textrm{otherwise} \end{cases} \\
v_{k+1}(s) &â† \max_a ğ”¼\big[R_{t+1} + Î³ v_k(S_{t+1}) \big| S_t=s, A_t=a\big].
\end{aligned}$$

~~~
In a finite-horizon task with $H$ steps, the optimal value function is reached
after $H$ iterations of the above assignment:
~~~
- We can show by induction that $v_k(s)$ is the maximum return reachable from
  state $s$ in last $k$ steps of an episode.
~~~
- If every episode ends in at most $H$ steps, then $v_{H+1}$ must be equal
  to $v_H$.

---
# Relations to Graph Algorithms

Searching for optimal value functions of deterministic problems is in fact
search for the shortest path in a suitable graph.

~~~
![w=80%,mh=80%,h=center,v=middle](trellis.svg)

---
# Bellman-Ford-Moore Algorithm

$$v_{k+1}(s) â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

Bellman-Ford-Moore algorithm:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices:
  d[v] = 0 if v == s else +âˆ

for iteration in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```
---
# Uniqueness of Bellman Equation Solution

Not only does the optimal value function fulfill the Bellman equation in the
current settings, the converse is also true: If a value function satisfies
the Bellman equation, it is optimal.
~~~

To sketch the proof of the statement, consider for a contradiction that
some solution of Bellman equation is not an optimal value function.
Therefore, there exist states with different than optimal values.
~~~

Among those states, at least one is always the last one on a trajectory
(if it is present). However, it can be easily verified that if its
value is not optimal, Bellman equation cannot hold in this state.

---
section: Value Iteration
# Bellman Backup Operator

Our goal is now to handle also infinite horizon tasks, using discount factor of
$Î³ < 1$. However, we still assume finite number of states and actions.

~~~
For any value function $vâˆˆâ„^{|ğ“¢|}$ we define _Bellman backup operator_ $B : â„^{|ğ“¢|} \rightarrow â„^{|ğ“¢|}$ as
$$Bv(s) â‰ \max_a ğ”¼\big[R_{t+1} + Î³ v(S_{t+1}) \big| S_t=s, A_t=a\big].$$

~~~
It is not difficult to show that Bellman backup operator is a _contraction_
(even for infinite number of states):
$$\sup_s \big|Bv_1(s) - Bv_2(s)\big| â‰¤ Î³ \sup_s \big|v_1(s) - v_2(s)\big|.$$

~~~
Considering a normed vector space $â„^{|ğ“¢|}$ with supremum norm $||â‹…||_âˆ$,
from the Banach fixed-point theorem it follows there exist a _unique value function_
$v_*$ such that $Bv_* = v_*$.

~~~
Such a unique $v_*$ is the _optimal value function_, because it satistifes the
Bellman equation.

---
# Bellman Backup Operator

Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,
because
$$||Bv - v_*||_âˆ = ||Bv - Bv_*||_âˆ â‰¤ Î³||v - v_*||,$$
and therefore $B^nv \rightarrow v_*$.

---
# Value Iteration Algorithm

We can turn the iterative application of Bellman backup operator into an
algorithm.
$$Bv(s) â‰ \max_a ğ”¼\big[R_{t+1} + Î³ v(S_{t+1}) \big| S_t=s, A_t=a\big]$$

![w=75%,h=center](value_iteration.svgz)

---
# Value Iteration Algorithm

Although we have described the so-called _synchronous_ implementation requiring
two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify
the value function in place (if a fixed ordering is used, usually such value
iteration is called _Gauss-Seidel_).

~~~
Even with such asynchronous update value iteration can be proven to converge,
and usually performs better in practise.

~~~
For example, the Bellman-Ford-Moore algorithm also updates the distances
in-place. In the case of dynamic programming, we can extend the invariant
from â€œ$v_k(s)$ is the maximum return reachable from state $s$ in last $k$ steps
of an episodeâ€ to include not only all trajectories of $k$ steps, but also any
number of longer trajectories.

---
# Bellman Backup Operator as a Contraction

To show that Bellman backup operator is a contraction, we proceed as follows:
~~~
$$\begin{aligned}
||Bv_1 - Bv_2||_âˆ &= ||\max_a ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - \max_a ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ \\
                  &= ||\max_a \big(ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - \max_a ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]\big)||_âˆ \\
                  &â‰¤ ||\max_a \big(ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]\big)||_âˆ \\
                  &= \max_a\big( || ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ\big) \\
                  &= \max_a\left( \left|\left| âˆ‘\nolimits_{s', r} p\left(s', r \middle| s, a\right)Î³(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &= Î³ \max_a\left(\left|\left| âˆ‘\nolimits_{s'} p\left(s' \middle| s, a\right)(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &â‰¤ Î³ ||v_1 - v_2||_âˆ,
\end{aligned}$$

where the last line follows from the fact that for any $s$ and $a$,
$âˆ‘_{s'} p(s' | s, a)$ sums to 1.

---
# Speed of Convergence

Assuming maximum reward is $R_\textrm{max}$, we have that
$$v_*(s) â‰¤ âˆ‘_{t=0}^âˆ Î³^t R_\textrm{max} = \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Starting with $v(s) â† 0$, we have
$$||B^k v - v_*||_âˆ â‰¤ Î³^k ||v - v_*||_âˆ â‰¤ Î³^k \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Compare to finite horizon case, where $B^T v = v_*$.

---
# Value Iteration Example

Consider a simple betting game, where a gambler repeatedly bets on the outcome
of a coin flip (with a given win probability), either losing their stake or
winning the same amount of coins that was bet. The gambler wins if they obtain
100 coins, and lose if they run our of money.

~~~
We can formulate the problem as an undiscounted episodic MDP. The states
are the coins owned by the gambler, $\{1, â€¦, 99\}$, and actions are
stakes $\{1, â€¦, \min(s, 100-s)\}$. The reward is $+1$ when reaching 100
and 0 otherwise.

~~~
The state-value function then gives probability of winning from each state,
and policy prescribes a stake with a given capital.

---
# Value Iteration Example

For a coin flip win probability 40%, the value iteration proceeds as follows.

![w=91%,h=center](value_iteration_example.svgz)

---
section: Policy Iteration
# Policy Iteration Algorithm

We now propose another approach of computing optimal policy. The approach,
called _policy iteration_, consists of repeatedly performing policy
_evaluation_ and policy _improvement_.

## Policy Evaluation

Given a policy $Ï€$, policy evaluation computes $v_Ï€$.

Recall that
$$\begin{aligned}
  v_Ï€(s) &â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] \\
         &= ğ”¼_Ï€\left[R_{t+1} + Î³ v_Ï€(S_{t+1}) \middle | S_t = s\right] \\
         &= âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

If the dynamics of the MDP $p$ is known, the above is a system of linear
equations, and therefore, $v_Ï€$ can be computed exactly.

---
# Policy Evaluation
The equation
$$v_Ï€(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]$$
is called _Bellman equation for $v_Ï€$_ and analogously to Bellman optimality
equation, it can be proven that
- under the same assumptions as before ($Î³<1$ or termination), $v_Ï€$ exists and is unique;
~~~
- $v_Ï€$ is a fixed point of the Bellman equation
  $$v_{k+1}(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_k(s')\right];$$
~~~
- iterative application of the Bellman equation to any $v$ converges to $v_Ï€$
  (the proof is easier than for the optimality equation, because $v_Ï€$ is
  defined using an expectation and expectations are linear, so we get the first half
  of the proof â€œfor freeâ€).

---
class: middle
# Policy Evaluation

![w=100%](policy_evaluation.svgz)

---
# Policy Improvement

Given $Ï€$ and computed $v_Ï€$, we would like to _improve_ the policy.
A straightforward way to do so is to define a policy using a _greedy_ action
$$\begin{aligned}
  Ï€'(s) &â‰ \argmax_a q_Ï€(s, a) \\
        &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

For such $Ï€'$, by construction it obviously holds that
$$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s).$$

---
# Policy Improvement Theorem

Let $Ï€$ and $Ï€'$ be any pair of deterministic policies, such that
$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s)$.

Then for all states $s$, $v_{Ï€'}(s) â‰¥ v_Ï€(s)$.

~~~
The proof is straightforward, we repeatedly expand $q_Ï€$ and use the
assumption of the policy improvement theorem:

~~~
$$\begin{aligned}
v_Ï€(s) &â‰¤ q_Ï€(s, Ï€'(s)) \\
       &= ğ”¼[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s, A_t = Ï€'(s)] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s] \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ q_Ï€(S_{t+1}, Ï€'(S_{t+1})) | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ ğ”¼[R_{t+2} + Î³ v_Ï€(S_{t+2}) | S_{t+1}, A_{t+1} = Ï€'(S_{t+1})] | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 v_Ï€(S_{t+2}) | S_t = s] \\
       &â€¦ \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 R_{t+3} + â€¦ | S_t = s] = v_{Ï€'}(s)
\end{aligned}$$

---
# Policy Improvement Example

![w=50%](gridworld_4x4.svgz)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.svgz)

---
# Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$Ï€_0 \stackrel{E}{\longrightarrow} v_{Ï€_0} \stackrel{I}{\longrightarrow}
  Ï€_1 \stackrel{E}{\longrightarrow} v_{Ï€_1} \stackrel{I}{\longrightarrow}
  Ï€_2 \stackrel{E}{\longrightarrow} v_{Ï€_2} \stackrel{I}{\longrightarrow}
  â€¦ \stackrel{I}{\longrightarrow} Ï€_* \stackrel{E}{\longrightarrow} v_{Ï€_*}.$$

~~~
The result is a sequence of monotonically improving policies $Ï€_i$. Note that
when $Ï€' = Ï€$, also $v_{Ï€'} = v_Ï€$, which means Bellman optimality equation is
fulfilled and both $v_Ï€$ and $Ï€$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $Ï€_{k+1}$, we usually start with $v_{Ï€_k}$,
which is assumed to be a good approximation to $v_{Ï€_{k+1}}$.

---
# Policy Iteration Algorithm
![w=70%,h=center](policy_iteration.svgz)

---
# Value Iteration as Policy Iteration

Note that value iteration is in fact a policy iteration, where policy evaluation
is performed only for one step:

$$\begin{aligned}
  Ï€'(s) &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(policy improvement)} \\
  v'(s) &= âˆ‘\nolimits_a Ï€'(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(one step of policy evaluation)}
\end{aligned}$$

Substituting the former into the latter, we get
$$v'(s) = \max_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s)\right] = Bv(s).$$

---
# Generalized Policy Iteration

Therefore, it seems that to achieve convergence, it is not necessary to perform
policy evaluation exactly.

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

~~~
![w=30%,mw=50%,h=center](gpi.svgz)![w=80%,mw=50%,h=center](gpi_convergence.svgz)

If both processes stabilize, we know we have obtained optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework. Keeping estimated returns for the action-value function, we perform
policy evaluation by sampling one episode according to current policy. We then
update the action-value function by averaging over the observed returns,
including the currently sampled episode.

---
# Monte Carlo Methods

To hope for convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
~~~
- _first visit_: only the first occurence of a state-action pair in an episode is
  considered
- _every visit_: all occurences of a state-action pair are considered.

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.svgz)


---
# Monte Carlo and $Îµ$-soft Policies

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$
and we call it $Îµ$-greedy, if one action has a maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $Îµ$-soft
policies, and using $Îµ$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $Îµ$-soft behaviour
â€œinsideâ€ the environment and prove equivalence.)

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{t+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$
