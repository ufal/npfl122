title: NPFL122, Lecture 2
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Markov Decision Process, Optimal Solutions, Monte Carlo Methods

## Milan Straka

### October 10, 2022

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](../01/mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](../01/mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
~~~
- $𝓐$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a **reward** $r ∈ ℝ$,
~~~
- $γ ∈ [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.

---
# Partially Observable MDPs

![w=68%,h=center](../01/pomdp.svgz)

**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(𝓢, 𝓐, p, γ, 𝓞, o)$, where in addition to an MDP,
- $𝓞$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$ is used as agent input
  instead of the state $S_t$.

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $γ=1$,
because the return $G ≝ ∑_{t=0}^H γ^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $γ$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_π(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_π(s) & ≝ 𝔼_π\big[G_t \big| S_t = s\big] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right] \\
         & = 𝔼_{A_t ∼ π(s)} 𝔼_{S_{t+1},R_{t+1} ∼ p(s,A_t)} \big[R_{t+1}
           + γ 𝔼_{A_{t+1} ∼ π(S_{t+1})} 𝔼_{S_{t+2},R_{t+2} ∼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + … \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\big[G_t \big| S_t = s, A_t = a\big] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and action-value function can be of course expressed using one another:
$$v_π(s) = 𝔼_{a∼π}\big[q_π(s, a)\big],~~~~~~~q_π(s, a) = 𝔼_{s', r ∼ p}\big[r + γv_π(s')\big].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ≝ \max_π v_π(s),$$
~~~
analogously
$$q_*(s, a) ≝ \max_π q_π(s, a).$$

~~~
Any policy $π_*$ with $v_{π_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $π_*(s) ≝ \argmax_a q_*(s, a) = \argmax_a 𝔼\big[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a\big]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $γ < 1$, there always exists a unique optimal
state-value function, a unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: Dynamic Programming
# Dynamic Programming

Dynamic programming is an approach devised by Richard Bellman in 1950s.

~~~
To apply it to MDP, we now consider finite-horizon problems with finite number
of states $𝓢$, finite number of actions $𝓐$, and known MDP dynamics $p$.
Note that without loss of generality, we can assume that every episode takes
exactly $H$ steps (by introducing a suitable absorbing state, if necessary).

~~~
The following recursion is usually called
the **Bellman equation**:
$$\begin{aligned}
  v_*(s) &= \max_a 𝔼\big[R_{t+1} + γ v_*(S_{t+1}) \big| S_t=s, A_t=a\big] \\
         &= \max_a ∑_{s', r} p(s', r | s, a) \big[r + γ v_*(s')\big].
\end{aligned}$$

~~~
It must hold for an optimal value function in a MDP, because future decisions
do not depend on the current one. Therefore, the optimal policy can be
expressed as one action followed by optimal policy from the resulting state.

---
# Dynamic Programming

To turn the Bellman equation into an algorithm, we change the equal sign to an assignment:
$$\begin{aligned}
v_0(s) &← \begin{cases} 0&\textrm{for the terminal state $s$} \\ -∞&\textrm{otherwise} \end{cases} \\
v_{k+1}(s) &← \max_a 𝔼\big[R_{t+1} + γ v_k(S_{t+1}) \big| S_t=s, A_t=a\big].
\end{aligned}$$

~~~
In a finite-horizon task with $H$ steps, the optimal value function is reached
after $H$ iterations of the above assignment:
~~~
- We can show by induction that $v_k(s)$ is the maximum return reachable from
  state $s$ in last $k$ steps of an episode.
~~~
- If every episode ends in at most $H$ steps, then $v_{H+1}$ must be equal
  to $v_H$.

---
# Relations to Graph Algorithms

In current settings, searching for the optimal value function of a deterministic
MDP problem (i.e., when there are always just a single next state and a single
reward) is in fact the same as searching for the longest path in a suitable
graph:

![w=60%,h=center](value_trellis.svgz)

where the value of an edge going from $v_{t+1}(s_i)$ to $v_t(s_j)$ is either the
highest reward some transition $p(s_i, a) → (s_j, r)$ produces, or $-∞$ if
no action from the state $s_i$ leads to $s_j$.

---
# Bellman-Ford-Moore Algorithm

Consider the dynamic programming algorithm of the repeated Bellman equation application:
$$\begin{aligned}
v_0(s) &← \begin{cases} 0&\textrm{for the terminal state $s$} \\ -∞&\textrm{otherwise} \end{cases} \\
v_{k+1}(s) &← \max_a 𝔼\big[R_{t+1} + γ v_k(S_{t+1}) \big| S_t=s, A_t=a\big].
\end{aligned}$$

~~~
The Bellman-Ford-Moore shortest-path algorithm can be considered its special-case:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices:
  d[v] = 0 if v == s else +∞

for iteration in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```
---
# Uniqueness of Bellman Equation Solution

Not only does the optimal value function fulfill the Bellman equation in the
current settings, the converse is also true: If a value function satisfies
the Bellman equation, it is optimal.
~~~

To sketch the proof of the statement, consider for a contradiction that
some solution of Bellman equation is not an optimal value function.
Therefore, there exist states with different than optimal values.
~~~

Among those states, we choose such a state that all trajectories from it
contains only states with optimal values. We can find it by starting in an
arbitrary state with different than optimal value, and then repeatedly
switching into a reachable state with different than optimal value function.

For such a state, however, if its value is not optimal, then the Bellman
equation cannot hold in this state, which is a contradiction.

---
section: Value Iteration
# Bellman Backup Operator

Our goal is now to handle also infinite-horizon tasks, using discount factor of
$γ < 1$. However, we still assume finite number of states and actions.

~~~
For any value function $v∈ℝ^{|𝓢|}$ we define **Bellman backup operator** $B : ℝ^{|𝓢|} \rightarrow ℝ^{|𝓢|}$ as
$$Bv(s) ≝ \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big].$$

~~~
Considering the supremum norm $\|x\|_∞ ≝ \sup_s |x(s)|$, we will show that
Bellman backup operator is a _contraction_ (even for infinite number of states), i.e.,
$$\sup_s \big|Bv_1(s) - Bv_2(s)\big| = \big\|Bv_1 - Bv_2\big\|_∞ ≤ γ \|v_1 - v_2\|_∞.$$

~~~
Applying the Banach fixed-point theorem on the normed vector space $ℝ^{|𝓢|}$ with the supremum norm
then yields that there exists a _unique value function_ $v_*$ such that $Bv_* = v_*$.

~~~
Such a unique $v_*$ is the _optimal value function_, because it satistifes the
Bellman equation.

---
# Bellman Backup Operator

Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,
because
$$\big\|Bv - v_*\big\|_∞ = \big\|Bv - Bv_*\big\|_∞ ≤ γ\|v - v_*\|,$$
and therefore $B^nv \rightarrow v_*$.

---
# Value Iteration Algorithm

We can turn the iterative application of Bellman backup operator into an
algorithm.
$$Bv(s) ≝ \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big]$$

![w=75%,h=center](value_iteration.svgz)

---
# Value Iteration Algorithm

Although we have described the so-called _synchronous_ implementation requiring
two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify
the value function in place (if a fixed ordering is used, usually such value
iteration is called _Gauss-Seidel_).

<div class="algorithm">

- for $s ∈ S$ in some fixed order:
  - $v(s) ← \max_a 𝔼\big[R_{t+1} + γ v(S_{t+1}) \big| S_t=s, A_t=a\big]$
</div>

~~~
Even with such asynchronous update, value iteration can be proven to converge,
and usually performs better in practice.

~~~
For example, the Bellman-Ford-Moore algorithm also updates the distances
in-place. In the case of dynamic programming, we can extend the invariant
from “$v_k(s)$ is the maximum return reachable from state $s$ in last $k$ steps
of an episode” to include not only all trajectories of $k$ steps, but also any
number of longer trajectories.

~~~
If you are interested, try proving that the above Gauss-Seidel iteration is also
a contraction.

---
# Bellman Backup Operator as a Contraction

To show that Bellman backup operator is a contraction, we proceed as follows:

~~~
$\displaystyle \big\|Bv_1 - Bv_2\big\|_∞ = \big\|\max_a 𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - \max_a 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = \big\|\max_a \big(𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - \max_a 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\big)\big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} ≤ \big\|\max_a \big(𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\big)\big\|_∞$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = \max_a\Big( \Big\| 𝔼\big[R_{t+1} + γ v_1(S_{t+1})\big] - 𝔼\big[R_{t+1} + γ v_2(S_{t+1})\big]\Big\|_∞\Big)$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = \max_a\Big( \Big\| ∑\nolimits_{s', r} p(s', r | s, a)γ\big(v_1(s') - v_2(s')\big)\Big\|_∞\Big)$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} = γ \max_a\Big( \Big\| ∑\nolimits_{s'} p(s' | s, a)(v_1(s') - v_2(s'))\Big\|_∞\Big)$

~~~
$\displaystyle \phantom{\big\|Bv_1 - Bv_2\big\|_∞} ≤ γ \|v_1 - v_2\|_∞$

where the last line follows from the fact that for any $s$ and $a$,
$∑_{s'} p(s' | s, a)$ sums to 1.

---
# Speed of Convergence

Assuming maximum reward is $R_\textrm{max}$, we have that
$$v_*(s) ≤ ∑_{t=0}^∞ γ^t R_\textrm{max} = \frac{R_\textrm{max}}{1-γ}.$$

~~~
Starting with $v(s) ← 0$, we have
$$\big\|B^k v - v_*\big\|_∞ ≤ γ^k \|v - v_*\|_∞ ≤ γ^k \frac{R_\textrm{max}}{1-γ}.$$

~~~
Compare to finite-horizon case, where $B^T v = v_*$.

---
# Value Iteration Example

Consider a simple betting game, where a gambler repeatedly bets on the outcome
of a coin flip (with a given win probability), either losing their stake or
winning the same amount of coins that was bet. The gambler wins if they obtain
100 coins, and lose if they run our of money.

~~~
We can formulate the problem as an undiscounted episodic MDP. The states
are the coins owned by the gambler, $\{1, …, 99\}$, and actions are the
stakes $\{1, …, \min(s, 100-s)\}$. The reward is $+1$ when reaching 100
and 0 otherwise.

~~~
The state-value function then gives probability of winning from each state,
and policy prescribes a stake with a given capital.

---
# Value Iteration Example

For a coin flip win probability 40%, the value iteration proceeds as follows.

![w=91%,h=center](value_iteration_example.svgz)

---
section: Policy Iteration
# Policy Iteration Algorithm

We now propose another approach of computing optimal policy. The approach,
called **policy iteration**, consists of repeatedly performing policy
**evaluation** and policy **improvement**.

## Policy Evaluation

Given a policy $π$, policy evaluation computes $v_π$.

Recall that
$$\begin{aligned}
  v_π(s) &≝ 𝔼_π\big[G_t \big| S_t = s\big] \\
         &= 𝔼_π\big[R_{t+1} + γ v_π(S_{t+1}) \big | S_t = s\big] \\
         &= ∑\nolimits_a π(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v_π(s')\big].
\end{aligned}$$

If the dynamics of the MDP $p$ is known, the above is a system of linear
equations, and therefore, $v_π$ can be computed exactly.

---
# Policy Evaluation
The equation
$$v_π(s) = ∑\nolimits_a π(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \left[r + γ v_π(s')\right]$$
is called **Bellman equation for $v_π$** and analogously to Bellman optimality
equation, it can be proven that
- under the same assumptions as before ($γ<1$ or termination), $v_π$ exists and is unique;
~~~
- $v_π$ is a fixed point of the Bellman equation
  $$v_{k+1}(s) = ∑\nolimits_a π(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v_k(s')\big];$$
~~~
- iterative application of the Bellman equation to any $v$ converges to $v_π$
  (the proof is easier than for the optimality equation, because $v_π$ is
  defined using an expectation and expectations are linear, so we get the first half
  of the proof “for free”).

---
class: middle
# Policy Evaluation

![w=100%](policy_evaluation.svgz)

---
# Policy Improvement

Given $π$ and computed $v_π$, we would like to **improve** the policy.
A straightforward way to do so is to define a policy using a _greedy_ action
$$\begin{aligned}
  π'(s) &≝ \argmax_a q_π(s, a) \\
        &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v_π(s')\big].
\end{aligned}$$

For such $π'$, by construction it obviously holds that
$$q_π(s, π'(s)) ≥ v_π(s).$$

---
# Policy Improvement Theorem

Let $π$ and $π'$ be any pair of deterministic policies, such that
$q_π(s, π'(s)) ≥ v_π(s)$.

Then for all states $s$, $v_{π'}(s) ≥ v_π(s)$.

~~~
The proof is straightforward, we repeatedly expand $q_π$ and use the
assumption of the policy improvement theorem:

~~~
$\displaystyle \qquad v_π(s) ≤ q_π(s, π'(s))$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼[R_{t+1} + γ v_π(S_{t+1}) | S_t = s, A_t = π'(s)]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ v_π(S_{t+1}) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} ≤ 𝔼_{π'}[R_{t+1} + γ q_π(S_{t+1}, π'(S_{t+1})) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ 𝔼[R_{t+2} + γ v_π(S_{t+2}) | S_{t+1}, A_{t+1} = π'(S_{t+1})] | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} = 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 v_π(S_{t+2}) | S_t = s]$

~~~
$\displaystyle \qquad \phantom{v_π(s)} …$

~~~
$\displaystyle \qquad \phantom{v_π(s)} ≤ 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 R_{t+3} + … | S_t = s] = v_{π'}$

---
# Policy Improvement Example

![w=50%](gridworld_4x4.svgz)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.svgz)

---
# Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$π_0 \stackrel{E}{\longrightarrow} v_{π_0} \stackrel{I}{\longrightarrow}
  π_1 \stackrel{E}{\longrightarrow} v_{π_1} \stackrel{I}{\longrightarrow}
  π_2 \stackrel{E}{\longrightarrow} v_{π_2} \stackrel{I}{\longrightarrow}
  … \stackrel{I}{\longrightarrow} π_* \stackrel{E}{\longrightarrow} v_{π_*}.$$

~~~
The result is a sequence of monotonically improving policies $π_i$. Note that
when $π' = π$, also $v_{π'} = v_π$, which means Bellman optimality equation is
fulfilled and both $v_π$ and $π$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluating policy $π_{k+1}$, we usually start with $v_{π_k}$,
which is assumed to be a good approximation to $v_{π_{k+1}}$.

---
# Policy Iteration Algorithm
![w=70%,h=center](policy_iteration.svgz)

---
# Value Iteration as Policy Iteration

Note that value iteration is in fact a policy iteration, where policy evaluation
is performed only for one step:

$$\begin{aligned}
  π'(s) &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] &\textit{(policy improvement)} \\
  v'(s) &= ∑\nolimits_a π'(a|s) ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] &\textit{(one step of policy evaluation)}
\end{aligned}$$

Substituting the former into the latter, we get
$$v'(s) = \max_a ∑\nolimits_{s', r} p(s', r | s, a) \big[r + γ v(s')\big] = Bv(s).$$

---
# Generalized Policy Iteration

Therefore, it seems that to achieve convergence, it is not necessary to perform
the policy evaluation exactly.

**Generalized Policy Evaluation** is a general concept of interleaving policy
evaluation and policy improvement at various granularity.

~~~
![w=30%,mw=50%,h=center](gpi.svgz)![w=80%,mw=50%,h=center](gpi_convergence.svgz)

If both processes stabilize, we know we have obtained optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework. Keeping estimated returns for the action-value function, we perform
policy evaluation by sampling one episode according to current policy. We then
update the action-value function by averaging over the observed returns,
including the currently sampled episode.

---
# Monte Carlo Methods

To hope for convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
~~~
- _first visit_: only the first occurence of a state-action pair in an episode is
  considered
- _every visit_: all occurences of a state-action pair are considered.

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.svgz)


---
# Monte Carlo and $ε$-soft Policies

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
A policy is called $ε$-soft, if
$$π(a|s) ≥ \frac{ε}{|𝓐(s)|}.$$
and we call it $ε$-greedy, if one action has a maximum probability of
$1-ε+\frac{ε}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $ε$-soft
policies, and using $ε$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $ε$-soft behaviour
“inside” the environment and prove equivalence.)

---
# Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{t+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$
