title: NPFL122, Lecture 2
class: title
# Markov Decision Process, Optimal Solutions, Monte Carlo Methods

## Milan Straka

### October 15, 2018

---
section: MDP Definition
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a _reward_ $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a _discount factor_.

~~~
Let a _return_ $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# (State-)Value and Action-Value Functions

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_Ï€(s)$, or
_state-value function_, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_Ï€[q_Ï€(s, a)], \\
  q_Ï€(s, a) &= ğ”¼_Ï€[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an _optimal policy_.

~~~
## Existence
Under some mild assumptions, there always exists a unique optimal state-value function,
unique optimal action-value function, and (not necessarily unique) optimal policy.
The mild assumptions are that either termination is guaranteed from all
reachable states, or $Î³ < 1$.

---
section: Dynamic Programming
# Dynamic Programming

Dynamic programming is an approach devised by Richard Bellman in 1950s.

~~~
To apply it to MDP, we now consider finite-horizon problems (i.e., with episodes
of bounded length) with finite number of states $ğ“¢$ and actions $ğ“$, and known
MDP dynamics $p$.

~~~
The following recursion, which must obviously hold for an optimal value function,
is usually called the _Bellman equation_:
$$\begin{aligned}
  v_*(s) &= \max_a ğ”¼\left[R_{t+1} + Î³ v_*(S_{t+1}) \middle| S_t=s, A_t=a\right] \\
         &= \max_a âˆ‘_{s', r} p(s', r | s, a) \left[r + Î³ v_*(s')\right].
\end{aligned}$$

---
# Dynamic Programming

To turn the Bellman equation into an algorithm, we change the equal signs to assignments:
$$\begin{aligned}
v_0(s) &â† 0 \\
v_{k+1}(s) &â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].
\end{aligned}$$

~~~
It is easy to show that if the problem consists of episodes of length at most
$T$ steps, the optimal value function is reached after $T$ iteration of the
above assignment (we can show by induction that $v_k(s)$ is the maximum return
reachable from state $s$ in $k$ steps).

---
# Relations to Graph Algorithms

Searching for optimal value functions of deterministic problems is in fact
search for shortest path in a suitable graph.

~~~
![w=80%,mh=80%,h=center,v=middle](trellis.svg)

---
# Bellman-Ford-Moore Algorithm

$$v_{k+1}(s) â† \max_a ğ”¼\left[R_{t+1} + Î³ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

Bellman-Ford-Moore algorithm:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices: d[v] = 0 if v == s else +âˆ

for i in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```

---
section: Value Iteration
# Bellman Backup Operator

Our goal is now to handle also infinite horizon tasks, using discount factor of
$Î³ < 1$.

~~~
For any value function $vâˆˆâ„^{|ğ“¢|}$ we define _Bellman backup operator_ $B : â„^{|ğ“¢|} â†’ â„^{|ğ“¢|}$ as
$$Bv(s) â‰ \max_a ğ”¼\left[R_{t+1} + Î³ v(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

~~~
It is not difficult to show that Bellman backup operator is a _contraction_:
$$\max_s \left|Bv_1(s) - Bv_2(s)\right| â‰¤ Î³ \max_s \left|v_1(s) - v_2(s)\right|.$$

~~~
Considering a normed vector space $â„^{|ğ“¢|}$ with sup-norm $||â‹…||_âˆ$,
from Banach fixed-point theorem it follows there exist a _unique value function_
$v_*$ such that
$$Bv_* = v_*.$$

~~~
That unique $v_*$ is _optimal value function_, because Bellman backup operator
does not change it and $v_*$ is the only such value function.

---
# Bellman Backup Operator

Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,
because
$$||Bv - v_*||_âˆ = ||Bv - Bv_*||_âˆ â‰¤ Î³||v - v_*||,$$
and therefore $B^nv â†’ v_*$.

---
# Value Iteration Algorithm

We can turn the iterative application of Bellman backup operator into an
algorithm.
$$Bv(s) â‰ \max_a ğ”¼\left[R_{t+1} + Î³ v(S_{t+1}) \middle| S_t=s, A_t=a\right]$$

![w=75%,h=center](value_iteration.pdf)

---
# Value Iteration Algorithm

Although we have described the so-called _synchronous_ implementation requiring
two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify
the value function in place (if a fixed ordering is used, usually such value
iteration is called _Gauss-Seidel_).

~~~
Even with such asynchronous update value iteration can be proven to converge,
and usually performs better in practise.

---
# Bellman Backup Operator as a Contraction

To show that Bellman backup operator is a contraction, we proceed as follows:
$$\begin{aligned}
||Bv_1 - Bv_2||_âˆ &= ||\max_a ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - \max_a ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ \\
                  &â‰¤ \max_a\left( || ğ”¼\left[R_{t+1} + Î³ v_1(S_{t+1})\right] - ğ”¼\left[R_{t+1} + Î³ v_2(S_{t+1})\right]||_âˆ\right) \\
                  &= \max_a\left( \left|\left| âˆ‘\nolimits_{s', r} p\left(s', r \middle| s, a\right)Î³(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &= Î³ \max_a\left(\left|\left| âˆ‘\nolimits_{s', r} p\left(s' \middle| s, a\right)(v_1(s') - v_2(s'))\right|\right|_âˆ\right) \\
                  &â‰¤ Î³ ||v_1 - v_2||_âˆ,
\end{aligned}$$

where the second line follows from $|\max_x f(x) - \max_x g(x)| â‰¤ \max_x |f(x) - g(x)|$
and the last line from the fact that from any given $s$ and $a$, the
$âˆ‘_{s'} p(s' | s, a)$ sums to 1.

---
# Speed of Convergence

Assuming maximum reward is $R_\textrm{max}$, we have that
$$v_*(s) â‰¤ âˆ‘_{t=0}^âˆ Î³^t R_\textrm{max} = \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Starting with $v(s) â† 0$, we have
$$||B^k v - v_*||_âˆ â‰¤ Î³^k ||v - v_*||_âˆ = Î³^k \frac{R_\textrm{max}}{1-Î³}.$$

~~~
Compare to finite horizon case, where $B^T v = v_*$.

---
section: Policy Iteration
# Policy Iteration Algorithm

We now propose another approach of computing optimal policy. The approach,
called _policy iteration_, consists of repeatedly performing policy
_evaluation_ and policy _improvement_.

## Policy Evaluation

Given a policy $Ï€$, policy evaluation computes $v_Ï€$.

Recall that
$$\begin{aligned}
  v_Ï€(s) &â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] \\
         &= ğ”¼_Ï€\left[R_{t+1} + Î³ v_Ï€(S_{t+1}) \middle | S_t = s\right] \\
         &= âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

If the dynamics of the MDP $p$ is known, the above is a system of linear
equations, and therefore, $v_Ï€$ can be computed exactly.

---
# Policy Evaluation
The equation
$$v_Ï€(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right]$$
is called _Bellman equation for $v_Ï€$_ and analogously to Bellman optimality
equation, it can be proven that
- under the same assumptions as before ($Î³<1$ or termination), $v_Ï€$ exists and
  is unique;
- $v_Ï€$ is a fixed point of the Bellman equation
  $$v_{k+1}(s) = âˆ‘\nolimits_a Ï€(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_k(s')\right];$$
- iterative application of the Bellman equation to any $v$ converges to $v_Ï€$.

---
class: middle
# Policy Evaluation

![w=100%](policy_evaluation.pdf)

---
# Policy Improvement

Given $Ï€$ and computed $v_Ï€$, we would like to _improve_ the policy.
A straightforward way to do so is to define a policy using a _greedy_ action
$$\begin{aligned}
  Ï€'(s) &â‰ \argmax_a q_Ï€(s, a) \\
        &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v_Ï€(s')\right].
\end{aligned}$$

For such $Ï€'$, we can easily show that
$$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s).$$

---
# Policy Improvement Theorem

Let $Ï€$ and $Ï€'$ be any pair of deterministic policies, such that
$q_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s)$.

Then for all states $s$, $v_{Ï€'}(s) â‰¥ v_Ï€(s)$.

~~~
The proof is straightforward, we repeatedly expand $q_Ï€$ and use the
assumption of the policy improvement theorem:

$$\begin{aligned}
v_Ï€(s) &â‰¤ q_Ï€(s, Ï€'(s)) \\
       &= ğ”¼[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s, A_t = Ï€'(s)] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ v_Ï€(S_{t+1}) | S_t = s] \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ q_Ï€(S_{t+1}, Ï€'(S_{t+1})) | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ E[R_{t+2} + Î³ v_Ï€(S_{t+2}) | S_{t+1}, A_{t+1} = Ï€'(S_{t+1})] | S_t = s] \\
       &= ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 v_Ï€(S_{t+2}) | S_t = s] \\
       &â€¦ \\
       &â‰¤ ğ”¼_{Ï€'}[R_{t+1} + Î³ R_{t+2} + Î³^2 R_{t+3} + â€¦ | S_t = s] = v_{Ï€'}(s)
\end{aligned}$$

---
# Policy Improvement Example

![w=50%](gridworld_4x4.pdf)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.pdf)

---
# Policy Iteration Algorithm

Policy iteration consists of repeatedly performing policy evaluation and policy
improvement:
$$Ï€_0 \stackrel{E}{\longrightarrow} v_{Ï€_0} \stackrel{I}{\longrightarrow}
  Ï€_1 \stackrel{E}{\longrightarrow} v_{Ï€_1} \stackrel{I}{\longrightarrow}
  Ï€_2 \stackrel{E}{\longrightarrow} v_{Ï€_2} \stackrel{I}{\longrightarrow}
  â€¦ \stackrel{I}{\longrightarrow} Ï€_* \stackrel{E}{\longrightarrow} v_{Ï€_*}.$$

~~~
The result is a sequence of monotonically improving policies $Ï€_i$. Note that
when $Ï€' = Ï€$, also $v_{Ï€'} = v_Ï€$, which means Bellman optimality equation is
fulfilled and both $v_Ï€$ and $Ï€$ are optimal.

~~~
Considering that there is only a finite number of policies, the optimal policy
and optimal value function can be computed in finite time (contrary to value
iteration, where the convergence is only asymptotic).

~~~
Note that when evaluation policy $Ï€_{k+1}$, we usually start with $v_{Ï€_k}$,
which is assumed to be a good approximation to $v_{Ï€_{k+1}}$.

---
# Policy Iteration Algorithm
![w=70%,h=center](policy_iteration.pdf)

---
# Value Iteration as Policy Iteration

Note that value iteration is in fact a policy iteration, where policy evaluation
is performed only for one step:

$$\begin{aligned}
  Ï€'(s) &= \argmax_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(policy improvement)} \\
  v'(s) &= âˆ‘\nolimits_a Ï€'(a|s) âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s')\right] &\textit{(one step of policy evaluation)}
\end{aligned}$$

Substituting the former into the latter, we get
$$v'(s) = \max_a âˆ‘\nolimits_{s', r} p(s', r | s, a) \left[r + Î³ v(s)\right] = Bv(s).$$

---
# Generalized Policy Iteration

Therefore, it seems that to achieve convergence, it is not necessary to perform
policy evaluation exactly.

_Generalized Policy Evaluation_ is a general idea of interleaving policy
evaluation and policy improvement at various granularity.

~~~
![w=30%,mw=50%,h=center](gpi.pdf)![w=80%,mw=50%,h=center](gpi_convergence.pdf)

If both processes stabilize, we know we have obtained optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

We now present first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for action-value function $q$ instead of $v$.

~~~
We can formulate Monte Carlo methods in the generalized policy improvement
framework.

~~~
Keeping estimated returns for action-value function, we perform policy
evaluation by sampling one episode according to current policy. We then update
action-value function by averaging over observed returns, including the sampled
episode.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly selects the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple time in one episode, the
sampled returns are not independent. The literature distinguish two cases:
- _first visit_: only first occurence of a state-action pair in an episode is;
  considered
- _every visit_: all occurences of a state-action pair is considered.

Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.pdf)


---
# Monte Carlo and $Îµ$-soft Policies

A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$

~~~
For $Îµ$-soft policy, Monte Carlo policy evaluation also converges, without need
of expoding starts.

~~~
We call a policy $Îµ$-greedy, if one action has maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for class of $Îµ$-soft
policies, and using<br>$Îµ$-greedy policy in policy improvement step, policy
iteration has same convergence properties. (We can embed the $Îµ$-soft behaviour
â€œinsideâ€ the environment and prove equivalence.)

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{T+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$


