title: NPFL122, Lecture 2
class: title, langtech, cc-by-nc-sa
# Markov Decision Process, Optimal Solutions, Monte Carlo Methods

## Milan Straka

### October 12, 2020

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](../01/diagram.svgz)

~~~~
# Markov Decision Process

![w=55%,h=center](../01/diagram.svgz)

A **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizont tasks** then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_Ï€[q_Ï€(s, a)], \\
  q_Ï€(s, a) &= ğ”¼_Ï€[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizont tasks or if $Î³ < 1$, there always exists a unique optimal
state-value function, unique optimal action-value function, and (not necessarily
unique) optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
Keeping estimated returns for the action-value function, we evaluate
the current policy by sampling one episode while using it. We then update
the action-value function by averaging over the observed returns, including
the currently sampled episode.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume **exploring starts**, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
~~~
- **first visit**: only the first occurence of a state-action pair in an episode is
  considered
- **every visit**: all occurences of a state-action pair are considered.

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](../01/monte_carlo_exploring_starts.svgz)


---
# Monte Carlo and $Îµ$-soft Policies

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$
and we call it $Îµ$-greedy, if one action has a maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
For $Îµ$-soft policy, Monte Carlo policy evaluation also converges, without the need
of exploring starts.

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{t+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: POMDP
# Partially Observable MDPs

Recall that a MDP is a quadruple $(ğ“¢, ğ“, p, Î³)$, where:
- $ğ“¢$ is a set of states,
- $ğ“$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a reward $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a discount factor.

~~~
**Partially observable Markov decision process** (POMDP) extends the Markov decision
process to a sextuple $(ğ“¢, ğ“, p, Î³, ğ“, o)$, where in addition to an MDP:
- $ğ“$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model.

and agents are provided only with an observation $O_t$ instead of state $S_t$.

~~~
In robotics (out of the domain of this course), several approaches are used to
handle POMDPs, to model uncertainty, imprecise mechanisms and inaccurate
sensors.
