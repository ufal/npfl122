title: NPFL122, Lecture 8
class: title, langtech, cc-by-nc-sa
# Advantage Actor-Critic, Continuous Action Space

## Milan Straka

### December 3, 2018

---
section: Refresh
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(â†’Î¸) â‰ ð”¼_{h, Ï€} v_Ï€(s)$. To compute the gradient
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆð“¢} Î¼(s) âˆ‘_{a âˆˆ ð“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$
REINFORCE algorithm estimates the $q_Ï€(s, a)$ by a single sample.

![w=75%,h=center](../07/reinforce.pdf)

---
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆð“¢} Î¼(s) âˆ‘_{a âˆˆ ð“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$v_Ï€(s) = ð”¼_{a âˆ¼ Ï€} q_Ï€(s, a)$. Then, better-than-average returns are positive
and worse-than-average returns are negative.

The resulting value is also called an _advantage function_
$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s)$.

Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](../07/reinforce_with_baseline.pdf)

---
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called _actor-critic_ methods.

The idea is straightforward â€“ instead of estimating the episode return using the
whole episode rewards, we can use $n$-step temporal difference estimation.

![w=64%,h=center](../07/actor_critic.pdf)

---
section: A3C
# Asynchronous Methods for Deep RL

A 2015 paper from Volodymyr Mnih et al., the same group as DQN.

~~~
The authors propose an asynchronous framework, where multiple workers share one
neural network, each training using either an off-line or on-line RL algorithm.

~~~
They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C
(an _asynchronous advantage actor-critic_ method). For A3C, they compare
a version with and without LSTM.

~~~
The authors also introduce _entropy regularization term_ $Î² H(Ï€(s; â†’Î¸))$ to the
loss to support exploration and discourage premature convergence.

---
# Asynchronous Methods for Deep RL

![w=45%,h=center](asynchronous_q_learning.pdf)

---
# Asynchronous Methods for Deep RL

![w=68%,h=center](asynchronous_q_learning_nstep.pdf)

---
# Asynchronous Methods for Deep RL

![w=68%,h=center](a3c.pdf)

---
# Asynchronous Methods for Deep RL

![w=100%](a3c_performance.pdf)
![w=85%,mw=50%,h=center](a3c_performance_table.pdf)![w=50%](a3c_speedup.pdf)

---
# Asynchronous Methods for Deep RL

![w=85%,h=center](a3c_data_efficiency_episodes.pdf)

---
# Asynchronous Methods for Deep RL

![w=85%,h=center](a3c_data_efficiency_time.pdf)

---
# Asynchronous Methods for Deep RL

![w=100%,v=middle](a3c_learning_rates.pdf)

---
section: PAAC
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workes to only generate episodes. Such approach
was described in May 2017 by Celemente et al., who named their agent
_parallel advantage actor-critic_ (PAAC).

![w=70%,h=center](paac_framework.pdf)

---
# Parallel Advantage Actor Critic

![w=85%,h=center](paac_algorithm.pdf)

---
# Parallel Advantage Actor Critic

![w=70%,h=center](paac_performance.pdf)

The authors use $8$ workers, $n_e=32$ parallel environments, $5$-step returns,
$Î³=0.99$, $Îµ=0.1$, $Î²=0.01$ and a learning rate of $Î±=0.0007â‹…n_e=0.0224$.

The $\textrm{arch}_\textrm{nips}$ is from A3C: 16 filters $8Ã—8$ stride 4, 32
filters $4Ã—4$ stride 2, a fully connected layer with 256 units. The
$\textrm{arch}_\textrm{nature}$ is from DQN: 32 filters $8Ã—8$ stride 4, 64
filters $4Ã—4$ stride 2, 64 filters $3Ã—3$ stride 1 and 512-unit fully connected
layer. All nonlinearities are ReLU.

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_epochs.pdf)

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_time.pdf)

---
# Parallel Advantage Actor Critic

![w=100%,v=middle](paac_time_usage.pdf)

---
section: Continuous Action Space
# Continuous Action Space

Until now, the actions were discreet. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b âˆˆ â„$, or more generally from a Cartesian product
of several such ranges:
$$âˆ_i [a_i, b_i].$$

~~~
![w=40%,f=right](normal_distribution.pdf)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $Î¼$ and variance $Ïƒ^2$, probability density function of $ð“(Î¼, Ïƒ^2)$
is
$$p(x) â‰ \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} e^{\large-\frac{(x - Î¼)^2}{2Ïƒ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution we suitably parametrize the action value,
usually using the normal distribution. Considering only one real-valued action,
we therefore have
$$Ï€(a | s; â†’Î¸) â‰ P\Big(a âˆ¼ ð“\big(Î¼(s; â†’Î¸), Ïƒ(s; â†’Î¸)^2\big)\Big),$$
where $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ are function approximation of mean and standard
deviation of the action distribution.

The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a regular regression (i.e., one output neuron
  without activation);
- the standard variance (which must be positive) being computed again as
  a regression, followed most commonly by either $\exp$ or
  $\operatorname{softplus}$, where $\operatorname{softplus}(x) â‰ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

~~~
```python
  mu = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.exp(log_sd)   # or sd = tf.nn.softplus(sd)

  normal_dist = tf.distributions.Normal(mu, sd)

  # Loss computed as - log Ï€(a|s) - entropy_regularization
  loss = - normal_dist.log_prob(self.actions) * self.returns \
         - args.entropy_regularization * normal_dist.entropy()
```

---
# Continuous Action Space

When the action consists of several real values, i.e., action is a suitable
subregion of $â„^n$ for $n>1$, we can:
- either use multivariate Gaussian distribution;
- or factorize the probability into a product of univariate normal
  distributions.

~~~
Modeling the action distribution using a single normal distribution might be
insufficient, in which case a mixture of normal distributions is usually used.

~~~
Sometimes, the continuous action space is used even for discrete output -- when
modeling pixels intensities (256 values) or sound amplitude (2$^{16}$ values),
instead of a softmax we use discretized mixture of distributions,
usually $\operatorname{logistic}$ (a distribution with a sigmoid cdf). Then,
$$Ï€(a) = âˆ‘_i p_i\Big(Ïƒ\big((a + 0.5 - Î¼_i) / Ïƒ_i\big) - Ïƒ\big((a - 0.5 - Î¼_i) / Ïƒ_i\big) \Big).$$
However, such mixtures are usually used in generative modeling, not in
reinforcement learning.
