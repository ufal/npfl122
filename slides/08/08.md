title: NPFL122, Lecture 8
class: title, langtech, cc-by-nc-sa
# SAC, Eligibility Traces

## Milan Straka

### November 21, 2022

---
section: SAC
# Soft Actor Critic

The paper _Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor_ by Tuomas Haarnoja et al. from Jan 2018
introduces a different off-policy algorithm for continuous action space.

~~~
It was followed by a continuation paper _Soft Actor-Critic Algorithms and
Applications_ in Dec 2018.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize, instead of just ad-hoc adding the entropy penalty. Such an approach
is an instance of _regularized policy optimization_.

---
# Soft Actor Critic Objective

Until now, our goal was to optimize
$$ğ”¼_Ï€ \big[G_0\big].$$

~~~
Assume the rewards are deterministic and that $Î¼_Ï€$ is on-policy distribution of
a policy $Ï€$.

In the soft actor-critic, the authors instead propose to optimize the maximum
entropy objective
$$\begin{aligned}
Ï€_* &= \argmax_Ï€ ğ”¼_{sâˆ¼Î¼_Ï€} \Big[ğ”¼_{aâˆ¼Ï€(s)}\big[r(s, a)\big] + Î± H(Ï€(â‹…|s))\Big] \\
    &= \argmax_Ï€ ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€(s)}\big[r(s, a) - Î± \log Ï€(a|s)\big].
\end{aligned}$$

~~~
Note that the value of $Î±$ is dependent on the magnitude of returns and that
for a fixed policy, the entropy penalty can be â€œhiddenâ€ in the reward.

---
# Soft Actor Critic Objective

To maximize the regularized objective, we define the following augmented reward:
$$r_Ï€(s, a) â‰ r(s, a) + ğ”¼_{s' âˆ¼ p(s, a)} \big[Î± H(Ï€(â‹…|s'))\big].$$

~~~
From now on, we consider **soft action-value** function corresponding to this
augmented reward.

---
section: SPE
# Soft Policy Evaluation

Our goal is now to derive **soft policy iteration**, an analogue of policy iteration algorithm.

~~~
We start by considering soft policy evaluation. Let a modified Bellman backup
operator $ğ“£_Ï€$ be defined as
$$ğ“£_Ï€ q(s, a) â‰ r(s, a) + Î³ ğ”¼_{s' âˆ¼ p(s, a)} \big[v(s')\big],$$
where the **soft (state-)value** function $v(s)$ is defined as
$$
v(s) = ğ”¼_{a âˆ¼ Ï€} \big[q(s, a)\big] + Î±H(Ï€(â‹…|s)) = ğ”¼_{a âˆ¼ Ï€} \big[q(s, a) - Î± \log Ï€(a|s)\big].$$

~~~
This modified Bellman backup operator corresponds to the usual one for the
augmented rewards $r_Ï€(s, a)$, and therefore the repeated application
$ğ“£_Ï€^k q$ converges to $q_Ï€$ according to the original proof.

---
section: SPI
# Soft Policy Improvement

While the soft policy evaluation was a straightforward modification of the
original policy evaluation, the soft policy improvement is quite different.

~~~
Assume we have a policy $Ï€$, its action-value function $q_Ï€$ from the soft
policy evaluation, and we want to improve the policy. Furthermore, we should
select the improved policy from a family of parametrized distributions $Î $.

~~~
We define the improved policy $Ï€'$ as
$$Ï€'(â‹…|s) â‰ \argmin_{Ï€Ì„ âˆˆ Î } J_Ï€(Ï€Ì„) â‰ \argmin_{Ï€Ì„ âˆˆ Î } D_\textrm{KL}\Bigg( Ï€Ì„(â‹…|s) \Bigg\| \frac{\exp\big(\frac{1}{Î±} q_Ï€(s, â‹…)\big)}{z_Ï€(s)} \Bigg),$$
where $z_Ï€(s)$ is the partition function (i.e.,  normalization factor such that
the right-hand side is a distribution), which does not depend on the new policy
and thus can be ignored.

---
# Soft Policy Improvement

We now prove that $q_{Ï€'}(s, a) â‰¥ q_Ï€(s, a)$ for any state $s$ and action $a$.

~~~
We start by noting that $J_Ï€(Ï€') â‰¤ J_Ï€(Ï€)$, because we can always choose $Ï€$ as
the improved policy.
~~~
Therefore,
$$ğ”¼_{aâˆ¼Ï€'} \big[Î±\log Ï€'(a|s) - q_Ï€(s, a) + \textcolor{gray}{Î±\log z_Ï€(s)}\big] â‰¤
  ğ”¼_{aâˆ¼Ï€} \big[Î±\log Ï€(a|s) - q_Ï€(s, a) + \textcolor{gray}{Î±\log z_Ï€(s)}\big],$$

~~~
which results in
$$ğ”¼_{aâˆ¼Ï€'} \big[q_Ï€(s, a) - Î±\log Ï€'(a|s)\big] â‰¥ v_Ï€(s).$$

~~~
We now finish the proof analogously to the original one:
$$\begin{aligned}
q_Ï€(s, a) &= r(s, a) + Î³ğ”¼_{s'}[v_Ï€(s')] \\
          &â‰¤ r(s, a) + Î³ğ”¼_{s'}[ğ”¼_{a'âˆ¼Ï€'} [q_Ï€(s', a') - Î±\log Ï€'(a'|s')] \\
          &â€¦ \\
          &â‰¤ q_{Ï€'}(s, a).
\end{aligned}$$

---
# Soft Policy Iteration

The soft policy iteration algorithm alternates between the soft policy
evaluation and soft policy improvement steps.

~~~
The repeated application of these two steps produce better and better policies.
In other words, we get a monotonically increasing sequence of soft action-value
functions.

~~~
If the soft action-value function is bounded (the paper assumes
a bounded reward and a finite number of actions to bound the entropy), the
repeated application converges to some $q_*$, from which we get a $Ï€_*$ using
the soft policy improvement step. (It is not clear to me why the algorithm
should converge in finite time, but we can make the rest of the slide
conditional on â€œif the algorithm convergesâ€).

~~~
It remains to show that the $Ï€_*$ is indeed the optimal policy
fulfilling $q_{Ï€_*}(s, a) â‰¥ q_Ï€(s, a)$.

~~~
However, this follows from the fact that at convergence,
$J_{Ï€_*}(Ï€_*) â‰¤ J_{Ï€_*}(Ï€)$, and following the same reasoning as in the proof of
the soft policy improvement, we obtain the required $q_{Ï€_*}(s, a) â‰¥ q_Ï€(s, a)$.

---
# Soft Policy Improvement Derivation

The following derivation is not in the original paper, but it is my
understanding of how the softmax of the action-value function arises.
For simplicity, we assume finite number of actions.

~~~
Assuming we have a policy $Ï€$ and its action-value function $q_Ï€$,
we usually improve the policy using
$$\begin{aligned}
  Î½(â‹…|s)
  &= \argmax_Î½ ğ”¼_{aâˆ¼Î½(â‹…|s)} \big[q_Ï€(s, a)\big] \\
  &= \argmax_Î½ âˆ‘\nolimits_a q_Ï€(s, a) Î½(a|s) \\
  &= \argmax_Î½ â†’q_Ï€(s, â‹…)^T â†’Î½(â‹…|s), \\
\end{aligned}$$

~~~
which results in a greedy improvement with the form of
$$Î½(s) = \argmax\nolimits_a q_Ï€(s, a).$$

---
# Soft Policy Improvement Derivation

Now consider instead the regularized objective
$$\begin{aligned}
  Î½(â‹…|s)
  &= \argmax_Î½ \big( ğ”¼_{aâˆ¼Î½(â‹…|s)} \big[q_Ï€(s, a)\big] + Î±H(Î½(â‹…|s))\big) \\
  &= \argmax_Î½ \big(ğ”¼_{aâˆ¼Î½} \big[q_Ï€(s, a) - Î± \log Î½(a|s)\big]\big)
\end{aligned}$$

~~~
To maximize it for a given $s$, we form a Lagrangian
$$ğ“› = \Big(âˆ‘\nolimits_a Î½(a|s) \big(q_Ï€(s, a) - Î±\log Î½(a|s)\big)\Big) - Î»\Big(1 - âˆ‘\nolimits_a Î½(a|s)\Big).$$

~~~
The derivative with respect to $Î½(a|s)$ is
$$\frac{âˆ‚ğ“›}{âˆ‚Î½(a|s)} = q_Ï€(s, a) - Î±\log Î½(a|s) - Î± + Î».$$

~~~
Setting it to zero, we get $Î±\log Î½(a|s) = q_Ï€(s, a) + Î» - Î±$, resulting in $Î½(a|s) âˆ e^{\frac{1}{Î±} q_Ï€(s, a)}$.

---
section: SACAlgorithm
# Soft Actor Critic

Our soft actor critic will be an off-policy algorithm with continuous action
space. The model consist of two critics $q_{â†’Î¸_1}$ and $q_{â†’Î¸_2}$, two target
critics $q_{â†’Î¸Ì„_1}$ and $q_{â†’Î¸Ì„_2}$ and finally a single actor $Ï€_{â†’Ï†}$.

~~~
The authors state that
- with a single critic, all the described experiments still converge;
~~~
- they adopted the two critics from the TD3 paper;
~~~
- using two critics â€œsignificantly speed up trainingâ€.

---
# Soft Actor Critic â€“ Critic Training

To train the critic, we use the modified Bellman backup operator, resulting in
the loss
$$J_q(â†’Î¸_i) = ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€_{â†’Ï†}(s)} \Big[\big(q_{â†’Î¸_i}(s, a) - \big(r(s, a) + Î³ ğ”¼_{s' âˆ¼ p(s, a)} [v_\textrm{min}(s')]\big)\big)^2\Big],$$

~~~
where
$$v_\textrm{min}(s) = ğ”¼_{aâˆ¼Ï€_{â†’Ï†}(s)} \Big[\min_i\big(q_{â†’Î¸Ì„_i}(s, a) \big) - Î± \log Ï€_{â†’Ï†}(a | s)\Big].$$

~~~
The target critics are updated using exponentiation moving averages with
momentum $Ï„$.

---
# Soft Actor Critic â€“ Actor Training

The actor is updated by directly minimizing the KL divergence, resulting in the
loss
$$J_Ï€(â†’Ï†) = ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€_{â†’Ï†}(s)}\Big[Î± \log\big(Ï€_{â†’Ï†}(a, s)\big) - \min_i\big(q_{â†’Î¸_i}(s, a)\big)\Big].$$

~~~
Given that our critics are differentiable, we now reparametrize the policy as
$$a = f_{â†’Ï†}(s, Îµ).$$

~~~
Specifically, we sample $Îµ âˆ¼ ğ“(0, 1)$ and let $f_{â†’Ï†}$ produce an unbounded
Gaussian distribution (a diagonal one if the actions are vectors).

~~~
Together, we obtain
$$J_Ï€(â†’Ï†) = ğ”¼_{sâˆ¼Î¼_Ï€, Îµâˆ¼ğ“(0, 1)}\Big[Î± \log\big(Ï€_{â†’Ï†}(f_{â†’Ï†}(s, Îµ), s)\big) - \min_i\big(q_{â†’Î¸_i}(s, f_{â†’Ï†}(s, Îµ))\big)\Big].$$

---
# Soft Actor Critic â€“ Bounding Actions

In practice, the actions need to be bounded.

~~~
The authors propose to apply an invertible squashing function $\tanh$
on the unbounded Gaussian distribution.

~~~
Consider that our policy produces an unbounded action $Ï€(u | s)$.
To define a distribution $Ï€Ì„(a | s)$ with $a = \tanh(u)$, we need to employ
the change of variables, resulting in
$$Ï€Ì„(a | s) = Ï€(u | s) \bigg(\frac{âˆ‚a}{âˆ‚u}\bigg)^{-1} = Ï€(u | s) \bigg(\frac{âˆ‚\tanh(u)}{âˆ‚u}\bigg)^{-1}.$$

~~~
Therefore, the log-likelihood has quite a simple form
$$\log Ï€Ì„(a | s) = \log Ï€(u | s) - \log\big(1 - \tanh^2(u)\big).$$

---
# Soft Actor Critic â€“ Automatic Entropy Adjustment

One of the most important hyperparameters is the entropy penalty $Î±$.

~~~
In the second paper, the authors presented an algorithm for automatic adjustment
of its value.

~~~
Instead of setting the entropy penalty $Î±$, they propose to specify target
entropy value $ğ“—$ and then solve a constrained optimization problem
$$Ï€_* = \argmax_Ï€ ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€(s)} \big[r(s, a)\big]\textrm{~~such that~~}ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€(s)}\big[-\log Ï€(a | s)\big] â‰¥ ğ“—.$$

~~~
We can then form a Lagrangian with a multiplier $Î±$
$$ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€(s)} \Big[r(s, a) + Î±\big(-\log Ï€(a | s) - ğ“—\big)\Big],$$
which should be maximized with respect to $Ï€$ and minimized with respect
to $Î± â‰¥ 0$.

---
# Soft Actor Critic â€“ Automatic Entropy Adjustment

To optimize the Lagrangian, we perform _dual gradient descent_, where we
alternate between maximization with respect to $Ï€$ and minimization with respect
to $Î±$.

~~~
While such a procedure is guaranteed to converge only under the convexity
assumptions, the authors report that the dual gradient descent works in practice
also with nonlinear function approximation.

~~~
To conclude, the automatic entropy adjustment is performed by introducing
a final loss
$$J(Î±) = ğ”¼_{sâˆ¼Î¼_Ï€, aâˆ¼Ï€(s)} \big[-Î± \log Ï€(a | s) - Î± ğ“—\big].$$

---
# Soft Actor Critic
![w=93%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=93%,h=center](sac_hyperparameters.svgz)

---
# Soft Actor Critic
![w=86%,h=center](sac_results.svgz)

---
# Soft Actor Critic
![w=100%,v=middle](sac_ablations.svgz)

---
section: ControlVariates
# Off-policy Correction Using Control Variates

Let $G_{t:t+n}$ be the estimated $n$-step return
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + \Big[\textrm{episode still running in }t+n\Big] Î³^n V(S_{t+n}),$$

~~~
which can be written recursively as
$$G_{t:t+n} \begin{cases}
  0 & \mathrm{if~episode~ended~before~}t, \\
  V(S_t) & \mathrm{if~}n=0, \\
  R_{t+1} + Î³ G_{t+1:t+n} & \mathrm{otherwise}.
\end{cases}$$

~~~
For simplicity, we do not explicitly handle the first case (â€œthe episode
has already endedâ€) in the following.

---
# Off-policy Correction Using Control Variates

Note that we can write
$$\begin{aligned}
G_{t:t+n} - V(S_t)
  &= R_{t+1} + Î³ G_{t+1:t+n} - V(S_t) \\
  &= R_{t+1} + Î³ \big(G_{t+1:t+n} - V(S_{t+1})\big) + Î³V(S_{t+1}) - V(S_t),
\end{aligned}$$

~~~
which yields
$$G_{t:t+n} - V(S_t) = R_{t+1} + Î³V(S_{t+1}) - V(S_t) + Î³\big(G_{t+1:t+n} - V(S_{t+1})\big).$$

~~~
Denoting the TD error as $Î´_t â‰ R_{t+1} + Î³V(S_{t+1}) - V(S_t)$, we can
therefore write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i Î´_{t+i}.$$

---
# Off-policy Correction Using Control Variates

Now consider applying the IS off-policy correction to $G_{t:t+n}$ using the
importance sampling ratio
$$Ï_t â‰ \frac{Ï€(A_t | S_t)}{b(A_t | S_t)},~~~Ï_{t:t+n} â‰ âˆ_{i=0}^n Ï_{t+i}.$$

~~~
First note that
$$ğ”¼_{A_t âˆ¼ b} \big[Ï_t\big] = âˆ‘_{A_t} b(A_t | S_t) \frac{Ï€(A_t | S_t)}{b(A_t | S_t)} = 1,$$

~~~
which can be extended to
$$ğ”¼_b \big[Ï_{t:t+n}\big] = 1.$$

---
# Off-policy Correction Using Control Variates

Until now, we used
$$G_{t:t+n}^\mathrm{IS} â‰ Ï_{t:t+n-1} G_{t:t+n}.$$

~~~
However, such correction has unnecessary variance. Notably, when expanding
$G_{t:t+n}$
$$G_{t:t+n}^\mathrm{IS} = Ï_{t:t+n-1} \big(R_{t+1} + Î³ G_{t+1:t+n}\big),$$

~~~
the $R_{t+1}$ depends only on $Ï_t$, not on $Ï_{t+1:t+n-1}$, and given that
the expectation of the importance sampling ratio is 1, we can simplify to
$$G_{t:t+n}^\mathrm{IS} = Ï_t R_{t+1} + Ï_{t:t+n-1} Î³ G_{t+1:t+n}.$$

~~~
Such an estimate can be written recursively as
$$G_{t:t+n}^\mathrm{IS} = Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{IS}\big).$$

---
# Off-policy Correction Using Control Variates

We can reduce the variance even further â€“ when $Ï_t=0$, we might consider
returning the value of $V(S_t)$ instead of 0.

~~~
Therefore, we might add another term, the so-called **control variate**, to the
estimate
$$G_{t:t+n}^\mathrm{CV} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t),$$
which is valid, since the expected value of $1-Ï_t$ is zero and $Ï_t$ and $S_t$
are independent.

~~~
Similarly as before, rewriting to
$$\begin{aligned}
G_{t:t+n}^\mathrm{CV} - V(S_t)
  &= Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) - Ï_tV(S_t) \\
  &= Ï_t \big(R_{t+1} + Î³ V(S_{t+1}) - V(S_t) + Î³ (G_{t+1:t+n}^\mathrm{CV} - V(S_{t+1}))\big)
\end{aligned}$$

~~~
results in
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Ï_{t:t+i} Î´_{t+i}.$$

---
section: EligibilityTraces
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $Î»$-return

For a given $Î» âˆˆ [0,1]$, we define **$Î»$-return** as
$$G_t^Î» â‰ (1 - Î») âˆ‘_{i=1}^âˆ Î»^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](traces_weighting.svgz)

~~~
Alternatively, the $Î»$-return can be written recursively as
$$\begin{aligned}
G_t^Î» &= (1 - Î») G_{t:t+1} \\
      &+ Î» (R_{t+1} + Î³ G_{t+1}^Î»).
\end{aligned}$$

---
# $Î»$-return

In an episodic task with time of termination $T$, we can rewrite the $Î»$-return
to
$$G_t^Î» = (1 - Î») âˆ‘_{i=1}^{T-t-1} Î»^{i-1} G_{t:t+i} + Î»^{T-t-1} G_t.$$

~~~
![w=60%,h=center](traces_example.svgz)

---
# Truncated $Î»$-return

We might also set a limit on the largest value of $n$, obtaining
**truncated $Î»$-return**
$$G_{t:t+n}^Î» â‰ (1 - Î») âˆ‘_{i=1}^{n-1} Î»^{i-1} G_{t:t+i} + Î»^{n-1} G_{t:t+n}.$$

~~~
The truncated $Î»$ return can be again written recursively as

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»),~~G_{t:t+1}^Î» = G_{t:t+1}.$$

~~~
Similarly to before, we can express the truncated $Î»$ return as a sum of TD
errors

$$\begin{aligned}
  G_{t:t+n}^Î» - V(S_t)
  & = (1 - Î») \big(R_{t+1} + Î³V(S_{t+1})\big) + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î») - V(S_t) \\
  & = R_{t+1} + Î³V(S_{t+1}) - V(S_t) + Î» Î³ \big(G_{t+1:t+n}^Î» - V(S_{t+1})\big),
\end{aligned}$$

~~~
obtaining an analogous estimate $G_{t:t+n}^Î» = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Î´_{t+i}.$

---
# Variable $Î»$s

The (truncated) $Î»$-return can be generalized to utilize different $Î»_i$ at each
step $i$. Notably, we can generalize the recursive definition

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»)$$

~~~
to
$$G_{t:t+n}^{Î»_i} = (1 - Î»_{t+1}) G_{t:t+1} + Î»_{t+1} (R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i}),$$

~~~
and express this quantity again by a sum of TD errors:

$$G_{t:t+n}^{Î»_i} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left(âˆ_{j=1}^i Î»_{t+j}\right) Î´_{t+i}.$$

---
# Off-policy Traces with Control Variates

Finally, we can combine the eligibility traces with off-policy estimation using
control variates:
$$G_{t:t+n}^{Î»,\mathrm{CV}} â‰ (1 - Î») âˆ‘_{i=1}^{n-1} Î»^{i-1} G_{t:t+i}^\mathrm{CV} + Î»^{n-1} G_{t:t+n}^\mathrm{CV}.$$

~~~
Recalling that
$$G_{t:t+n}^\mathrm{CV} = Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t),$$
~~~

we can rewrite $G_{t:t+n}^{Î»,\mathrm{CV}}$ recursively as
$$G_{t:t+n}^{Î»,\mathrm{CV}} = (1 - Î») G_{t:t+1}^\mathrm{CV} + Î» \Big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\Big),$$

~~~
which we can simplify by expanding $G_{t:t+1}^\mathrm{CV}=Ï_t(R_{t+1} + Î³V(S_{t+1})) + (1-Ï_t)V(S_t)$ to
$$G_{t:t+n}^{Î»,\mathrm{CV}} - V(S_t) = Ï_t \big(R_{t+1} + Î³V(S_{t+1}) - V(S_t)\big) + Î³Î»Ï_t \big(G_{t+1:t+n}^{Î»,\mathrm{CV}} - V(S_{t+1})\big).$$

---
# Off-policy Traces with Control Variates

Consequently, analogously as before, we can write the off-policy traces estimate
with control variates as

$$G_{t:t+n}^\mathrm{Î»,CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Ï_{t:t+i} Î´_{t+i},$$

~~~
and by repeating the above derivation we can extend the result also for time-variable $Î»_i$, we obtain
$$G_{t:t+n}^\mathrm{Î»_i,CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i \left(âˆ_{j=1}^i Î»_{t+j}\right) Ï_{t:t+i} Î´_{t+i}.$$

---
section: Recapitulation
class: tablewide
style: table {line-height: 1}
# Return Recapitulation

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} â‰ R_{t+1} + Î³ G_{t+1:t+n}$                                                         | $V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i Î´_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t)$ | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Ï_{t:t+i} Î´_{t+i}$ |
| $G_{t:t+n}^Î» â‰ (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»)$                             | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Î´_{t+i}$       |
| $G_{t:t+n}^{Î»_i} â‰ (1 - Î»_{t+1}) G_{t:t+1} + Î»_{t+1} (R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i})$         | $V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left({\scriptstyle âˆ_{j=1}^i Î»_{t+j}}\right) Î´_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{Î»,\mathrm{CV}} &â‰ (1 - Î») G_{t:t+1}^\mathrm{CV} \\&+ Î» \big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\big)\end{aligned}$ | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Ï_{t:t+i} Î´_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{Î»_i,\mathrm{CV}} &â‰ (1 - Î»_{t+1}) G_{t:t+1}^\mathrm{CV} \\+& Î»_{t+1} \big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\big)\end{aligned}$ | $\begin{aligned}&V(S_t)\\&\textstyle + âˆ‘\nolimits_{i=0}^{n-1} Î³^i \left({\scriptstyle âˆ_{j=1}^i Î»_{t+j}}\right) Ï_{t:t+i} Î´_{t+i}\end{aligned}$ |
