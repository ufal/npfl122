title: NPFL122, Lecture 8
class: title, langtech, cc-by-sa
# TD3, SAC, TRPO, PPO

## Milan Straka

### November 23, 2020

---
section: TD3
# Twin Delayed Deep Deterministic Policy Gradient

The paper Addressing Function Approximation Error in Actor-Critic Methods by
Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which

~~~
- decrease maximization bias by training two critics and choosing the minimum of
  their predictions;

~~~
- introduce several variance-lowering optimizations:
  - delayed policy updates;
  - target policy smoothing.

~~~

The TD3 algorithm has been together with SAC one of the state-of-the-art
algorithms for off-policy continuous-actions RL training (as of 2020).

---
# TD3 – Maximization Bias

Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.
In Q-learning, the maximization bias was caused by the explicit $\max$ operator.
For DDPG methods, it can be caused by the gradient descent itself. Let
$→θ_\textit{approx}$ be the parameters maximizing the $q_→θ$ and let
$→θ_\textit{true}$ be the hypothetical parameters which maximise true $q_π$,
and let $π_\textit{approx}$ and $π_\textit{true}$ denote the corresponding
policies.

~~~
Because the gradient direction is a local maximizer, for sufficiently small
$α<ε_1$ we have
$$𝔼\big[q_→θ(s, π_\textit{approx})\big] ≥ 𝔼\big[q_→θ(s, π_\textit{true})\big].$$

~~~
However, for real $q_π$ and for sufficiently small $α<ε_2$ it holds that
$$𝔼\big[q_π(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

~~~
Therefore, if $𝔼\big[q_→θ(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{true})\big]$,
for $α < \min(ε_1, ε_2)$
$$𝔼\big[q_→θ(s, π_\textit{approx})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

---
# TD3 – Maximization Bias

![w=50%](td3_bias.svgz)![w=50%](td3_bias_dqac.svgz)

~~~
Analogously to Double DQN we could compute the learning targets using
the current policy and the target critic, i.e., $r + γ q_{→θ'}(s', π_→φ(s'))$
(instead of using target policy and target critic as in DDPG), obtaining DDQN-AC algorithm.
However, the authors found out that the policy changes too slowly and the target
and current networks are too similar.

~~~
Using the original Double Q-learning, two pairs of actors and critics could be
used, with the learning targets computed by the opposite critic, i.e.,
$r + γ q_{→θ_2}(s', π_{→φ_1}(s'))$ for updating $q_{→θ_1}$. The resulting DQ-AC
algorithm is slightly better, but still suffering from overestimation.

---
# TD3 – Algorithm

The authors instead suggest to employ two critics and one actor. The actor is
trained using one of the critics, and both critics are trained using the same
target computed using the _minimum_ value of both critics as
$$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s')).$$

~~~
Furthermore, the authors suggest two additional improvements for variance
reduction.
- For obtaining higher quality target values, the authors propose to train the
  critics more often. Therefore, critics are updated each step, but the actor
  and the target networks are updated only every $d$-th step ($d=2$ is used in
  the paper).

~~~
- To explicitly model that similar actions should lead to similar results,
  a small random noise is added to performed actions when computing the target
  value:
  $$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s') + ε)~~~\textrm{for}~~~
    ε ∼ \operatorname{clip}(𝓝(0, σ), -c, c).$$

---
# TD3 – Algorithm

![w=43%,h=center](td3_algorithm.svgz)

---
# TD3 – Algorithm

![w=80%,h=center](td3_hyperparameters.svgz)

---
# TD3 – Results

![w=70%,h=center](td3_results_curves.svgz)
![w=70%,h=center](td3_results.svgz)

---
# TD3 – Ablations

![w=100%,h=center](td3_ablations.svgz)
![w=100%,h=center](td3_ablations_dqac.svgz)

---
# TD3 – Ablations

![w=65%,h=center](td3_ablations_results.svgz)

---
section: SAC
# Soft Actor Critic

The paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor by Tuomas Haarnoja et al. introduces
a different off-policy algorithm for continuous action space.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize.

~~~
TO BE FINISHED LATER

---
# Soft Actor Critic
![w=60%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=90%](sac_results.svgz)
