title: NPFL122, Lecture 8
class: title, langtech, cc-by-sa
# Continuous Actions

## Milan Straka

### December 02, 2019

---
section: Refresh
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.

Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

where $P(s → … → s'|π)$ is probability of transitioning from state $s$ to $s'$
using 0, 1, … steps.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. The loss is defined as
$$∇_→θ -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_→θ -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_→θ \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ -\ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](../07/reinforce.pdf)

---
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

Then, better-than-average returns are positive and worse-than-average returns
are negative.

The resulting $q_π(s, a) - v_π(s)$ function is also called an _advantage function_
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](../07/reinforce_with_baseline.pdf)

---
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called _actor-critic_ methods.

The idea is straightforward – instead of estimating the episode return using the
whole episode rewards, we can use $n$-step temporal difference estimation.

---
# Actor-Critic

![w=85%,h=center](../07/actor_critic.pdf)

---
# Asynchronous Methods for Deep RL

A 2015 paper from Volodymyr Mnih et al., the same group as DQN.

The authors propose an asynchronous framework, where multiple workers share one
neural network, each training using either an off-line or on-line RL algorithm.

They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C
(an _asynchronous advantage actor-critic_ method). For A3C, they compare
a version with and without LSTM.

The authors also introduce _entropy regularization term_ $β H(π(s; →θ))$ to the
loss to support exploration and discourage premature convergence.

---
section: PAAC
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workes to only generate episodes. Such approach
was described in May 2017 by Clemente et al., who named their agent
_parallel advantage actor-critic_ (PAAC).

![w=70%,h=center](paac_framework.pdf)

---
# Parallel Advantage Actor Critic

![w=85%,h=center](paac_algorithm.pdf)

---
# Parallel Advantage Actor Critic

![w=70%,h=center](paac_performance.pdf)

The authors use $8$ workers, $n_e=32$ parallel environments, $5$-step returns,
$γ=0.99$, $ε=0.1$, $β=0.01$ and a learning rate of $α=0.0007⋅n_e=0.0224$.

The $\textrm{arch}_\textrm{nips}$ is from A3C: 16 filters $8×8$ stride 4, 32
filters $4×4$ stride 2, a dense layer with 256 units. The
$\textrm{arch}_\textrm{nature}$ is from DQN: 32 filters $8×8$ stride 4, 64
filters $4×4$ stride 2, 64 filters $3×3$ stride 1 and 512-unit fully connected
layer. All nonlinearities are ReLU.

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_epochs.pdf)

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_time.pdf)

---
# Parallel Advantage Actor Critic

![w=100%,v=middle](paac_time_usage.pdf)

---
section: Continuous Action Space
# Continuous Action Space

Until now, the actions were discrete. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product
of several such ranges:
$$∏_i [a_i, b_i].$$

~~~
![w=40%,f=right](normal_distribution.pdf)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$
is
$$p(x) ≝ \frac{1}{\sqrt{2 π σ^2}} e^{\large-\frac{(x - μ)^2}{2σ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution we suitably parametrize the action value,
usually using the normal distribution. Considering only one real-valued action,
we therefore have
$$π(a | s; →θ) ≝ P\Big(a ∼ 𝓝\big(μ(s; →θ), σ(s; →θ)^2\big)\Big),$$
where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard
deviation of the action distribution.

The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a regular regression (i.e., one output neuron
  without activation);
- the standard variance (which must be positive) being computed again as
  a regression, followed most commonly by either $\exp$ or
  $\operatorname{softplus}$, where $\operatorname{softplus}(x) ≝ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

~~~
```python
  mus = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.math.exp(sds)   # or sds = tf.math.softplus(sds)

  action_dist = tfp.distributions.Normal(mus, sds)

  # Loss computed as - log π(a|s) - entropy_regularization
  loss = - action_dict.log_prob(actions) * returns \
         - args.entropy_regularization * action_dist.entropy()
```

---
# Continuous Action Space

When the action consists of several real values, i.e., action is a suitable
subregion of $ℝ^n$ for $n>1$, we can:
- either use multivariate Gaussian distribution;
- or factorize the probability into a product of univariate normal
  distributions.

~~~
Modeling the action distribution using a single normal distribution might be
insufficient, in which case a mixture of normal distributions is usually used.

~~~
Sometimes, the continuous action space is used even for discrete output -- when
modeling pixels intensities (256 values) or sound amplitude (2$^{16}$ values),
instead of a softmax we use discretized mixture of distributions,
usually $\operatorname{logistic}$ (a distribution with a sigmoid cdf). Then,
$$π(a) = ∑_i p_i\Big(σ\big((a + 0.5 - μ_i) / σ_i\big) - σ\big((a - 0.5 - μ_i) / σ_i\big) \Big).$$
However, such mixtures are usually used in generative modeling, not in
reinforcement learning.

---
section: DPG
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

~~~
## Deterministic Policy Gradient Theorem
Assume that the policy $π(s; →θ)$ is deterministic and computes
an action $a∈ℝ$. Then under several assumptions about continuousness, the
following holds:
$$∇_→θ J(→θ) ∝ 𝔼_{s∼μ(s)} \Big[∇_→θ π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al.

---
# Deterministic Policy Gradient Theorem – Proof

The proof is very similar to the original (stochastic) policy gradient theorem.
We assume that $p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a r(s, a), π(s; →θ), ∇_→θ π(s; →θ)$
are continuous in all params.

~~~
$\displaystyle ∇_→θ v_π(s) = ∇_→θ q_π(s, π(s; →θ))$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ\Big(r\big(s, π(s; →θ)\big) + ∫_{s'} p\big(s' | s, π(s; →θ)\big) v_π(s') \d s'\Big)$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a r(s, a) \big|_{a=π(s; →θ)} + ∇_→θ ∫_{s'} p\big(s' | s, π(s; →θ)\big) v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a \Big(r(s, a) + ∫_{s'} p\big(s' | s, a\big) v_π(s') \d s' \Big) \Big|_{a=π(s; →θ)}\\
                    \qquad\qquad\qquad + ∫_{s'} p\big(s' | s, π(s; →θ)\big) ∇_→θ v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s; →θ)} + ∫_{s'} p\big(s' | s, π(s; →θ)\big) ∇_→θ v_π(s') \d s'$

~~~
We finish the proof analogously to the gradient theorem by continually expanding $∇_→θ v_π(s')$.

---
section: DDPG
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
(similarly to how expected Sarsa is also an off-policy algorithm).

~~~
We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,
training $q(s, a; →θ)$ using a deterministic variant of the Bellman equation:
$$q(S_t, A_t; →θ) = 𝔼_{R_{t+1}, S_{t+1}} \big[R_{t+1} + γ q(S_{t+1}, π(S_{t+1}; →θ))\big]$$
and $π(s; →θ)$ according to the deterministic policy gradient theorem.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

The authors utilize a replay buffer, a target network (updated by exponential
moving average with $τ=0.001$), batch normalization for CNNs, and perform
exploration by adding a normal-distributed noise to predicted actions.
Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the
policy and critic network, respectively.

---
# Deep Deterministic Policy Gradients

![w=65%,h=center](ddpg.pdf)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.pdf)

---
# Deep Deterministic Policy Gradients

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

![w=57%,h=center](ddpg_results.pdf)
