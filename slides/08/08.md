title: NPFL122, Lecture 8
class: title, langtech, cc-by-sa
# SAC, Eligibility Traces

## Milan Straka

### November 22, 2021

---
section: SAC
# Soft Actor Critic

The paper _Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor_ by Tuomas Haarnoja et al. from Jan 2018
introduces a different off-policy algorithm for continuous action space.

~~~
It was followed by a continuation paper _Soft Actor-Critic Algorithms and
Applications_ in Dec 2018.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize, instead of just ad-hoc adding the entropy penalty.

---
# Soft Actor Critic Objective

Until now, our goal was to optimize
$$𝔼_π \big[G_0\big].$$

~~~
Assume the rewards are deterministic and that $μ_π$ is on-policy distribution of
a policy $π$.

In the soft actor-critic, the authors instead propose to optimize the maximum
entropy objective
$$π_* = \argmax_π 𝔼_{s, a ∼ μ_π} \big[r(s, a) + α H(π(⋅|s))\big].$$

~~~
Note that the value of $α$ is dependent on the maginute of returns and that
for a fixed policy, the entropy penalty can be “hidden” in the reward.

---
# Soft Actor Critic Objective

The authors in fact utilize slightly different objective, notably a one with
the following augmented reward
$$r_π(s, a) ≝ r(s, a) + 𝔼_{s' ∼ p(s, a)} \big[α H(π(⋅|s'))\big].$$

~~~
From now on, we consider **soft action-value** function corresponding to this
augmented reward.

---
section: SPE
# Soft Policy Evaluation

Our goal is now to derive **soft policy iteration**, an analogue of policy iteration algorithm.

~~~
We start by considering soft policy evaluation. Let a modified Bellman backup
operator $𝓣_π$ be defined as
$$𝓣_π q(s, a) ≝ r(s, a) + γ 𝔼_{s' ∼ p(s, a)} \big[v(s')\big],$$
where the **soft (state-)value** function $v(s)$ is defined as
$$
v(s) = 𝔼_{a ∼ π} \big[q(s, a) - α \log π(a|s)\big].$$

~~~
This modified Bellman backup operator corresponds to the usual one for the
augmented rewards $r_π(s, a)$, and therefore the repeated application
$𝓣_π^k q$ converges to $q_π$ according to the original proof.

---
section: SPI
# Soft Policy Improvement

While the soft policy evaluation was a straightforward modification of the
original policy evaluation, the soft policy improvement is quite different.

~~~
Assume we have a policy $π$, its action-value function $q_π$ from the soft
policy evaluation, and we want to improve the policy. Furthermore, we should
select the improved policy from a family of parametrized distributions $Π$.

~~~
We define the improved policy $π'$ as
$$π'(⋅|s) ≝ \argmin_{π̄ ∈ Π} J_π(π̄) ≝ \argmin_{π̄ ∈ Π} D_\textrm{KL}\Bigg( π̄(⋅|s) \Bigg\| \frac{\exp\big(\frac{1}{α} q_π(s, ⋅)\big)}{z_π(s)} \Bigg),$$
where $z_π(s)$ is the partition function (i.e.,  normalization factor such that
the right-hand side is a distribution), which does not depend on the new policy
and thus can be ignored.

---
# Soft Policy Improvement

We now prove that $q_{π'}(s, a) ≥ q_π(s, a)$ for any state $s$ and action $a$.

~~~
We start by noting that $J_π(π') ≤ J_π(π)$, because we can always choose $π$ as
the improved policy.
~~~
Therefore,
$$𝔼_{a∼π'} \big[α\log π'(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big] ≤
  𝔼_{a∼π} \big[α\log π(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big],$$

~~~
which results in
$$𝔼_{a∼π'} \big[q_π(s, a) - α\log π'(a|s)\big] ≥ v_π(s).$$

~~~
We now finish the proof analogously to the original one:
$$\begin{aligned}
q_π(s, a) &= r(s, a) + γ𝔼_{s'}[v_π(s')] \\
          &≤ r(s, a) + γ𝔼_{s'}[𝔼_{a'∼π'} [q_π(s', a') - α\log π'(a'|s')] \\
          &… \\
          &≤ q_{π'}(s, a).
\end{aligned}$$

---
# Soft Policy Iteration

The soft policy iteration algorithm alternates between the soft policy
evaluation and soft policy improvement steps.

~~~
The repeated application of these two steps produce better and better policies.
In other words, we get a monotonically increasing sequence of soft action-value
functions.

~~~
If the soft action-value function if bounded (the paper assumes
a bounded reward and a finite number of actions to bound the entropy), the
repeated application converges to some $q_*$, from which we get a $π_*$ using
the soft policy improvement step.

~~~
It remains to show that the $π_*$ is indeed the optimal policy
fulfilling $q_{π_*}(s, a) ≥ q_π(s, a)$.

~~~
However, this follows from the fact that at convergence,
$J_{π_*}(π_*) ≤ J_{π_*}(π)$, and following the same reasoning as in the proof of
the soft policy improvement, we obtain the required $q_{π_*}(s, a) ≥ q_π(s, a)$.

---
section: SACAlgorithm
# Soft Actor Critic

Our soft actor critic will be an off-policy algorithm with continuous action
space. The model consist of two critics $q_{→θ_1}$ and $q_{→θ_2}$, two target
critics $q_{→θ̄_1}$ and $q_{→θ̄_2}$ and finally a single actor $π_{→φ}$.

~~~
The authors state that
- with a single critic, all the described experiments still converge;
~~~
- they adopted the two critics from the TD3 paper;
~~~
- using two critics “significantly speed up training”.

---
# Soft Actor Critic – Critic Training

To train the critic, we use the modified Bellman backup operator, resulting in
the loss
$$J_q(→θ_i) = 𝔼_{s,a ∼ μ} \Big[\big(q_{→θ_i}(s, a) - \big(r(s, a) + γ 𝔼_{s' ∼ p(s, a)} [v_\textrm{min}(s')]\big)\big)^2\Big],$$

~~~
where
$$v_\textrm{min}(s) = 𝔼_{a ∼ π_{→φ}} \Big[\min_i\big(q_{→θ̄_i}(s, a) \big) - α \log π_{→φ}(a | s)\Big].$$

~~~
The target critics are updated using exponentiation moving averages with
momentum $τ$.

---
# Soft Actor Critic – Actor Training

The actor is updated by directly minimizing the KL divergence, resulting in the
loss
$$J_π(→φ) = 𝔼_{s ∼ μ} 𝔼_{a ∼ π_{→φ}}\Big[α \log\big(π_{→φ}(a, s)\big) - \min_i\big(q_{→θ_i}(s, a)\big)\Big].$$

~~~
Given that our critics are differentiable, we now reparametrize the policy as
$$a = f_{→φ}(s, ε).$$

~~~
Specifically, we sample $ε ∼ 𝓝(0, 1)$ and let $f_{→φ}$ produce an unbounded
Gaussian distribution (a diagonal one if the actions are vectors).

~~~
Together, we obtain
$$J_π(→φ) = 𝔼_{s ∼ μ} 𝔼_{ε ∼ 𝓝(0, 1)}\Big[α \log\big(π_{→φ}(f_{→φ}(s, ε), s)\big) - \min_i\big(q_{→θ_i}(s, f_{→φ}(s, ε))\big)\Big].$$

---
# Soft Actor Critic – Bounding Actions

In practice, the actions need to be bounded.

~~~
The authors propose to apply an invertible squashing function $\tanh$
on the unbounded Gaussian distribution.

~~~
Consider that our policy produces an unbounded action $π(u | s)$.
To define a distribution $π̄(a | s)$ with $a = \tanh(u)$, we need to employ
the change of variables, resulting in
$$π̄(a | s) = π(u | s) \bigg(\frac{∂a}{∂u}\bigg)^{-1} = π(u | s) \bigg(\frac{∂\tanh(u)}{∂u}\bigg)^{-1}.$$

~~~
Therefore, the log-likelihood has quite a simple form
$$\log π̄(a | s) = \log π(u | s) - \log\big(1 - \tanh^2(u)\big).$$

---
# Soft Actor Critic – Automatic Entropy Adjustment

One of the most important hyperparameters is the entropy penalty $α$.

~~~
In the second paper, the authors presented an algorithm for automatic adjustment
of its value.

~~~
Instead of setting the entropy penalty $α$, they propose to specify target
entropy value $𝓗$ and then solve a constrained optimization problem
$$π_* = \argmax_π 𝔼_{s, a ∼ μ_π} \big[r(s, a)\big]\textrm{~~such that~~}𝔼_{s, a ∼ μ_π}\big[-\log π(a | s)\big] ≥ 𝓗.$$

~~~
We can then form a Lagrangian with a multiplier $α$
$$𝔼_{s, a ∼ μ_π} \Big[r(s, a) + α\big(-\log π(a | s) - 𝓗\big)\Big],$$
which should be maximized with respect to $π$ and minimized with respect
to $α ≥ 0$.

---
# Soft Actor Critic – Automatic Entropy Adjustment

To optimize the Lagrangian, we perform the _dual gradient descent_, where we
alternate between maximization with respect to $π$ and minimization with respect
to $α$.

~~~
While such a procedure is guaranteed to converge only under the convexity
assumptions, the authors report that the dual gradient descent works in practice
also with non-linear function approximation.

~~~
To conclude, the automatic entropy adjustment is performed by introducing
a final loss
$$J(α) = 𝔼_{a ∼ π} \big[-α \log π(a | s) - α 𝓗\big].$$

---
# Soft Actor Critic
![w=93%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=93%,h=center](sac_hyperparameters.svgz)

---
# Soft Actor Critic
![w=86%,h=center](sac_results.svgz)

---
# Soft Actor Critic
![w=100%,v=middle](sac_ablations.svgz)

---
section: ControlVariates
# Off-policy Correction Using Control Variates

Let $G_{t:t+n}$ be the estimated $n$-step return
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + \Big[\textrm{episode still running in }t+n\Big] γ^n V(S_{t+n}),$$

~~~
which can be written recursively as
$$G_{t:t+n} \begin{cases}
  0 & \mathrm{if~episode~ended~before~}t, \\
  V(S_t) & \mathrm{if~}n=0, \\
  R_{t+1} + γ G_{t+1:t+n} & \mathrm{otherwise}.
\end{cases}$$

---
# Off-policy Correction Using Control Variates

Note that we can write
$$\begin{aligned}
G_{t:t+n} - V(S_t)
  &= R_{t+1} + γ G_{t+1:t+n} - V(S_t) \\
  &= R_{t+1} + γ \big(G_{t+1:t+n} - V(S_{t+1})\big) + γV(S_{t+1}) - V(S_t),
\end{aligned}$$

~~~
which yields
$$G_{t:t+n} - V(S_t) = R_{t+1} + γV(S_{t+1}) - V(S_t) + γ\big(G_{t+1:t+n} - V(S_{t+1})\big).$$

~~~
Denoting the TD error as $δ_t ≝ R_{t+1} + γV(S_{t+1}) - V(S_t)$, we can
therefore write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}.$$

---
# Off-policy Correction Using Control Variates

Now consider applying the IS off-policy correction to $G_{t:t+n}$ using the
importance sampling ratio
$$ρ_t ≝ \frac{π(A_t | S_t)}{b(A_t | S_t)},~~~ρ_{t:t+n} ≝ ∏_{i=0}^n ρ_{t+i}.$$

~~~
First note that
$$𝔼_{A_t ∼ b} \big[ρ_t\big] = ∑_{A_t} b(A_t | S_t) \frac{π(A_t | S_t)}{b(A_t | S_t)} = 1,$$

~~~
which can be extended to
$$𝔼_b \big[ρ_{t:t+n}\big] = 1.$$

---
# Off-policy Correction Using Control Variates

Until now, we used
$$G_{t:t+n}^\mathrm{IS} ≝ ρ_{t:t+n-1} G_{t:t+n}.$$

~~~
However, such correction has unnecessary variance. Notably, when expanding
$G_{t:t+n}$
$$G_{t:t+n}^\mathrm{IS} = ρ_{t:t+n-1} \big(R_{t+1} + γ G_{t+1:t+n}\big),$$

~~~
the $R_{t+1}$ depends only on $ρ_t$, not on $ρ_{t+1:t+n-1}$, and given that
the expectation of the importance sampling ratio is 1, we can simplify to
$$G_{t:t+n}^\mathrm{IS} = ρ_t R_{t+1} + ρ_{t:t+n-1} γ G_{t+1:t+n}.$$

~~~
Such an estimate can be written recursively as
$$G_{t:t+n}^\mathrm{IS} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big).$$

---
# Off-policy Correction Using Control Variates

We can reduce the variance even further – when $ρ_t=0$, we might consider
returning the value of $V(S_t)$ instead of 0.

~~~
Therefore, we might add another term, the so-called **control variate**, to the
estimate
$$G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t),$$
which is valid, since the expected value of $1-ρ_t$ is zero and $ρ_t$ and $S_t$
are independent.

~~~
Similarly as before, rewriting to
$$\begin{aligned}
G_{t:t+n}^\mathrm{CV} - V(S_t)
  &= ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) - ρ_tV(S_t) \\
  &= ρ_t \big(R_{t+1} + γ V(S_{t+1}) - V(S_t) + γ (G_{t+1:t+n}^\mathrm{CV} - V(S_{t+1}))\big)
\end{aligned}$$

~~~
results in
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}.$$

---
section: EligibilityTraces
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $λ$-return

For a given $λ ∈ [0,1]$, we define **$λ$-return** as
$$G_t^λ ≝ (1 - λ) ∑_{i=1}^∞ λ^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](traces_weighting.svgz)

~~~
Alternatively, the $λ$-return can be written recursively as
$$\begin{aligned}
G_t^λ &= (1 - λ) G_{t:t+1} \\
      &+ λ (R_{t+1} + γ G_{t+1}^λ).
\end{aligned}$$

---
# $λ$-return

In an episodic task with time of termination $T$, we can rewrite the $λ$-return
to
$$G_t^λ = (1 - λ) ∑_{i=1}^{T-t-1} λ^{i-1} G_{t:t+i} + λ^{T-t-1} G_t.$$

~~~
![w=60%,h=center](traces_example.svgz)
