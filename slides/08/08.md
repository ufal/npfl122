title: NPFL122, Lecture 8
class: title, langtech, cc-by-sa
# SAC, Eligibility Traces

## Milan Straka

### November 21, 2022

---
section: SAC
# Soft Actor Critic

The paper _Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor_ by Tuomas Haarnoja et al. from Jan 2018
introduces a different off-policy algorithm for continuous action space.

~~~
It was followed by a continuation paper _Soft Actor-Critic Algorithms and
Applications_ in Dec 2018.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize, instead of just ad-hoc adding the entropy penalty. Such an approach
is an instance of _regularized policy optimization_.

---
# Soft Actor Critic Objective

Until now, our goal was to optimize
$$𝔼_π \big[G_0\big].$$

~~~
Assume the rewards are deterministic and that $μ_π$ is on-policy distribution of
a policy $π$.

In the soft actor-critic, the authors instead propose to optimize the maximum
entropy objective
$$\begin{aligned}
π_* &= \argmax_π 𝔼_{s∼μ_π} \Big[𝔼_{a∼π(s)}\big[r(s, a)\big] + α H(π(⋅|s))\Big] \\
    &= \argmax_π 𝔼_{s∼μ_π, a∼π(s)}\big[r(s, a) - α \log π(a|s)\big].
\end{aligned}$$

~~~
Note that the value of $α$ is dependent on the magnitude of returns and that
for a fixed policy, the entropy penalty can be “hidden” in the reward.

---
# Soft Actor Critic Objective

To maximize the regularized objective, we define the following augmented reward:
$$r_π(s, a) ≝ r(s, a) + 𝔼_{s' ∼ p(s, a)} \big[α H(π(⋅|s'))\big].$$

~~~
From now on, we consider **soft action-value** function corresponding to this
augmented reward.

---
section: SPE
# Soft Policy Evaluation

Our goal is now to derive **soft policy iteration**, an analogue of policy iteration algorithm.

~~~
We start by considering soft policy evaluation. Let a modified Bellman backup
operator $𝓣_π$ be defined as
$$𝓣_π q(s, a) ≝ r(s, a) + γ 𝔼_{s' ∼ p(s, a)} \big[v(s')\big],$$
where the **soft (state-)value** function $v(s)$ is defined as
$$
v(s) = 𝔼_{a ∼ π} \big[q(s, a)\big] + αH(π(⋅|s)) = 𝔼_{a ∼ π} \big[q(s, a) - α \log π(a|s)\big].$$

~~~
This modified Bellman backup operator corresponds to the usual one for the
augmented rewards $r_π(s, a)$, and therefore the repeated application
$𝓣_π^k q$ converges to $q_π$ according to the original proof.

---
section: SPI
# Soft Policy Improvement

While the soft policy evaluation was a straightforward modification of the
original policy evaluation, the soft policy improvement is quite different.

~~~
Assume we have a policy $π$, its action-value function $q_π$ from the soft
policy evaluation, and we want to improve the policy. Furthermore, we should
select the improved policy from a family of parametrized distributions $Π$.

~~~
We define the improved policy $π'$ as
$$π'(⋅|s) ≝ \argmin_{π̄ ∈ Π} J_π(π̄) ≝ \argmin_{π̄ ∈ Π} D_\textrm{KL}\Bigg( π̄(⋅|s) \Bigg\| \frac{\exp\big(\frac{1}{α} q_π(s, ⋅)\big)}{z_π(s)} \Bigg),$$
where $z_π(s)$ is the partition function (i.e.,  normalization factor such that
the right-hand side is a distribution), which does not depend on the new policy
and thus can be ignored.

---
# Soft Policy Improvement

We now prove that $q_{π'}(s, a) ≥ q_π(s, a)$ for any state $s$ and action $a$.

~~~
We start by noting that $J_π(π') ≤ J_π(π)$, because we can always choose $π$ as
the improved policy.
~~~
Therefore,
$$𝔼_{a∼π'} \big[α\log π'(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big] ≤
  𝔼_{a∼π} \big[α\log π(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big],$$

~~~
which results in
$$𝔼_{a∼π'} \big[q_π(s, a) - α\log π'(a|s)\big] ≥ v_π(s).$$

~~~
We now finish the proof analogously to the original one:
$$\begin{aligned}
q_π(s, a) &= r(s, a) + γ𝔼_{s'}[v_π(s')] \\
          &≤ r(s, a) + γ𝔼_{s'}[𝔼_{a'∼π'} [q_π(s', a') - α\log π'(a'|s')] \\
          &… \\
          &≤ q_{π'}(s, a).
\end{aligned}$$

---
# Soft Policy Iteration

The soft policy iteration algorithm alternates between the soft policy
evaluation and soft policy improvement steps.

~~~
The repeated application of these two steps produce better and better policies.
In other words, we get a monotonically increasing sequence of soft action-value
functions.

~~~
If the soft action-value function is bounded (the paper assumes
a bounded reward and a finite number of actions to bound the entropy), the
repeated application converges to some $q_*$, from which we get a $π_*$ using
the soft policy improvement step. (It is not clear to me why the algorithm
should converge in finite time, but we can make the rest of the slide
conditional on “if the algorithm converges”).

~~~
It remains to show that the $π_*$ is indeed the optimal policy
fulfilling $q_{π_*}(s, a) ≥ q_π(s, a)$.

~~~
However, this follows from the fact that at convergence,
$J_{π_*}(π_*) ≤ J_{π_*}(π)$, and following the same reasoning as in the proof of
the soft policy improvement, we obtain the required $q_{π_*}(s, a) ≥ q_π(s, a)$.

---
# Soft Policy Improvement Derivation

The following derivation is not in the original paper, but it is my
understanding of how the softmax of the action-value function arises.
For simplicity, we assume finite number of actions.

~~~
Assuming we have a policy $π$ and its action-value function $q_π$,
we usually improve the policy using
$$\begin{aligned}
  ν(⋅|s)
  &= \argmax_ν 𝔼_{a∼ν(⋅|s)} \big[q_π(s, a)\big] \\
  &= \argmax_ν ∑\nolimits_a q_π(s, a) ν(a|s) \\
  &= \argmax_ν →q_π(s, ⋅)^T →ν(⋅|s), \\
\end{aligned}$$

~~~
which results in a greedy improvement with the form of
$$ν(s) = \argmax\nolimits_a q_π(s, a).$$

---
# Soft Policy Improvement Derivation

Now consider instead the regularized objective
$$\begin{aligned}
  ν(⋅|s)
  &= \argmax_ν \big( 𝔼_{a∼ν(⋅|s)} \big[q_π(s, a)\big] + αH(ν(⋅|s))\big) \\
  &= \argmax_ν \big(𝔼_{a∼ν} \big[q_π(s, a) - α \log ν(a|s)\big]\big)
\end{aligned}$$

~~~
To maximize it for a given $s$, we form a Lagrangian
$$𝓛 = \Big(∑\nolimits_a ν(a|s) \big(q_π(s, a) - α\log ν(a|s)\big)\Big) - λ\Big(1 - ∑\nolimits_a ν(a|s)\Big).$$

~~~
The derivative with respect to $ν(a|s)$ is
$$\frac{∂𝓛}{∂ν(a|s)} = q_π(s, a) - α\log ν(a|s) - α + λ.$$

~~~
Setting it to zero, we get $α\log ν(a|s) = q_π(s, a) + λ - α$, resulting in $ν(a|s) ∝ e^{\frac{1}{α} q_π(s, a)}$.

---
section: SACAlgorithm
# Soft Actor Critic

Our soft actor critic will be an off-policy algorithm with continuous action
space. The model consist of two critics $q_{→θ_1}$ and $q_{→θ_2}$, two target
critics $q_{→θ̄_1}$ and $q_{→θ̄_2}$ and finally a single actor $π_{→φ}$.

~~~
The authors state that
- with a single critic, all the described experiments still converge;
~~~
- they adopted the two critics from the TD3 paper;
~~~
- using two critics “significantly speed up training”.

---
# Soft Actor Critic – Critic Training

To train the critic, we use the modified Bellman backup operator, resulting in
the loss
$$J_q(→θ_i) = 𝔼_{s∼μ_π, a∼π_{→φ}(s)} \Big[\big(q_{→θ_i}(s, a) - \big(r(s, a) + γ 𝔼_{s' ∼ p(s, a)} [v_\textrm{min}(s')]\big)\big)^2\Big],$$

~~~
where
$$v_\textrm{min}(s) = 𝔼_{a∼π_{→φ}(s)} \Big[\min_i\big(q_{→θ̄_i}(s, a) \big) - α \log π_{→φ}(a | s)\Big].$$

~~~
The target critics are updated using exponentiation moving averages with
momentum $τ$.

---
# Soft Actor Critic – Actor Training

The actor is updated by directly minimizing the KL divergence, resulting in the
loss
$$J_π(→φ) = 𝔼_{s∼μ_π, a∼π_{→φ}(s)}\Big[α \log\big(π_{→φ}(a, s)\big) - \min_i\big(q_{→θ_i}(s, a)\big)\Big].$$

~~~
Given that our critics are differentiable, we now reparametrize the policy as
$$a = f_{→φ}(s, ε).$$

~~~
Specifically, we sample $ε ∼ 𝓝(0, 1)$ and let $f_{→φ}$ produce an unbounded
Gaussian distribution (a diagonal one if the actions are vectors).

~~~
Together, we obtain
$$J_π(→φ) = 𝔼_{s∼μ_π, ε∼𝓝(0, 1)}\Big[α \log\big(π_{→φ}(f_{→φ}(s, ε), s)\big) - \min_i\big(q_{→θ_i}(s, f_{→φ}(s, ε))\big)\Big].$$

---
# Soft Actor Critic – Bounding Actions

In practice, the actions need to be bounded.

~~~
The authors propose to apply an invertible squashing function $\tanh$
on the unbounded Gaussian distribution.

~~~
Consider that our policy produces an unbounded action $π(u | s)$.
To define a distribution $π̄(a | s)$ with $a = \tanh(u)$, we need to employ
the change of variables, resulting in
$$π̄(a | s) = π(u | s) \bigg(\frac{∂a}{∂u}\bigg)^{-1} = π(u | s) \bigg(\frac{∂\tanh(u)}{∂u}\bigg)^{-1}.$$

~~~
Therefore, the log-likelihood has quite a simple form
$$\log π̄(a | s) = \log π(u | s) - \log\big(1 - \tanh^2(u)\big).$$

---
# Soft Actor Critic – Automatic Entropy Adjustment

One of the most important hyperparameters is the entropy penalty $α$.

~~~
In the second paper, the authors presented an algorithm for automatic adjustment
of its value.

~~~
Instead of setting the entropy penalty $α$, they propose to specify target
entropy value $𝓗$ and then solve a constrained optimization problem
$$π_* = \argmax_π 𝔼_{s∼μ_π, a∼π(s)} \big[r(s, a)\big]\textrm{~~such that~~}𝔼_{s∼μ_π, a∼π(s)}\big[-\log π(a | s)\big] ≥ 𝓗.$$

~~~
We can then form a Lagrangian with a multiplier $α$
$$𝔼_{s∼μ_π, a∼π(s)} \Big[r(s, a) + α\big(-\log π(a | s) - 𝓗\big)\Big],$$
which should be maximized with respect to $π$ and minimized with respect
to $α ≥ 0$.

---
# Soft Actor Critic – Automatic Entropy Adjustment

To optimize the Lagrangian, we perform _dual gradient descent_, where we
alternate between maximization with respect to $π$ and minimization with respect
to $α$.

~~~
While such a procedure is guaranteed to converge only under the convexity
assumptions, the authors report that the dual gradient descent works in practice
also with nonlinear function approximation.

~~~
To conclude, the automatic entropy adjustment is performed by introducing
a final loss
$$J(α) = 𝔼_{s∼μ_π, a∼π(s)} \big[-α \log π(a | s) - α 𝓗\big].$$

---
# Soft Actor Critic
![w=93%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=93%,h=center](sac_hyperparameters.svgz)

---
# Soft Actor Critic
![w=86%,h=center](sac_results.svgz)

---
# Soft Actor Critic
![w=100%,v=middle](sac_ablations.svgz)

---
section: ControlVariates
# Off-policy Correction Using Control Variates

Let $G_{t:t+n}$ be the estimated $n$-step return
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + \Big[\textrm{episode still running in }t+n\Big] γ^n V(S_{t+n}),$$

~~~
which can be written recursively as
$$G_{t:t+n} \begin{cases}
  0 & \mathrm{if~episode~ended~before~}t, \\
  V(S_t) & \mathrm{if~}n=0, \\
  R_{t+1} + γ G_{t+1:t+n} & \mathrm{otherwise}.
\end{cases}$$

~~~
For simplicity, we do not explicitly handle the first case (“the episode
has already ended”) in the following.

---
style: .katex-display { margin: .8em 0 }
# Off-policy Correction Using Control Variates

Note that we can write
$$\begin{aligned}
G_{t:t+n} - V(S_t)
  &= R_{t+1} + γ G_{t+1:t+n} - V(S_t) \\
  &= R_{t+1} + γ \big(G_{t+1:t+n} - V(S_{t+1})\big) + γV(S_{t+1}) - V(S_t),
\end{aligned}$$

~~~
which yields
$$G_{t:t+n} - V(S_t) = R_{t+1} + γV(S_{t+1}) - V(S_t) + γ\big(G_{t+1:t+n} - V(S_{t+1})\big).$$

~~~
Denoting the TD error as $δ_t ≝ R_{t+1} + γV(S_{t+1}) - V(S_t)$, we can
therefore write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}.$$

~~~
Incidentally, to correctly handle the “the episode has already ended” case, it
would be enough to define $δ_t ≝ R_{t+1} + [¬\textrm{done}]⋅γV(S_{t+1}) - V(S_t)$.

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |

---
# Off-policy Correction Using Control Variates

Now consider applying the IS off-policy correction to $G_{t:t+n}$ using the
importance sampling ratio
$$ρ_t ≝ \frac{π(A_t | S_t)}{b(A_t | S_t)},~~~ρ_{t:t+n} ≝ ∏_{i=0}^n ρ_{t+i}.$$

~~~
First note that
$$𝔼_{A_t ∼ b} \big[ρ_t\big] = ∑_{A_t} b(A_t | S_t) \frac{π(A_t | S_t)}{b(A_t | S_t)} = 1,$$

~~~
which can be extended to
$$𝔼_b \big[ρ_{t:t+n}\big] = 1.$$

---
# Off-policy Correction Using Control Variates

Until now, we used
$$G_{t:t+n}^\mathrm{IS} ≝ ρ_{t:t+n-1} G_{t:t+n}.$$

~~~
However, such correction has unnecessary variance. Notably, when expanding
$G_{t:t+n}$
$$G_{t:t+n}^\mathrm{IS} = ρ_{t:t+n-1} \big(R_{t+1} + γ G_{t+1:t+n}\big),$$

~~~
the $R_{t+1}$ depends only on $ρ_t$, not on $ρ_{t+1:t+n-1}$, and given that
the expectation of the importance sampling ratio is 1, we can simplify to
$$G_{t:t+n}^\mathrm{IS} = ρ_t R_{t+1} + ρ_{t:t+n-1} γ G_{t+1:t+n}.$$

~~~
Such an estimate can be written recursively as
$$G_{t:t+n}^\mathrm{IS} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big).$$

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |

---
# Off-policy Correction Using Control Variates

We can reduce the variance even further – when $ρ_t=0$, we might consider
estimating the return using $V(S_t)$ instead of 0.

~~~
To utilize this idea, we turn to **control variates**, which is
a general method of reducing variance of Monte Carlo estimators. Let $μ$ be
an unknown expectation, which we estimate using an unbiased estimator $m$.
Assume we have another **correlated** statistic $k$ with a known expectation $κ$.

~~~
We can then use an estimate $m^* ≝ m - c(k - κ)$, which is also an unbiased
estimator of $μ$, with variance
$$\Var(m^*) = \Var(m) + c^2 \Var(k) - 2c\Cov(m, k).$$

~~~
To arrive at the optimal value of $c$, we can set the derivative of $\Var(m^*)$
to 0, obtaining
$$c = \frac{\Cov(m, k)}{\Var(k)}.$$

---
# Off-policy Correction Using Control Variates

In case of the value function estimate
$$G_{t:t+n}^\mathrm{IS} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big),$$
we might consider using $ρ_t$ as the correlated statistic $k$, with known
expectation $κ=1$, because if $ρ_t ≫ 1$, then our return estimate is probably an
overestimate, and vice versa.

~~~
The optimal value of $c$ should then be
$$c = \frac{\Cov(m, k)}{\Var(k)} = \frac{𝔼_b\big[(G_{t:t+n}^\mathrm{IS} - v_π(S_t))(ρ_t-1)\big]}{𝔼_b\big[(ρ_t-1)^2\big]},$$
which is however difficult to compute.
~~~
Instead, considering the estimate when $ρ_t = 0$, we get
$$ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big) + c(1 - ρ_t) \xlongequal{ρ_t=0} c.$$
~~~
Because a reasonable estimate in case of $ρ_t = 0$ is $V(S_t)$, we use $c = V(S_t)$.

---
# Off-policy Correction Using Control Variates

The estimate with the **control variate** term is therefore
$$G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t),$$
which adds no bias, since the expected value of $1-ρ_t$ is zero and $ρ_t$ and $S_t$
are independent.

~~~
Similarly as before, rewriting to
$$\begin{aligned}
G_{t:t+n}^\mathrm{CV} - V(S_t)
  &= ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) - ρ_tV(S_t) \\
  &= ρ_t \big(R_{t+1} + γ V(S_{t+1}) - V(S_t) + γ (G_{t+1:t+n}^\mathrm{CV} - V(S_{t+1}))\big)
\end{aligned}$$

~~~
results in
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}.$$

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t)$ | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}$ |

---
section: EligibilityTraces
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $λ$-return

For a given $λ ∈ [0,1]$, we define **$λ$-return** as
$$G_t^λ ≝ (1 - λ) ∑_{i=1}^∞ λ^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](traces_weighting.svgz)

~~~
Alternatively, the $λ$-return can be written recursively as
$$\begin{aligned}
G_t^λ &= (1 - λ) G_{t:t+1} \\
      &+ λ (R_{t+1} + γ G_{t+1}^λ).
\end{aligned}$$

---
# $λ$-return

In an episodic task with time of termination $T$, we can rewrite the $λ$-return
to
$$G_t^λ = (1 - λ) ∑_{i=1}^{T-t-1} λ^{i-1} G_{t:t+i} + λ^{T-t-1} G_t.$$

~~~
![w=60%,h=center](traces_example.svgz)
