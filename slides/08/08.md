title: NPFL122, Lecture 8
class: title, langtech, cc-by-nc-sa
# TD3, SAC, TRPO, PPO

## Milan Straka

### November 23, 2020

---
section: TD3
# Twin Delayed Deep Deterministic Policy Gradient

The paper Addressing Function Approximation Error in Actor-Critic Methods by
Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which

~~~
- decrease maximization bias by training two critics and choosing minimum of
  their predictions;

~~~
- introduce several variance-lowering optimizations:
  - delayed policy updates;
  - target policy smoothing.

~~~

The TD3 algorithm has been together with SAC one of the state-of-the-art
algorithms for off-policy continuous-actions RL training (as of 2020).

---
# TD3 â€“ Maximization Bias

Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.
In Q-learning, the maximization bias was caused by the explicit $\max$ operator.
For DDPG methods, it can be caused by the gradient descent itself. Let
$â†’Î¸_\textit{approx}$ be the parameters maximizing the $q_â†’Î¸$ and let
$â†’Î¸_\textit{true}$ be the hypothetical parameters which maximise true $q_Ï€$,
and let $Ï€_\textit{approx}$ and $Ï€_\textit{true}$ denote the corresponding
policies.

~~~
Because the gradient direction is a local maximizer, for sufficiently small
$Î±<Îµ_1$ we have
$$ð”¼\big[q_â†’Î¸(s, Ï€_\textit{approx})\big] â‰¥ ð”¼\big[q_â†’Î¸(s, Ï€_\textit{true})\big].$$

~~~
However, for real $q_Ï€$ and for sufficiently small $Î±<Îµ_2$ it holds that
$$ð”¼\big[q_Ï€(s, Ï€_\textit{true})\big] â‰¥ ð”¼\big[q_Ï€(s, Ï€_\textit{approx})\big].$$

~~~
Therefore, if $ð”¼\big[q_â†’Î¸(s, Ï€_\textit{true})\big] â‰¥ ð”¼\big[q_Ï€(s, Ï€_\textit{true})\big]$,
for $Î± < \min(Îµ_1, Îµ_2)$
$$ð”¼\big[q_â†’Î¸(s, Ï€_\textit{approx})\big] â‰¥ ð”¼\big[q_Ï€(s, Ï€_\textit{approx})\big].$$

---
# TD3 â€“ Maximization Bias

![w=50%](td3_bias.svgz)![w=50%](td3_bias_dqac.svgz)

~~~
Analogously to Double DQN we could compute the learning targets using
the current policy and the target critic, i.e., $r + Î³ q_{â†’Î¸'}(s', Ï€_â†’Ï†(s'))$
(instead of using target policy and target critic as in DDPG), obtaining DDQN-AC algorithm.
However, the authors found out that the policy changes too slowly and the target
and current networks are too similar.

~~~
Using the original Double Q-learning, two pairs of actors and critics could be
used, with the learning targets computed by the opposite critic, i.e.,
$r + Î³ q_{â†’Î¸_2}(s', Ï€_{â†’Ï†_1}(s))$ for updating $q_{â†’Î¸_1}$. The resulting DQ-AC
algorithm is slightly better, but still suffering from oversetimation.

---
# TD3 â€“ Algorithm

The authors instead suggest to employ two critics and one actor. The actor is
trained using one of the critics, and both critics are trained using the same
target computed using the _minimum_ value of both critics as
$$r + Î³ \min_{i=1,2} q_{â†’Î¸'_i}(s', Ï€_{â†’Ï†'}(s')).$$

~~~
Furthermore, the authors suggest two additional improvements for variance
reduction.
- For obtaining higher quality target values, the authors propose to train the
  critics more often. Therefore, critics are updated each step, but the actor
  and the target networks are updated only every $d$-th step ($d=2$ is used in
  the paper).

~~~
- To explictly model that similar actions should lead to similar results,
  a small random noise is added to performed actions when computing the target
  value:
  $$r + Î³ \min_{i=1,2} q_{â†’Î¸'_i}(s', Ï€_{â†’Ï†'}(s') + Îµ)~~~\textrm{for}~~~
    Îµ âˆ¼ \operatorname{clip}(ð“(0, Ïƒ), -c, c).$$

---
# TD3 â€“ Algorithm

![w=43%,h=center](td3_algorithm.svgz)

---
# TD3 â€“ Algorithm

![w=80%,h=center](td3_hyperparameters.svgz)

---
# TD3 â€“ Results

![w=70%,h=center](td3_results_curves.svgz)
![w=70%,h=center](td3_results.svgz)

---
# TD3 â€“ Ablations

![w=100%,h=center](td3_ablations.svgz)
![w=100%,h=center](td3_ablations_dqac.svgz)

---
# TD3 â€“ Ablations

![w=65%,h=center](td3_ablations_results.svgz)

---
section: SAC
# Soft Actor Critic

The paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor by Tuomas Haarnoja et al. introduces
a different off-policy algorithm for continuous action space.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize.

---
# Soft Actor Critic
![w=60%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=90%](sac_results.svgz)

---
section: NPG
# Natural Policy Gradient

The following approach has been introduced by Kakade (2002).

~~~
Using policy gradient theorem, we are able to compute $âˆ‡ v_Ï€$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $â†’d$ which maximizes $v_Ï€(s; â†’Î¸ + â†’d)$ under
the constraint that $|â†’d|^2$ is bounded by a small constant is exactly
the gradient $âˆ‡ v_Ï€$.

~~~
Normally, the length $|â†’d|^2$ is computed using Euclidean metric. But in general,
any metric could be used. Representing a metric using a positive-definite matrix
$â‡‰G$ (identity matrix for Euclidean metric), we can compute the distance as
$|â†’d|^2 = âˆ‘_{ij} G_{ij} d_i d_j = â†’d^T â‡‰G â†’d$. The steepest ascent direction is
then given by $â‡‰G^{-1} âˆ‡ v_Ï€$.

~~~
Note that when $â‡‰G$ is the Hessian $â‡‰H v_Ï€$, the above process is exactly
Newton's method.

---
# Natural Policy Gradient

![w=100%,v=middle](npg.svgz)

---
# Natural Policy Gradient

A suitable choice for the metric is _Fisher information matrix_ defined as
$$F_s(â†’Î¸) â‰ ð”¼_{Ï€(a | s; â†’Î¸)} \left[\frac{âˆ‚ \log Ï€(a | s; â†’Î¸)}{âˆ‚ â†’Î¸_i} \frac{âˆ‚ \log Ï€(a | s; â†’Î¸)}{âˆ‚ â†’Î¸_j} \right]
\color{gray} = ð”¼[âˆ‡ Ï€(a | s; â†’Î¸)] ð”¼[âˆ‡ Ï€(a | s; â†’Î¸)]^T.$$

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

~~~
Recall Kullback-Leibler distance (or relative entropy) defined as
$$D_\textrm{KL}(â†’p || â†’q) â‰ âˆ‘_i p_i \log \frac{p_i}{q_i} \color{gray} = H(p, q) - H(p).$$

~~~
The Fisher information matrix is also a Hessian of the
$D_\textrm{KL}(Ï€(a | s; â†’Î¸) || Ï€(a | s; â†’Î¸')$:
$$F_s(â†’Î¸) = \frac{âˆ‚^2}{âˆ‚Î¸_i' âˆ‚Î¸_j'} D_\textrm{KL}(Ï€(a | s; â†’Î¸) || Ï€(a | s; â†’Î¸')\Big|_{â†’Î¸' = â†’Î¸}.$$

---
# Natural Policy Gradient

Using the metric
$$F(â†’Î¸) = ð”¼_{s âˆ¼ Î¼_â†’Î¸} F_s(â†’Î¸)$$
we want to update the parameters using $â†’d_F â‰ F(â†’Î¸)^{-1} âˆ‡ v_Ï€$.

~~~
An interesting property of using the $â†’d_F$ to update the parameters is that
- updating $â†’Î¸$ using $âˆ‡ v_Ï€$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $â†’Î¸$ using $F(â†’Î¸)^{-1} âˆ‡ v_Ï€$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $â†’d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$â†’d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $â‡‰Aâ†’x = â†’b$
in an iterative manner, by using $â‡‰A$ only to compute products $â‡‰Aâ†’v$ for
a suitable $â†’v$.

~~~
Therefore, $â†’d_F$ is found as a solution of
$$F(â†’Î¸)â†’d_F = âˆ‡ v_Ï€$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{Î±}{\sqrt{(âˆ‡ v_Ï€)^T F(â†’Î¸)^{-1} âˆ‡ v_Ï€}}.$$

---
section: TRPO
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $Ï€, Ï€Ìƒ$, we can write
$$v_Ï€Ìƒ = v_Ï€ + ð”¼_{s âˆ¼ Î¼(Ï€Ìƒ)} ð”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s),$$
where $a_Ï€(a | s)$ is the advantage function $q_Ï€(a | s) - v_Ï€(s)$ and
$Î¼(Ï€Ìƒ)$ is the on-policy distribution of the policy $Ï€Ìƒ$.

~~~
Analogously to policy improvement, we see that if $a_Ï€(a | s) â‰¥0$, policy
$Ï€Ìƒ$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s âˆ¼ Î¼(Ï€Ìƒ)$ is costly. Therefore, we instead
consider
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ð”¼_{s âˆ¼ Î¼(Ï€)} ð”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s).$$

---
# Trust Region Policy Optimization
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ð”¼_{s âˆ¼ Î¼(Ï€)} ð”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s)$$

It can be shown that for parametrized $Ï€(a | s; â†’Î¸)$ the $L_Ï€(Ï€Ìƒ)$ matches
$v_{Ï€Ìƒ}$ to the first order.

~~~
Schulman et al. additionally proves that if we denote
$Î± = D_\textrm{KL}^\textrm{max}(Ï€_\textrm{old} || Ï€_\textrm{new})
   = \max_s D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) || Ï€_\textrm{new}(â‹…|s)\big)$, then
$$v_{Ï€_\textrm{new}} â‰¥ L_{Ï€_\textrm{old}}(Ï€_\textrm{new}) - \frac{4ÎµÎ³}{(1-Î³)^2}Î±\textrm{~~~where~~~}Îµ = \max_{s, a} |a_Ï€(s, a)|.$$

~~~
Therefore, TRPO minimizes $L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)$ subject to
$D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) < Î´$, where
- $D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) = ð”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0})} [D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) || Ï€_\textrm{new}(â‹…|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $Î´$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $Ï€$.

---
# Trust Region Policy Optimization

$$\textrm{minimize}~~L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)~~\textrm{subject to}~~D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) < Î´$$

The parameters are updated using $â†’d_F = F(â†’Î¸)^{-1} âˆ‡ L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{Î´/(â†’d_F^T F(â†’Î¸)^{-1} â†’d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
# Proximal Policy Optimization

A simplification of TRPO which can be implemented using a few lines of code.

Let $r_t(â†’Î¸) â‰ \frac{Ï€(A_t|S_t; â†’Î¸)}{Ï€(A_t|S_t; â†’Î¸_\textrm{old})}$. PPO
minimizes the objective
$$L^\textrm{CLIP}(â†’Î¸) â‰ ð”¼_t\Big[\min\big(r_t(â†’Î¸) AÌ‚_t, \operatorname{clip}(r_t(â†’Î¸), 1-Îµ, 1+Îµ) AÌ‚_t)\big)\Big].$$

Such $L^\textrm{CLIP}(â†’Î¸)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $AÌ‚_t$ are additionally estimated using _generalized
advantage estimation_ (which we will talk in some future lecture)
instead of the $n$-step definition we used until now,
i.e., $AÌ‚_t = âˆ‘_{i=0}^{n-1} Î³^i R_{t+1+i} + Î³^{n} V(S_{t+n}) - V(S_t)$.

![w=80%,h=center](ppo_algorithm.svgz)

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)
