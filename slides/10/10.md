title: NPFL122, Lecture 10
class: title, langtech, cc-by-sa
# V-trace, PopArt Normalization, Partially Observable MDPs

## Milan Straka

### December 16, 2019

---
section: IMPALA
# IMPALA

Impala (**Imp**ortance Weighted **A**ctor-**L**earner **A**rchitecture) was
suggested in Feb 2018 paper and allows massively distributed implementation
of an actor-critic-like learning algorithm.

~~~
Compared to A3C-based agents, which communicates gradients with respect to the
parameters of the policy, IMPALA actors communicates trajectories to the
centralized learner.

~~~
![w=50%](impala_overview.pdf)
~~~ ~~
![w=50%](impala_overview.pdf)![w=50%](impala_comparison.pdf)

~~~
If many actors are used, the policy used to generate a trajectory can lag behind
the latest policy. Therefore, a new **V-trace** off-policy actor-critic
algorithm is proposed.

---
# IMPALA – V-trace

Consider a trajectory $(S_t, A_t, R_{t+1})_{t=s}^{t=s+n}$ generated by
a behaviour policy $b$.

~~~
A regular $n$-step bootstrap target is estimated using
$$v_s = ∑_{t=s}^{s+n-1} γ^{t-s} R_{t+1} + γ^N V(S_t).$$

~~~
This quantity can be rewritten using a series of single-step TD errors as
$$v_s = V(S_s) + ∑_{t=s}^{s+n-1} γ^{t-s} \Big(R_{t+1} + γ V(S_{t+1}) - V(S_t)\Big).$$

---
# IMPALA – V-trace

In order to devise an off-policy estimate, we utilize the usual
importance sampling ratio
$$ρ_t ≝ \frac{π(A_t | S_t)}{b(A_t | S_t)}.$$

~~~
We can consider the off-policy estimate to consist of two parts:
- independently on the probability of the action $A_s$, we can estimate
  the return by our current estimate $V(S_s)$;
~~~
- depending on the IS ratio, we can correct the estimate by adding
  the value TD error of $\big(R_{s+1} + γ V(S_{s+1}) - V(S_s)\big)$;

arriving at the following estimate
$$v_s = V(S_s) + ρ_s \Big(R_{s+1} + γ V(S_{s+1}) - V(S_s)\Big).$$

---
# IMPALA – V-trace

The $n$-step V-trace target for $S_s$ is defined as
$$v_s ≝ V(S_s) + ∑_{t=s}^{s+n-1} γ^{t-s} \left(∏\nolimits_{i=s}^{t-1} c_i\right) δ_t V,$$
~~~
where $δ_t V$ is the temporal difference for V
$$δ_t V ≝ ρ_t \big(R_{t+1} + γV(s_{t+1}) - V(s_t)\big),$$
~~~
and $ρ_t$ and $c_i$ are truncated importance sampling ratios with $ρ̄ ≥ c̄$:
$$ρ_t ≝ \min\left(ρ̄, \frac{π(A_t | S_t)}{b(A_t | S_t)}\right),~~~~c_i ≝ \min\left(c̄, \frac{π(A_i | S_i)}{b(A_i | S_i)}\right).$$

~~~
Note that if $b=π$ and assuming $c̄ ≥ 1$, $v_s$ reduces to $n$-step Bellman
target.

---
# IMPALA – V-trace

Note that the truncated IS weights $ρ_t$ and $c_i$ play different roles:

~~~
- The $ρ_t$ appears in the definition of $δ_t V$ and defines the fixed point
  of the update rule. For $ρ̄=∞$, the target is the value function $v_π$,
  if $ρ̄<∞$, the fixed point is somewhere between $v_π$ and $v_b$. Notice that
  we do not compute a product of these $ρ_t$ coefficients.

~~~
  Concretely, it can be proven that the fixed point of the value function
  $v_s$ is the policy
  $$π_ρ̄(a|x) ∝ \min\big(ρ̄b(a|s), π(a|s)\big).$$
~~~
- The $c_i$ impacts the speed of convergence (the contraction rate of the
  Bellman operator), not the sought policy. Because a product of the $c_i$
  ratios is computed, it plays an important role in variance reduction.

~~~
However, the paper utilizes $c̄=1$ and out of $ρ̄ ∈ \{1, 10, 100\}$, $ρ̄=1$ works
empirically the best, so the distinction between $c$ and $ρ$ is not useful in
practise.

---
# IMPALA – V-trace

It is easy to see that the defined $n$-step V-trace target
$$v_s ≝ V(S_s) + ∑_{t=s}^{s+n-1} γ^{t-s} \left(∏\nolimits_{i=s}^{t-1} c_i\right) δ_t V$$

can be computed recursively as

$$v_s ≝ V(S_s) + δ_sV + γ c_s \Big(v_{s+1} - V(S_{s+1})\Big),$$
which is the form usually used for implementation.

---
# IMPALA – V-trace

Consider a parametrized functions computing $v(s; →θ)$ and $π(a|s; →ω)$,
we update the critic in the direction of
$$\Big(v_s - v(S_s; →θ)\Big) ∇_→θ v(S_s; →θ)$$

~~~
and the actor in the direction of the policy gradient
$$ρ_s ∇_→ω \log π(A_s | S_s; →ω)\big(R_{s+1} + γ v_{s+1} - v(S_s; →θ)\big),$$
where we estimate $Q_π(S_s, A_s)$ as $R_{s+1} + γ v_{s+1}$.

~~~
Finally, we again add the entropy regularization term $H(π(⋅ | S_s; →θ))$ to the
loss function.

---
# IMPALA

![w=60%,h=center](impala_throughput.pdf)

---
# IMPALA – Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $ε$ and the
global gradient norm clipping threshold.

~~~
![w=80%,h=center](pbt_overview.pdf)

---
# IMPALA – Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $ε$ and the
global gradient norm clipping threshold.

In population based training, several agents are trained in parallel. When an
agent is _ready_ (after 5000 episodes), then:
~~~
- it may be overwritten by parameters and hyperparameters of another agent, if
  it is sufficiently better (5000 episode mean capped human normalized score returns
  are 5% better);
~~~
- and independently, the hyperparameters may undergo a change (multiplied by
  either 1.2 or 1/1.2 with 33% chance).

---
# IMPALA – Architecture
![w=80%,h=center](impala_architecture.pdf)

---
# IMPALA

![w=100%,v=middle](impala_results.pdf)

---
# IMPALA – Learning Curves

![w=32%,h=center](impala_curves.pdf)

---
# IMPALA – Atari Games

![w=60%,h=center,v=middle](impala_results_atari.pdf)

---
# IMPALA – Atari Hyperparameters

![w=52%,h=center](impala_hyperparameters.pdf)

---
# IMPALA – Ablations

![w=60%,f=right](impala_ablations_table.pdf)

- **No-correction**: no off-policy correction
- **$ε$-correction**: add a small value $ε=10^{-6}$
  during gradient calculation to prevent $π$ to be
  very small and lead to unstabilities during $\log π$
  computation
- **1-step**: use $V(S_s)$ instead of $v_s$, but utilize
  $ρ_s$ in the policy gradient update

---
# IMPALA – Ablations

![w=63%,mw=80%,h=center,f=right](impala_ablations_graphs.pdf)

The effect of the policy lag (the number of updates the
actor is behind the learned policy) on the performance.

---
section: PopArt Normalization
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; →θ, σ, μ)$ is computed using a normalized value
predictor $n(s; →θ)$
$$v(s; →θ, σ, μ) ≝ σ n(s; →θ) + μ$$
and further assume that $n(s; →θ)$ is an output of a linear function
$$n(s; →θ) ≝ →ω^T f(s; →θ-\{→ω, b\}) + b.$$

~~~
We can update the $σ$ and $μ$ using exponentially moving average with decay rate
$β$ (in the paper, first moment $μ$ and second moment $υ$ is tracked, and
standard deviation is computed as $σ=\sqrt{υ-μ^2}$; decay rate $β=3 ⋅ 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $μ$ and $σ$, we can normalize the observed (unnormalized) returns as
$(G - μ) / σ$ and use an actor-critic algorithm with advantage $(G - μ)/σ - n(S; →θ)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $→ω, b$ computing the
unnormalized value estimate are updated under any change $μ → μ'$ and $σ → σ'$ as:
$$→ω' ≝ \frac{σ}{σ'}→ω,~~~~b' ≝ \frac{σb + μ - μ'}{σ'}.$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $→μ$, $→σ$ and $→n(s; →θ)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.pdf)

~~~
![w=100%](popart_atari_curves.pdf)

---
# PopArt Results

![w=90%,h=center](popart_atari_statistics.pdf)

---
# PopArt Results

![w=100%,v=middle](popart_dmlab_curves.pdf)

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2-recurrent_staleness.pdf)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2-results.pdf)![w=65%](r2d2-result_table.pdf)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2-hyperparameters.pdf)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2-training_progress.pdf)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2-ablations.pdf)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2-memory_size.pdf)

---
section: POMDPs
# Partially Observable MDPs

Recall that a _Markov decision process_ (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
- $𝓐$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a _reward_ $r ∈ ℝ$,
- $γ ∈ [0, 1]$ is a _discount factor_.

~~~
_Partially observable Markov decision process_ extends the Markov decision
process to a sextuple $(𝓢, 𝓐, p, γ, 𝓞, o)$, where in addition to an MDP
- $𝓞$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model.

~~~
In robotics (out of the domain of this course), several approaches are used to
handle POMDPs, to model uncertainty, imprecise mechanisms and inaccurate
sensors.

---
# Partially Observable MDPs

In Deep RL, partially observable MDPs are usually handled using recurrent
networks. After suitable encoding of input observation $O_t$ and previous
action $A_{t-1}$, a RNN (usually LSTM) unit is used to model the current $S_t$
(or its suitable latent representation), which is in turn utilized to produce
$A_t$.

![w=50%,h=center](merlin_rl-lstm.pdf)

---
section: MERLIN
# MERLIN

However, keeping all information in the RNN state is substantially limiting.
Therefore, _memory-augmented_ networks can be used to store suitable information
in external memory (in the lines of NTM, DNC or MANN models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=50%,h=center](merlin_rl-mem.pdf)

---
# MERLIN – Memory Module

![w=30%,f=right](merlin_rl-mem.pdf)

Let $→M$ be a memory matrix of size $N_\textit{mem} × 2|z|$.

~~~
Assume we have already encoded observations as $→e_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
by a deep LSTM (two layers are used in the paper) to compute $→h_t$.

~~~
Then, we apply a linear layer to $→h_t$, computing $K$ key vectors
$→k_1, … →k_K$ of length $2|z|$ and $K$ positive scalars $β_1, …, β_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $→k_i$ and all memory
rows $M_j$, multiply the similarities by $β_i$ and pass them through a $\softmax$
to obtain weights $→ω_i$. The read vector is then computed as $⇉M →w_i$.

~~~
**Writing:** We find one-hot write index $→v_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $→v_\textit{ret} ← γ →v_\textit{ret} + (1 - γ) →v_\textit{wr}$, and update
the memory matrix using $⇉M ← ⇉M + →v_\textit{wr}[→e_t, 0] + →v_\textit{ret}[0, →e_t]$.

---
# MERLIN — Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therfore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $z$ and storing them in memory.

~~~
According to the paper, the idea of unsupervised and predictive modeling has
been entertained for decades, and recent discussions have proposed such modeling
to be connected to hippocampal memory.

We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a _prior_ distribution over $z_t$ predicts next state variable
conditioned on history of state variables and actions $p(z_t | z_{t-1}, a_{t-1}, …, z_1, a_1)$,
and _posterior_ corrects the prior using the new observation $o_t$, forming
a better estimate $q(z_t | o_t, z_{t-1}, a_{t-1}, …, z_1, a_1)$.

---
# MERLIN — Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and posterior to the loss, to ensure
  consistency between the prior and posterior.

~~~
![w=85%,h=center](merlin_diagram.pdf)

---
# MERLIN — Algorithm

![w=37%,h=center](merlin_algorithm.pdf)

---
# MERLIN

![w=70%,h=center](merlin_tasks.pdf)

---
# MERLIN

![w=50%,h=center](merlin_analysis.pdf)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.pdf)

---
section: CTF-FTW
# For the Win agent for Capture The Flag

![w=100%](ctf_overview.pdf)

---
# For the Win agent for Capture The Flag

- Extension of the MERLIN architecture.

~~~
- Hierarchical RNN with two timescales.

~~~
- Population based training controlling KL divergence penalty weights,
  slow ticking RNN speed and gradient flow factor from fast to slow RNN.

---
# For the Win agent for Capture The Flag

![w=47%,h=center](ctf_architecture.pdf)

---
# For the Win agent for Capture The Flag

![w=80%,h=center](ctf_curves.pdf)

