title: NPFL122, Lecture 9
class: title, langtech, cc-by-nc-sa
# Eligibility Traces, Impala, R2D2, Agent57

## Milan Straka

### November 29, 2021

---
section: Refresh
# Off-policy Correction Using Control Variates

Denoting the TD error as $Î´_t â‰ R_{t+1} + Î³V(S_{t+1}) - V(S_t)$, we can
write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i Î´_{t+i}.$$

~~~
Furthermore, denoting the importance sampling ratio
$Ï_t â‰ \frac{Ï€(A_t | S_t)}{b(A_t | S_t)},~~~Ï_{t:t+n} â‰ âˆ_{i=0}^n Ï_{t+i},$
~~~
we can introduce the **control variate** to the estimate
$$G_{t:t+n}^\mathrm{CV} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t),$$

~~~
which can then be written as
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Ï_{t:t+i} Î´_{t+i}.$$

---
section: ETraces
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $Î»$-return

For a given $Î» âˆˆ [0,1]$, we define **$Î»$-return** as
$$G_t^Î» â‰ (1 - Î») âˆ‘_{i=1}^âˆ Î»^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](../08/traces_weighting.svgz)

~~~
Alternatively, the $Î»$-return can be written recursively as
$$\begin{aligned}
G_t^Î» &= (1 - Î») G_{t:t+1} \\
      &+ Î» (R_{t+1} + Î³ G_{t+1}^Î»).
\end{aligned}$$

---
# $Î»$-return

In an episodic task with time of termination $T$, we can rewrite the $Î»$-return
to
$$G_t^Î» = (1 - Î») âˆ‘_{i=1}^{T-t-1} Î»^{i-1} G_{t:t+i} + Î»^{T-t-1} G_t.$$

~~~
![w=60%,h=center](../08/traces_example.svgz)

---
# Truncated $Î»$-return

We might also set a limit on the largest value of $n$, obtaining
**truncated $Î»$-return**
$$G_{t:t+n}^Î» â‰ (1 - Î») âˆ‘_{i=1}^{n-1} Î»^{i-1} G_{t:t+i} + Î»^{n-1} G_{t:t+n}.$$

~~~
The truncated $Î»$ return can be again written recursively as

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»),~~G_{t:t+1}^Î» = G_{t:t+1}.$$

~~~
Similarly to before, we can express the truncated $Î»$ return as a sum of TD
errors

$$\begin{aligned}
  G_{t:t+n}^Î» - V(S_t)
  & = (1 - Î») \big(R_{t+1} + Î³V(S_{t+1})\big) + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î») - V(S_t) \\
  & = R_{t+1} + Î³V(S_{t+1}) - V(S_t) + Î» Î³ \big(G_{t+1:t+n}^Î» - V(S_{t+1})\big),
\end{aligned}$$

~~~
obtaining an analogous estimate $G_{t:t+n}^Î» = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Î´_{t+i}.$

---
# Variable $Î»$s

The (truncated) $Î»$-return can be generalized to utilize different $Î»_i$ at each
step $i$. Notably, we can generalize the recursive definition

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»)$$

~~~
to
$$G_{t:t+n}^{Î»_i} = (1 - Î»_{t+1}) G_{t:t+1} + Î»_{t+1} (R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i}),$$

~~~
and express this quantity again by a sum of TD errors:

$$G_{t:t+n}^{Î»_i} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left(âˆ_{j=1}^i Î»_{t+j}\right) Î´_{t+i}.$$

---
# Off-policy Traces with Control Variates

Finally, we can combine the eligibility traces with off-policy estimation using
control variates:
$$G_{t:t+n}^{Î»,\mathrm{CV}} â‰ (1 - Î») âˆ‘_{i=1}^{n-1} Î»^{i-1} G_{t:t+i}^\mathrm{CV} + Î»^{n-1} G_{t:t+n}^\mathrm{CV}.$$

~~~
Recalling that
$$G_{t:t+n}^\mathrm{CV} = Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t),$$
~~~

we can rewrite $G_{t:t+n}^{Î»,\mathrm{CV}}$ recursively as
$$G_{t:t+n}^{Î»,\mathrm{CV}} = (1 - Î») G_{t:t+1}^\mathrm{CV} + Î» \Big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\Big),$$

~~~
which we can simplify by expanding $G_{t:t+1}^\mathrm{CV}=Ï_t(R_{t+1} + Î³V(S_{t+1})) + (1-Ï_t)V(S_t)$ to
$$G_{t:t+n}^{Î»,\mathrm{CV}} - V(S_t) = Ï_t \big(R_{t+1} + Î³V(S_{t+1}) - V(S_t)\big) + Î³Î»Ï_t \big(G_{t+1:t+n}^{Î»,\mathrm{CV}} - V(S_{t+1})\big).$$

---
# Off-policy Traces with Control Variates

Consequently, analogously as before, we can write the off-policy traces estimate
with control variates as

$$G_{t:t+n}^\mathrm{Î»,CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Ï_{t:t+i} Î´_{t+i},$$

~~~
and by repeating the above derivation we can extend the result also for time-variable $Î»_i$, we obtain
$$G_{t:t+n}^\mathrm{Î»_i,CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i \left(âˆ_{j=1}^i Î»_{t+j}\right) Ï_{t:t+i} Î´_{t+i}.$$

---
section: RetRecap
class: tablewide
# Return Recapitulation

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} â‰ R_{t+1} + Î³ G_{t+1:t+n}$                                                         | $V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i Î´_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t)$ | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Ï_{t:t+i} Î´_{t+i}$ |
| $G_{t:t+n}^Î» â‰ (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»)$                             | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Î´_{t+i}$       |
| $G_{t:t+n}^{Î»_i} â‰ (1 - Î»_{t+1}) G_{t:t+1} + Î»_{t+1} (R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i})$         | $V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left({\scriptstyle âˆ_{j=1}^i Î»_{t+j}}\right) Î´_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{Î»,\mathrm{CV}} &â‰ (1 - Î») G_{t:t+1}^\mathrm{CV} \\&+ Î» \big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\big)\end{aligned}$ | $V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Ï_{t:t+i} Î´_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{Î»_i,\mathrm{CV}} &â‰ (1 - Î»_{t+1}) G_{t:t+1}^\mathrm{CV} \\+& Î»_{t+1} \big(Ï_t\big(R_{t+1} + Î³ G_{t+1:t+n}^{Î»_i,\mathrm{CV}}\big) + (1-Ï_t)V(S_t)\big)\end{aligned}$ | $\begin{aligned}&V(S_t)\\&\textstyle + âˆ‘\nolimits_{i=0}^{n-1} Î³^i \left({\scriptstyle âˆ_{j=1}^i Î»_{t+j}}\right) Ï_{t:t+i} Î´_{t+i}\end{aligned}$ |

---
section: TD($Î»$)
# TD($Î»$)

We have defined the $Î»$-return in the so-called **forward view**.

![w=80%,h=center,mh=80%,v=middle](traces_forward.svgz)

---
# TD($Î»$)

However, to allow on-line updates, we might consider also the **backward view**

![w=80%,h=center,mh=80%,v=middle](traces_backward.svgz)

---
# TD($Î»$)

TD($Î»$) is an algorithm implementing on-line policy evaluation utilizing the
backward view.

![w=80%,h=center](traces_td_lambda.svgz)

---
section: Vtrace
# V-trace

V-trace is a modified version of $n$-step return with off-policy correction,
defined in the Feb 2018 IMPALA paper as (using the notation from the paper):

$$G_{t:t+n}^\textrm{V-trace} â‰ V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left(âˆ\nolimits_{j=0}^{i-1} cÌ„_{t+j}\right) ÏÌ„_{t+i} Î´_{t+i},$$

~~~
where $ÏÌ„_t$ and $cÌ„_t$ are the truncated importance sampling ratios for $ÏÌ„ â‰¥ cÌ„$:
$$ÏÌ„_t â‰ \min\left(ÏÌ„, \frac{Ï€(A_t | S_t)}{b(A_t | S_t)}\right),~~~~cÌ„_t â‰ \min\left(cÌ„, \frac{Ï€(A_t | S_t)}{b(A_t | S_t)}\right).$$

~~~
Note that if $b=Ï€$ and assuming $cÌ„ â‰¥ 1$, $v_s$ reduces to $n$-step Bellman
target.

---
# V-trace

Note that the truncated IS weights $ÏÌ„_t$ and $cÌ„_t$ play different roles:

~~~
- The $ÏÌ„_t$ appears defines the fixed point of the update rule. For $ÏÌ„=âˆ$, the
  target is the value function $v_Ï€$, if $ÏÌ„<âˆ$, the fixed point is somewhere
  between $v_Ï€$ and $v_b$. Notice that we do not compute a product of these
  $ÏÌ„_t$ coefficients.

~~~
  Concretely, the fixed point of an operator defined by $G_{t:t+n}^\textrm{V-trace}$
  corresponds to a value function of the policy
  $$Ï€_ÏÌ„(a|s) âˆ \min\big(ÏÌ„b(a|s), Ï€(a|s)\big).$$

~~~
- The $cÌ„_t$ impacts the speed of convergence (the contraction rate of the
  Bellman operator), not the sought policy. Because a product of the $cÌ„_t$
  ratios is computed, it plays an important role in variance reduction.

~~~
However, the paper utilizes $cÌ„=1$ and out of $ÏÌ„ âˆˆ \{1, 10, 100\}$, $ÏÌ„=1$ works
empirically the best, so the distinction between $cÌ„_t$ and $ÏÌ„_t$ is not useful in
practice.

---
# V-trace Analysis

Let us define the (untruncated for simplicity; similar results can be proven for
a truncated one) V-trace operator $ğ“¡$ as:
$$ğ“¡ V(S_t) â‰ V(S_t) + ğ”¼_b \left[âˆ‘_{i â‰¥ 0} Î³^i \left(âˆ\nolimits_{j=0}^{i-1} cÌ„_{t+j}\right) ÏÌ„_{t+i} Î´_{t+i}\right],$$
where the expectation $ğ”¼_b$ is with respect to trajectories generated by behaviour policy $b$.
~~~
Assume there exists $Î² âˆˆ (0, 1]$ such that $ğ”¼_b ÏÌ„_0 â‰¥ Î²$.
~~~

It can be proven (see Theorem 1 in Appendix A.1 in the Impala paper if interested) that
such an operator is a contraction with a contraction constant
$$Î³^{-1} - \big(1 - Î³\big) âˆ‘_{i â‰¥ 0} Î³^i ğ”¼_b \left[\left(âˆ\nolimits_{j=0}^{i-1} cÌ„_j\right) ÏÌ„_i \right] â‰¤ 1-(1-Î³)Î²<1,$$
therefore, $ğ“¡$ has a unique fixed point.

---
# V-trace Analysis

We now prove that the fixed point of $ğ“¡$ is $V^{Ï€_ÏÌ„}$. We have:
$$\begin{aligned}
  & ğ”¼_b \Big[ ÏÌ„_t\big(R_{t+1} + Î³ V^{Ï€_ÏÌ„}(S_{t+1}) - V^{Ï€_ÏÌ„}(S_t)\big)\big| S_t\Big] \\
  &= âˆ‘_a b(a|S_t) \min\left(\bar \rho, \frac{Ï€(a|S_t)}{b(a|S_t)} \right) \Big[R_{t+1} + Î³ ğ”¼_{s' âˆ¼ p(S_t, a)} V^{Ï€_ÏÌ„}(s') - V^{Ï€_ÏÌ„}(S_t)\Big] \\
  &= \underbrace{âˆ‘_a Ï€_ÏÌ„(a|S_t) \Big[R_{t+1} + Î³ ğ”¼_{s' âˆ¼ p(S_t, a)} V^{Ï€_ÏÌ„}(s') - V^{Ï€_ÏÌ„}(S_t)\Big]}_{=0} âˆ‘_{a'} \min\big(ÏÌ„ b(a'|S_t), Ï€(a'|S_t) \big), \\
\end{aligned}$$
where the tagged part is zero, since it is the Bellman equation for $V^{Ï€_ÏÌ„}$.
~~~
This shows that $ğ“¡ V^{Ï€_ÏÌ„} = V^{Ï€_ÏÌ„}$, and therefore $V^{Ï€_ÏÌ„}$ is the unique fixed point of $ğ“¡$.

~~~
Consequently, in
$G_{t:t+n}^\mathrm{Î»_i,CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i \left(\scriptstyle âˆ_{j=1}^i Î»_{t+j}\right) Ï_{t:t+i} Î´_{t+i},$
only the last $Ï_{t+i}$ from every $Ï_{t:t+i}$ is actually needed for off-policy
correction; $Ï_{t:t+i-1}$ can be considered as traces.

---
section: IMPALA
# IMPALA

Impala (**Imp**ortance Weighted **A**ctor-**L**earner **A**rchitecture) was
suggested in Feb 2018 paper and allows massively distributed implementation
of an actor-critic-like learning algorithm.

~~~
Compared to A3C-based agents, which communicate gradients with respect to the
parameters of the policy, IMPALA actors communicate trajectories to the
centralized learner.

~~~
![w=50%](impala_overview.svgz)
~~~ ~~
![w=50%](impala_overview.svgz)![w=50%](impala_comparison.svgz)

~~~
If many actors are used, the policy used to generate a trajectory can lag behind
the latest policy. Therefore, the V-trace off-policy actor-critic
algorithm is employed.

---
# IMPALA

Consider a parametrized functions computing $v(s; â†’Î¸)$ and $Ï€(a|s; â†’Ï‰)$,
we update the critic in the direction of
$$\Big(G_{t:t+n}^\textrm{V-trace} - v(S_t; â†’Î¸)\Big) âˆ‡_{â†’Î¸} v(S_t; â†’Î¸),$$

~~~
and the actor in the direction of the policy gradient
$$ÏÌ„_t âˆ‡_{â†’Ï‰} \log Ï€(A_t | S_t; â†’Ï‰)\big(R_{t+1} + Î³G_{t+1:t+n}^\textrm{V-trace} - v(S_t; â†’Î¸)\big),$$
where we estimated $Q^Ï€(S_t, A_t)$ as $R_{t+1} + Î³G_{t+1:t+n}^\textrm{V-trace}$.

~~~
Finally, we again add the entropy regularization term $Î² H\big(Ï€(â‹… | S_t; â†’Ï‰)\big)$ to the
loss function.

---
# IMPALA

![w=60%,h=center](impala_throughput.svgz)

---
# IMPALA â€“ Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the
global gradient norm clipping threshold.

~~~
![w=80%,h=center](pbt_overview.svgz)

---
# IMPALA â€“ Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the
global gradient norm clipping threshold.

In population based training, several agents are trained in parallel. When an
agent is _ready_ (after 5000 episodes), then:
~~~
- it may be overwritten by parameters and hyperparameters of another randomly
  chosen agent, if it is sufficiently better (5000 episode mean capped human
  normalized score returns are 5% better);
~~~
- and independently, the hyperparameters may undergo a change (multiplied by
  either 1.2 or 1/1.2 with 33% chance).

---
# IMPALA â€“ Architecture
![w=80%,h=center](impala_architecture.svgz)

---
# IMPALA

![w=100%,v=middle](impala_results.svgz)

---
# IMPALA â€“ Learning Curves

![w=32%,h=center](impala_curves.svgz)

---
# IMPALA â€“ Atari Games

![w=60%,h=center,v=middle](impala_results_atari.svgz)

---
# IMPALA â€“ Atari Hyperparameters

![w=52%,h=center](impala_hyperparameters.svgz)

---
# IMPALA â€“ Ablations

![w=60%,f=right](impala_ablations_table.svgz)

- **No-correction**: no off-policy correction;
- **$Îµ$-correction**: add a small value $Îµ=10^{-6}$
  during gradient calculation to prevent $Ï€$ to be
  very small and lead to unstabilities during $\log Ï€$
  computation;
- **1-step**: no off-policy correction in update of the value function,
  TD errors are multiplied by the corresponding $Ï$ but no $c$s;
  it can be considered V-trace â€œwithout tracesâ€.

---
# IMPALA â€“ Ablations

![w=63%,mw=80%,h=center,f=right](impala_ablations_graphs.svgz)

The effect of the policy lag (the number of updates the
actor is behind the learned policy) on the performance.

---
section: PopArt
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; â†’Î¸, Ïƒ, Î¼)$ is computed using a normalized value
predictor $n(s; â†’Î¸)$
$$v(s; â†’Î¸, Ïƒ, Î¼) â‰ Ïƒ n(s; â†’Î¸) + Î¼$$
and further assume that $n(s; â†’Î¸)$ is an output of a linear function
$$n(s; â†’Î¸) â‰ â†’Ï‰^T f(s; â†’Î¸-\{â†’Ï‰, b\}) + b.$$

~~~
We can update the $Ïƒ$ and $Î¼$ using exponentially moving average with decay rate
$Î²$ (in the paper, first moment $Î¼$ and second moment $Ï…$ is tracked, and
the standard deviation is computed as $Ïƒ=\sqrt{Ï…-Î¼^2}$; decay rate $Î²=3 â‹… 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $Î¼$ and $Ïƒ$, we can normalize the observed (unnormalized) returns as
$(G - Î¼) / Ïƒ$ and use an actor-critic algorithm with advantage $(G - Î¼)/Ïƒ - n(S; â†’Î¸)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $â†’Ï‰, b$ used to compute the
value estimate
$$v(s; â†’Î¸, Ïƒ, Î¼) â‰ Ïƒ \Big(â†’Ï‰^T f(s; â†’Î¸-\{â†’Ï‰, b\}) + b\Big) + Î¼$$
are updated under any change $Î¼ â†’ Î¼'$ and $Ïƒ â†’ Ïƒ'$ as
$$\begin{aligned}
  â†’Ï‰' &â† \frac{Ïƒ}{Ïƒ'}â†’Ï‰,\\
  b' &â† \frac{Ïƒb + Î¼ - Î¼'}{Ïƒ'}.
\end{aligned}$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $â†’Î¼$, $â†’Ïƒ$ and $â†’n(s; â†’Î¸)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.svgz)

~~~
![w=100%](popart_atari_curves.svgz)

---
# PopArt Results

![w=85%,h=center](popart_atari_statistics.svgz)

Normalization statistics on chosen environments.

---
section: TransRews
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $ğ“£$
$$(ğ“£q)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[r + Î³ \max_{a'} q(s', a')\Big].$$

~~~
Instead of clipping the magnitude of rewards, we might use a function
$h: â„ â†’ â„$ to reduce their scale. We define a transformed Bellman operator
$ğ“£_h$ as
$$(ğ“£_hq)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[h\Big(r + Î³ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = Î± z$ for $Î± > 0$, then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_* = Î± q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $Î±$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ ğ“£ q_* = h \circ ğ“£(h^{-1} \circ h \circ q_*) = ğ“£_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

The authors use the following transformation for the Atari environments
$$h(x) â‰ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + Îµx$$
with $Îµ = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4Îµ (|x| + 1 + Îµ)} - 1}{2Îµ} \right)^2 - 1\right).$$

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Rewards are transformed instead of clipped, and no loss-of-life-as-episode-end
heuristic is used.

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2_recurrent_staleness.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $Î³ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2_memory_size.svgz)

---
section: Agent57
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
# Retrace

$\displaystyle \mathrlap{ğ“¡q(s, a) â‰ q(s, a) + ğ”¼_b \bigg[âˆ‘_{tâ‰¥0} Î³^t \left(âˆ\nolimits_{j=1}^t c_t\right)
  \Big(R_{t+1} + Î³ğ”¼_{A_{t+1} âˆ¼ Ï€} q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\Big)\bigg],}$

where there are several possibilities for defining the traces $c_t$:
~~~
- **importance sampling**, $c_t = Ï_t = \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$,
  - the usual off-policy correction, but with possibly very high variance,
  - note that $c_t = 1$ in the on-policy case;
~~~
- **Tree-backup TB(Î»)**, $c_t = Î» Ï€(A_t|S_t)$,
  - the Tree-backup algorithm extended with traces,
  - however, $c_t$ can be much smaller than 1 in the on-policy case;
~~~
- **Retrace(Î»)**, $c_t = Î» \min\big(1, \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}\big)$,
  - off-policy correction with limited variance, with $c_t = 1$ in the on-policy case.

~~~
The authors prove that $ğ“¡$ has a unique fixed point $q_Ï€$ for any
$0 â‰¤ c_t â‰¤ \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$.

---
# Never Give Up

![w=78%](ngu_novelty.png)![w=19%](ngu_betas_gammas.svgz)

The NGU agent uses transformed Retrace loss and considers rewards $r_t^Î² â‰ r_t^e + Î² r_t^i$ for

$\displaystyle\kern5em
  r_t^i â‰ r_t^\textrm{episodic} â‹… \operatorname{clip}\Big(1 â‰¤ Î±_t=1 + \tfrac{(gÌ‚ - g)^2 - Î¼_\textrm{err}}{Ïƒ_\textrm{err}} â‰¤ L=5\Big).$

~~~
We train a parametrized action-value function $q(s, a, Î²_i)$ corresponding to
reward $r_t^{Î²_i}$ for $Î²_0=0$ and $Î³_0=0.997, â€¦, Î²_{N-1}=Î²$ and $Î³_{N-1}=0.99$,
utilizing $q(s, a, 0)$ for evaluation.

---
# Never Give Up

![w=73%,h=center](ngu_results_table.svgz)
![w=75%,h=center](ngu_results.svgz)

---
# Never Give Up Ablations

![w=73%,h=center](ngu_ablations_embeddings.svgz)
![w=64%,h=center](ngu_ablations.svgz)

---
# Agent57

![w=32%,f=right](agent57_architecture.png)

Then Agent57 improves NGU with:
~~~
- splitting the action-value as $q(s, a, j; â†’Î¸) â‰ q(s, a, j; â†’Î¸^e) + Î²_j q(s, a, j; â†’Î¸^i)$, where

  - $q(s, a, j; â†’Î¸^e)$ is trained with $r_e$ as targets, and
  - $q(s, a, j; â†’Î¸^i)$ is trained with $r_i$ as targets.

~~~
  In fact, the transformed rewards are used as $q(s, a, j; â†’Î¸) â‰ h(h^{-1} q(s, a, j; â†’Î¸^e) + Î²_j h^{-1} q(s, a, j; â†’Î¸^i))$.

~~~
- instead of considering all $(Î²_j, Î³_j)$ equal, we train a meta-controller
  using a non-stationary multi-arm bandit algorithm, where arms correspond
  to the choice of $j$ for a whole episode (so an actor first samples a $j$
  using multi-arm bandit problem and then updates it according to the observed
  return);

~~~
- $Î³_{N-1}$ is increased from $0.997$ to $0.9999$.

---
# Agent57 â€“ Results

![w=35%,h=center](agent57_results.svgz)
![w=89%,h=center](agent57_results_table.svgz)

---
# Agent57 â€“ Ablations

![w=56%](agent57_ablations.svgz)![w=44%](agent57_ablations_arm.svgz)
