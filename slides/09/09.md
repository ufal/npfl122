title: NPFL122, Lecture 9
class: title, langtech, cc-by-nc-sa
# Eligibility traces, V-trace, IMPALA

## Milan Straka

### November 30, 2020

---
section: ControlVarietes
# Off-policy Correction Using Control Varietes

Let $G_{t:t+n}$ be the estimated $n$-step return
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + \big[\mathrm{episode~still~running~in~}t+n] Î³^n V(S_{t+n}),$$
which can be written recursively as
$$G_{t:t+n} \begin{cases}
  0 & \mathrm{if~episode~ended~before~}t, \\
  V(S_t) & \mathrm{if~}n=0, \\
  R_{t+1} + Î³ G_{t+1:t+n} & \mathrm{otherwise}.
\end{cases}$$

---
# Off-policy Correction Using Control Varietes

Note that we can write
$$\begin{aligned}
G_{t:t+n} - V(S_t)
  &= R_{t+1} + Î³ G_{t+1:t+n} - V(S_t) \\
  &= R_{t+1} + Î³ \big(G_{t+1:t+n} - V(S_{t+1})\big) + Î³V(S_{t+1}) - V(S_t),
\end{aligned}$$
which yields
$$G_{t:t+n} - V(S_t) = R_{t+1} + Î³V(S_{t+1}) - V(S_t) + Î³\big(G_{t+1:t+n} - V(S_{t+1})\big).$$

Denoting the TD error as $Î´_t â‰ R_{t+1} + Î³V(S_{t+1}) - V(S_t)$, we can
therefore write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i Î´_{t+i}.$$

---
# Off-policy Correction Using Control Varietes

Now consider applying the IS off-policy correction to $G_{t:t+n}$ using the
importance sampling ratio
$$Ï_t â‰ \frac{Ï€(A_t | S_t)}{b(A_t | S_t)},~~~Ï_{t:t+n} â‰ âˆ_{i=0}^n Ï_{t+i}.$$

~~~
First note that
$$ğ”¼_{A_t âˆ¼ b} \big[Ï_t\big] = âˆ‘_{A_t} b(A_t | S_t) \frac{Ï€(A_t | S_t)}{b(A_t | S_t)},$$
which can be extended to
$$ğ”¼_Î¼ \big[Ï_{t:t+n}\big] = 1.$$

---
# Off-policy Correction Using Control Varietes

Until now, we used
$$G_{t:t+n}^\mathrm{IS} â‰ Ï_{t:t+n-1} G_{t:t+n}.$$

~~~
However, such correction has unnecessary variance. Notably, when expanding
$G_{t:t+n}$
$$G_{t:t+n}^\mathrm{IS} = Ï_{t:t+n-1} \big(R_{t+1} + Î³ G_{t+1:t+n}\big),$$
the $R_{t+1}$ depends only on $Ï_t$, not on $Ï_{t+1:t+n}$, and given that
the expectation of the importance sampling ratio is 1, we can simplify to
$$G_{t:t+n}^\mathrm{IS} = Ï_t R_{t+1} + Ï_{t:t+n-1} Î³ G_{t+1:t+n}.$$

~~~
Such an estimate can be written recursively as
$$G_{t:t+n}^\mathrm{IS} = Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{IS}\big).$$

---
# Off-policy Correction Using Control Varietes

We can reduce the variance even further â€“ when $Ï_t=0$, we might consider
returning the value of $V(S_t)$ instead of 0.

~~~
Therefore, we might write
$$G_{t:t+n}^\mathrm{CV} â‰ Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - Ï_t)V(S_t),$$
which is valid since the expected value of $1-Ï_t$ is zero and $Ï_t$ and $S_t$
are independent.

~~~
Similarly as before, rewriting to
$$\begin{aligned}
G_{t:t+n}^\mathrm{CV} - V(S_t)
  &= Ï_t \big(R_{t+1} + Î³ G_{t+1:t+n}^\mathrm{CV}\big) - Ï_tV(S_t) \\
  &= Ï_t \big(R_{t+1} + Î³ V(S_{t+1}) - V(S_t) + Î³ (G_{t+1:t+n}^\mathrm{CV} - V(S_{t+1}))\big)
\end{aligned}$$
results in
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Ï_{t:t+i} Î´_{t+i}.$$

---
section: EligibilityTraces
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $Î»$-return

For a given $Î» âˆˆ [0,1]$, we define **$Î»$-return** as
$$G_t^Î» â‰ (1 - Î») âˆ‘_{i=1}^âˆ Î»^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](traces_weighting.svgz)

~~~
Alternatively, the $Î»$-return can be written recursively as
$$\begin{aligned}
G_t^Î» &= (1 - Î») G_{t:t+1} \\
      &+ Î» (R_{t+1} + Î³ G_{t+1}^Î»).
\end{aligned}$$

---
# $Î»$-return

In an episodic task with time of termination $T$, we can rewrite the $Î»$-return
to
$$G_t^Î» = (1 - Î») âˆ‘_{i=1}^{T-t-1} Î»^{i-1} G_{t:t+i} + Î»^{T-t-1} G_t.$$

~~~
![w=60%,h=center](traces_example.svgz)

---
# Truncated $Î»$-return

We might also set a limit on the largest value of $n$, obtaining
**truncated $Î»$-return**
$$G_{t:t+n}^Î» â‰ (1 - Î») âˆ‘_{i=1}^{n-1} Î»^{i-1} G_{t:t+i} + Î»^{n-1} G_{t:t+n}.$$

~~~
The truncated $Î»$ return can be again written recursively as

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»),~~G_{t:t+1}^Î» = G_{t:t+1}.$$

~~~
Similarly to before, we can express the truncated $Î»$ return as a sum of TD
errors

$$\begin{aligned}
  G_{t:t+n}^Î» - V(S_t)
  & = (1 - Î») \big(R_{t+1} + Î³V(S_{t+1})\big) + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î») \\
  & = R_{t+1} + Î³V(S_{t+1}) - V(S_t) + Î» Î³ \big(G_{t+1:t+n}^Î» - V(S_{t+1})\big),
\end{aligned}$$
obtaining an analogous estimate $G_{t:t+n}^Î» = V(S_t) + âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Î´_{t+i}.$

---
# Variable $Î»$s

The (truncated) $Î»$-return can be generalized to utilize different $Î»_i$ at each
step $i$. Notably, we can generalize the recursive definition

$$G_{t:t+n}^Î» = (1 - Î») G_{t:t+1} + Î» (R_{t+1} + Î³ G_{t+1:t+n}^Î»)$$

to

$$G_{t:t+n}^{Î»_i} = (1 - Î»_{t+1}) G_{t:t+1} + Î»_{t+1} (R_{t+1} + Î³ G_{t+1:t+n}^Î»),$$

~~~
and express this quantity again by a sum of TD errors:

$$G_{t:t+n}^{Î»_i} = V(S_t) + âˆ‘_{i=0}^{n-1} Î³^i \left(âˆ_{j=1}^i Î»_{t+j}\right) Î´_{t+i}.$$

---
# TD($Î»$)

We have defined the $Î»$-return in the so-called **forward view**.

![w=80%,h=center,mh=80%,v=middle](traces_forward.svgz)

---
# TD($Î»$)

However, to allow on-line updates, we might consider also the **backward view**

![w=80%,h=center,mh=80%,v=middle](traces_backward.svgz)

---
# TD($Î»$)

TD($Î»$) is an algorithm implementing on-line policy evaluation utilizing the
backward view.

![w=80%,h=center](traces_td_lambda.svgz)

---
# Off-policy Traces with Control Varietes

The eligibility traces can be combined with control varietes, obtaining (DETAILS
TO BE ADDED LATER)
$$G_{t:t+n}^\mathrm{Î»,CV}
  = âˆ‘\nolimits_{i=0}^{n-1} Î³^i Î»^i Ï_{t:t+i} Î´_{t+i}.$$

---
section: V-trace
# V-trace

V-trace is a modified version of $n$-step return with off-policy correction,
defined in the Feb 2018 IMPALA paper as (using the notation from the paper):

$$v_s â‰ V(S_s) + âˆ‘_{t=s}^{s+n-1} Î³^{t-s} \left(âˆ\nolimits_{i=s}^{t-1} c_i\right) Î´_t V,$$
~~~
where $Î´_t V$ is the temporal difference for V
$$Î´_t V â‰ Ï_t \big(R_{t+1} + Î³V(s_{t+1}) - V(s_t)\big),$$
~~~
and $Ï_t$ and $c_i$ are truncated importance sampling ratios with $ÏÌ„ â‰¥ cÌ„$:
$$Ï_t â‰ \min\left(ÏÌ„, \frac{Ï€(A_t | S_t)}{b(A_t | S_t)}\right),~~~~c_i â‰ \min\left(cÌ„, \frac{Ï€(A_i | S_i)}{b(A_i | S_i)}\right).$$

~~~
Note that if $b=Ï€$ and assuming $cÌ„ â‰¥ 1$, $v_s$ reduces to $n$-step Bellman
target.

---
# V-trace

Note that the truncated IS weights $Ï_t$ and $c_i$ play different roles:

~~~
- The $Ï_t$ appears in the definition of $Î´_t V$ and defines the fixed point
  of the update rule. For $ÏÌ„=âˆ$, the target is the value function $v_Ï€$,
  if $ÏÌ„<âˆ$, the fixed point is somewhere between $v_Ï€$ and $v_b$. Notice that
  we do not compute a product of these $Ï_t$ coefficients.

~~~
  Concretely, it can be proven that the fixed point of the value function
  $v_s$ is the policy
  $$Ï€_ÏÌ„(a|x) âˆ \min\big(ÏÌ„b(a|s), Ï€(a|s)\big).$$
~~~
- The $c_i$ impacts the speed of convergence (the contraction rate of the
  Bellman operator), not the sought policy. Because a product of the $c_i$
  ratios is computed, it plays an important role in variance reduction.

~~~
However, the paper utilizes $cÌ„=1$ and out of $ÏÌ„ âˆˆ \{1, 10, 100\}$, $ÏÌ„=1$ works
empirically the best, so the distinction between $c$ and $Ï$ is not useful in
practise.

---
# V-trace Analysis

Define the V-trace operator $ğ“¡$:
$$ ğ“¡ V(x) â‰ V(x) + ğ”¼_{\mu} \Big[\sum_{t\geq 0} \gamma^t \big(c_0\dots c_{t-1}\big) \rho_t\big(r_t+\gamma V(x_{t+1})-V(x_t)\big) \big|x_0=x, \mu \Big],$$

where the expectation $ğ”¼_{\mu}$ is with respect to the policy $\mu$ which has generated the trajectory $(x_t)_{t\geq 0}$, i.e., $x_0=x$, $x_{t+1}\sim p(\cdot|x_t, a_t)$, $a_t\sim \mu(\cdot|x_t)$.
Here we consider the infinite-horizon operator but very similar results hold for the $n$-step truncated operator.

Assume that there exists $\beta\in (0,1]$ such that $ğ”¼_{\mu}\rho_0\geq \beta$.

It holds (TO BE ADDED LATER) that $\| ğ“¡ V_1(x)- ğ“¡ V_2(x)  \| \leq \eta \|V_1 - V_2 \|_{\infty}$, with $\eta = \gamma^{-1} - (\gamma^{-1}-1) \sum_{t \ge 0} \gamma^{t} ğ”¼_\mu \left [\Big( \prod_{s=0}^{t-2} c_s \Big ) \rho_{t-1} \right ] \leq 1-(1-\gamma)\beta<1$, so $ğ“¡$ is a contraction mapping. Thus $ğ“¡$ possesses a unique fixed point.

---
# V-trace Analysis

Thus $ğ“¡$ possesses a unique fixed point. Let us now prove that this fixed point is $V^{\pi_{\bar \rho}}$. We have:
$$\begin{aligned}
  & ğ”¼_\mu \big[ \rho_t\big(r_t+\gamma V^{\pi_{\bar \rho}}(x_{t+1})-V^{\pi_{\bar \rho}}(x_t)\big)\big| x_t\Big] \\
  &= \sum_a \mu(a|x_t) \min\big(\bar \rho, \frac{\pi(a|x_t)}{\mu(a|x_t)} \big) \Big[r(x_t, a) + \gamma \sum_y p(y|x_t, a) V^{\pi_{\bar \rho}}(y) - V^{\pi_{\bar \rho}}(x_t)\Big] \\
  &= \underbrace{\sum_a \pi_{\bar \rho}(a|x_t) \Big[r(x_t, a) + \gamma \sum_y p(y|x_t, a) V^{\pi_{\bar \rho}}(y) - V^{\pi_{\bar \rho}}(x_t)\Big]}_{=0} \sum_b \min\big(\bar \rho \mu(b|x_t), \pi(b|x_t) \big) \\
  &= 0,
\end{aligned}$$
since this is the Bellman equation for $V^{\pi_{\bar \rho}}$. We deduce that $ğ“¡ V^{\pi_{\bar \rho}} = V^{\pi_{\bar \rho}}$, thus $V^{\pi_{\bar \rho}}$ is the unique fixed point of $ğ“¡$.

---
# V-trace

It is easy to see that the defined $n$-step V-trace target
$$v_s â‰ V(S_s) + âˆ‘_{t=s}^{s+n-1} Î³^{t-s} \left(âˆ\nolimits_{i=s}^{t-1} c_i\right) Î´_t V$$

can be computed recursively as

$$v_s â‰ V(S_s) + Î´_sV + Î³ c_s \Big(v_{s+1} - V(S_{s+1})\Big),$$
which is the form usually used for implementation.

---
section: IMPALA
# IMPALA

Impala (**Imp**ortance Weighted **A**ctor-**L**earner **A**rchitecture) was
suggested in Feb 2018 paper and allows massively distributed implementation
of an actor-critic-like learning algorithm.

~~~
Compared to A3C-based agents, which communicates gradients with respect to the
parameters of the policy, IMPALA actors communicates trajectories to the
centralized learner.

~~~
![w=50%](impala_overview.svgz)
~~~ ~~
![w=50%](impala_overview.svgz)![w=50%](impala_comparison.svgz)

~~~
If many actors are used, the policy used to generate a trajectory can lag behind
the latest policy. Therefore, a new **V-trace** off-policy actor-critic
algorithm is proposed.

---
# V-trace

Consider a parametrized functions computing $v(s; â†’Î¸)$ and $Ï€(a|s; â†’Ï‰)$,
we update the critic in the direction of
$$\Big(v_s - v(S_s; â†’Î¸)\Big) âˆ‡_â†’Î¸ v(S_s; â†’Î¸)$$

~~~
and the actor in the direction of the policy gradient
$$Ï_s âˆ‡_â†’Ï‰ \log Ï€(A_s | S_s; â†’Ï‰)\big(R_{s+1} + Î³ v_{s+1} - v(S_s; â†’Î¸)\big),$$
where we estimate $Q_Ï€(S_s, A_s)$ as $R_{s+1} + Î³ v_{s+1}$.

~~~
Finally, we again add the entropy regularization term $H(Ï€(â‹… | S_s; â†’Î¸))$ to the
loss function.


---
# IMPALA

![w=60%,h=center](impala_throughput.svgz)

---
# IMPALA â€“ Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the
global gradient norm clipping threshold.

~~~
![w=80%,h=center](pbt_overview.svgz)

---
# IMPALA â€“ Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the
global gradient norm clipping threshold.

In population based training, several agents are trained in parallel. When an
agent is _ready_ (after 5000 episodes), then:
~~~
- it may be overwritten by parameters and hyperparameters of another randomly
  chosen agent, if it is sufficiently better (5000 episode mean capped human
  normalized score returns are 5% better);
~~~
- and independently, the hyperparameters may undergo a change (multiplied by
  either 1.2 or 1/1.2 with 33% chance).

---
# IMPALA â€“ Architecture
![w=80%,h=center](impala_architecture.svgz)

---
# IMPALA

![w=100%,v=middle](impala_results.svgz)

---
# IMPALA â€“ Learning Curves

![w=32%,h=center](impala_curves.svgz)

---
# IMPALA â€“ Atari Games

![w=60%,h=center,v=middle](impala_results_atari.svgz)

---
# IMPALA â€“ Atari Hyperparameters

![w=52%,h=center](impala_hyperparameters.svgz)

---
# IMPALA â€“ Ablations

![w=60%,f=right](impala_ablations_table.svgz)

- **No-correction**: no off-policy correction;
- **$Îµ$-correction**: add a small value $Îµ=10^{-6}$
  during gradient calculation to prevent $Ï€$ to be
  very small and lead to unstabilities during $\log Ï€$
  computation;
- **1-step**: no off-policy correction in update of the value function,
  TD errors are multiplied by the corresponding $Ï$ (but no $c$s).

---
# IMPALA â€“ Ablations

![w=63%,mw=80%,h=center,f=right](impala_ablations_graphs.svgz)

The effect of the policy lag (the number of updates the
actor is behind the learned policy) on the performance.
