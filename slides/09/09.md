title: NPFL122, Lecture 9
class: title, langtech, cc-by-nc-sa
# Deterministic Policy Gradient, Advanced RL Algorithms

## Milan Straka

### December 10, 2018

---
section: Refresh
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a)$. Then, better-than-average returns are positive
and worse-than-average returns are negative.

The resulting value is also called an _advantage function_
$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s)$.

Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workes to only generate episodes. Such approach
was described in May 2017 by Celemente et al., who named their agent
_parallel advantage actor-critic_ (PAAC).

![w=70%,h=center](../08/paac_framework.pdf)

---
# Continuous Action Space

Until now, the actions were discreet. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b âˆˆ â„$, or more generally from a Cartesian product
of several such ranges:
$$âˆ_i [a_i, b_i].$$

![w=40%,f=right](../08/normal_distribution.pdf)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $Î¼$ and variance $Ïƒ^2$, probability density function of $ğ“(Î¼, Ïƒ^2)$
is
$$p(x) â‰ \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} e^{\large-\frac{(x - Î¼)^2}{2Ïƒ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution we suitably parametrize the action value,
usually using the normal distribution. Considering only one real-valued action,
we therefore have
$$Ï€(a | s; â†’Î¸) â‰ P\Big(a âˆ¼ ğ“\big(Î¼(s; â†’Î¸), Ïƒ(s; â†’Î¸)^2\big)\Big),$$
where $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ are function approximation of mean and standard
deviation of the action distribution.

The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a regular regression (i.e., one output neuron
  without activation);
- the standard variance (which must be positive) being computed again as
  a regression, followed most commonly by either $\exp$ or
  $\operatorname{softplus}$, where $\operatorname{softplus}(x) â‰ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

```python
  mu = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.exp(log_sd)   # or sd = tf.nn.softplus(sd)

  normal_dist = tf.distributions.Normal(mu, sd)

  # Loss computed as - log Ï€(a|s) - entropy_regularization
  loss = - normal_dist.log_prob(self.actions) * self.returns \
         - args.entropy_regularization * normal_dist.entropy()
```

---
section: DPG
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

~~~
## Deterministic Policy Gradient Theorem
Assume that the policy $Ï€(s; â†’Î¸)$ is deterministic and computes
an action $aâˆˆâ„$. Then under several assumptions about continuousness, the
following holds:
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ ğ”¼_{sâˆ¼Î¼(s)} \Big[âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s;â†’Î¸)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al.

---
# Deterministic Policy Gradient Theorem â€“ Proof

The proof is very similar to the original (stochastic) policy gradient theorem.
We assume that $p(s' | s, a), âˆ‡_a p(s' | s, a), r(s, a), âˆ‡_a r(s, a), Ï€(s; â†’Î¸), âˆ‡_â†’Î¸ Ï€(s; â†’Î¸)$
are continuous in all params.

~~~
$\displaystyle âˆ‡_â†’Î¸ v_Ï€(s) = âˆ‡_â†’Î¸ q_Ï€(s, Ï€(s; â†’Î¸))$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸\Big(r\big(s, Ï€(s; â†’Î¸)\big) + Î³ âˆ«_{s'} p\big(s' | s, Ï€(s; â†’Î¸)\big) v_Ï€(s') \d s'\Big)$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a r(s, a) \big|_{a=Ï€(s; â†’Î¸)} + Î³ âˆ‡_â†’Î¸ âˆ«_{s'} p\big(s' | s, Ï€(s; â†’Î¸)\big) v_Ï€(s') \d s'$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a \Big( r(s, a) \big|_{a=Ï€(s; â†’Î¸)} + Î³ âˆ«_{s'} p\big(s' | s, a)\big) v_Ï€(s') \d s' \Big) \\
                    \qquad\qquad\qquad + Î³ âˆ«_{s'} p\big(s' | s, Ï€(s; â†’Î¸)\big) âˆ‡_â†’Î¸ v_Ï€(s') \d s'$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s; â†’Î¸)} + Î³ âˆ«_{s'} p\big(s' | s, Ï€(s; â†’Î¸)\big) âˆ‡_â†’Î¸ v_Ï€(s') \d s'$

~~~
Similarly to the gradient theorem, we finish the proof by continually expanding $âˆ‡_â†’Î¸ v_Ï€(s')$.

---
section: DDPG
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
(similarly to how expected Sarsa is also an off-policy algorithm).

~~~
We therefore train function approximation for both $Ï€(s; â†’Î¸)$ and $q(s, a; â†’Î¸)$,
training $q(s, a; â†’Î¸)$ using a deterministic variant of the Bellman equation:
$$q(S_t, A_t; â†’Î¸) = ğ”¼_{R_{t+1}, S_{t+1}} \big[R_{t+1} + Î³ q(S_{t+1}, Ï€(S_{t+1}; â†’Î¸))\big]$$
and $Ï€(s; â†’Î¸)$ according to the deterministic policy gradient theorem.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

The authors utilize a replay buffer, a target network (updated by exponential
moving average with $Ï„=0.001$), batch normalization for CNNs, and perform
exploration by adding a normal-distributed noise to predicted actions.
Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the
policy and critic network, respectively.

---
# Deep Deterministic Policy Gradients

![w=65%,h=center](ddpg.pdf)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.pdf)

---
# Deep Deterministic Policy Gradients

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

![w=57%,h=center](ddpg_results.pdf)

---
section: NPG
# Natural Policy Gradient

The following approach has been introduced by Kakade (2002).

~~~
Using policy gradient theorem, we are able to compute $âˆ‡ v_Ï€$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $â†’d$ which maximizes $v_Ï€(s; â†’Î¸ + â†’d)$ under
the constraint that $|â†’d|^2$ is bounded by a small constant is exactly
the gradient $âˆ‡ v_Ï€$.

~~~
Normally, the length $|â†’d|^2$ is computed using Euclidean metric. But in general,
any metric could be used. Representing a metric using a positive-definite matrix
$â‡‰G$ (identity matrix for Euclidean metric), we can compute the distance as
$|â†’d|^2 = âˆ‘_{ij} G_{ij} d_i d_j = â†’d^T â‡‰G â†’d$. The steepest ascent direction is
then given by $â‡‰G^{-1} âˆ‡ v_Ï€$.

~~~
Note that when $â‡‰G$ is the Hessian $â‡‰H v_Ï€$, the above process is exactly
Newton's method.

---
# Natural Policy Gradient

![w=100%,v=middle](npg.pdf)

---
# Natural Policy Gradient

A suitable choice for the metric is _Fisher information matrix_ defined as
$$F_s(â†’Î¸) â‰ ğ”¼_{Ï€(a | s; â†’Î¸)} \left[\frac{âˆ‚ \log Ï€(a | s; â†’Î¸)}{âˆ‚ â†’Î¸_i} \frac{âˆ‚ \log Ï€(a | s; â†’Î¸)}{âˆ‚ â†’Î¸_j} \right]
\color{gray} = ğ”¼[âˆ‡ Ï€(a | s; â†’Î¸)] ğ”¼[âˆ‡ Ï€(a | s; â†’Î¸)]^T.$$

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

~~~
Recall Kullback-Leibler distance (or relative entropy) defined as
$$D_\textrm{KL}(â†’p || â†’q) â‰ âˆ‘_i p_i \log \frac{p_i}{q_i} \color{gray} = H(p, q) - H(p).$$

~~~
The Fisher information matrix is also a Hessian of the
$D_\textrm{KL}(Ï€(a | s; â†’Î¸) || Ï€(a | s; â†’Î¸')$:
$$F_s(â†’Î¸) = \frac{âˆ‚^2}{âˆ‚Î¸_i' âˆ‚Î¸_j'} D_\textrm{KL}(Ï€(a | s; â†’Î¸) || Ï€(a | s; â†’Î¸')\Big|_{â†’Î¸' = â†’Î¸}.$$

---
# Natural Policy Gradient

Using the metric
$$F(â†’Î¸) = ğ”¼_{s âˆ¼ Î¼_â†’Î¸} F_s(â†’Î¸)$$
we want to update the parameters using $â†’d_F â‰ F(â†’Î¸)^{-1} âˆ‡ v_Ï€$.

~~~
An interesting property of using the $â†’d_F$ to update the parameters is that
- updating $â†’Î¸$ using $âˆ‡ v_Ï€$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $â†’Î¸$ using $F(â†’Î¸)^{-1} âˆ‡ v_Ï€$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $â†’d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$â†’d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $â‡‰Aâ†’x = â†’b$
in an iterative manner, by using $â‡‰A$ only to compute products $â‡‰Aâ†’v$ for
a suitable $â†’v$.

~~~
Therefore, $â†’d_F$ is found as a solution of
$$F(â†’Î¸)â†’d_F = âˆ‡ v_Ï€$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{Î±}{\sqrt{(âˆ‡ v_Ï€)^T F(â†’Î¸)^{-1} âˆ‡ v_Ï€}}.$$

---
section: TRPO
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $Ï€, Ï€Ìƒ$, we can write
$$v_Ï€Ìƒ = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€Ìƒ)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s),$$
where $a_Ï€(a | s)$ is the advantage function $q_Ï€(a | s) - v_Ï€(s)$ and
$Î¼(Ï€Ìƒ)$ is the on-policy distribution of the policy $Ï€Ìƒ$.

~~~
Analogously to policy improvement, we see that if $a_Ï€(a | s) â‰¥0$, policy
$Ï€Ìƒ$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s âˆ¼ Î¼(Ï€Ìƒ)$ is costly. Therefore, we instead
consider
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s).$$

---
# Trust Region Policy Optimization
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s)$$

It can be shown that for parametrized $Ï€(a | s; â†’Î¸)$ the $L_Ï€(Ï€Ìƒ)$ matches
$v_{Ï€Ìƒ}$ to the first order.

~~~
Schulman et al. additionally proves that if we denote
$Î± = D_\textrm{KL}^\textrm{max}(Ï€_\textrm{old} || Ï€_\textrm{new})
   = \max_s D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) || Ï€_\textrm{new}(â‹…|s)\big)$, then
$$v_{Ï€_\textrm{new}} â‰¥ L_{Ï€_\textrm{old}}(Ï€_\textrm{new}) - \frac{4ÎµÎ³}{(1-Î³)^2}Î±\textrm{~~~where~~~}Îµ = \max_{s, a} |a_Ï€(s, a)|.$$

~~~
Therefore, TRPO minimizes $L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)$ subject to
$D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) < Î´$, where
- $D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) = ğ”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0})} [D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) || Ï€_\textrm{new}(â‹…|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $Î´$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $Ï€$.

---
# Trust Region Policy Optimization

$$\textrm{minimize}~~L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)~~\textrm{subject to}~~D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} || Ï€_â†’Î¸) < Î´$$

The parameters are updated using $â†’d_F = F(â†’Î¸)^{-1} âˆ‡ L_{Ï€_{â†’Î¸_0}}(Ï€_â†’Î¸)$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{Î´/(â†’d_F^T F(â†’Î¸)^{-1} â†’d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.pdf)

![w=100%](rllib_results.pdf)

---
section: PPO
# Proximal Policy Optimization

A simplification of TRPO which can be implemented using a few lines of code.

Let $r_t(â†’Î¸) â‰ \frac{Ï€(A_t|S_t; â†’Î¸)}{Ï€(A_t|S_t; â†’Î¸_\textrm{old})}$. PPO
minimizes the objective
$$L^\textrm{CLIP}(â†’Î¸) â‰ ğ”¼_t\Big[\min\big(r_t(â†’Î¸) AÌ‚_t, \operatorname{clip}(r_t(â†’Î¸), 1-Îµ, 1+Îµ) AÌ‚_t)\big)\Big].$$

Such $L^\textrm{CLIP}(â†’Î¸)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.pdf)

---
# Proximal Policy Optimization

The advantages $AÌ‚_t$ are additionally estimated using _generalized
advantage estimation_. Instead of the usual
$AÌ‚_t â‰ âˆ‘_{i=0}^{T-t-1} Î³^i R_{t+1+i} + Î³^{T-t} V(S_T) - V(S_t)$
the authors employ
$$AÌ‚_t â‰ âˆ‘_{i=0}^{T-t-1} (Î³Î»)^i Î´_{t+i},$$
where $Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)$.

![w=80%,h=center](ppo_algorithm.pdf)

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.pdf)

---
section: SAC
# Soft Actor Critic

The paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor by Tuomas Haarnoja et al. introduces
a different off-policy algorithm for continuous action space.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize.

---
# Soft Actor Critic
![w=60%,h=center](sac_algorithm.pdf)

---
# Soft Actor Critic
![w=90%](sac_results.pdf)
