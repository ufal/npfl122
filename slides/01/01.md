title: NPFL122, Lecture 1
class: title, langtech, cc-by-nc-sa
# Introduction to Reinforcement Learning

## Milan Straka

### October 5, 2020

---
section: Organization

# Organization

**Course Website** https://ufal.mff.cuni.cz/courses/npfl122

~~~

**Course Repository** https://github.com/ufal/npfl122

~~~

## Zoom

- The lectures and practicals are happening on Zoom.

- The recordings will be available from the course website.

~~~

## Piazza

- Piazza will be used as a communication platform.
~~~
  It allows sending
  - either notes or questions (the latter require an answer)
~~~
  - to everybody (signed or anonymously), to all instructors, to a specific instructor
~~~
  - students can answer other students' questions too
~~~
- Please use it whenever possible for communication with the instructors.
~~~
- You will get the invite link after the first lecture.

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you will be able to create an account
  using your CAS credentials and will be automatically assigned
  to the right group.
~~~
- Otherwise follow the instructions on Piazza; generally you will need to send
  me a message with several pieces of information and I will send it to ReCodEx
  administrators in batches.

---
# Course Requirements

## Practicals
~~~

- There will be 1-2 assignments a week, each with 2-week deadline.
~~~
  - Deadlines can be extended, but you need to write **before** the deadline.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the practicals, you need to get 80 non-bonus points. There will be
  assignments for at least 120 non-bonus points.
~~~
- If you get more than 80 points (be it bonus or non-bonus), they will be
  transferred to the exam (but at most 40 points are transfered).

~~~
## Lecture

You need to pass a written exam.
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus at most 40 surplus
  points from the practicals and plus at most 10 surplus points for community
  work (e.g., improving slides).
~~~
- You need 60/75/90 points to pass with grade 3/2/1.
---
section: History
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s â€“ Richard Bellman

~~~
- Trial and error learning â€“ since 1850s
  - Law and effect â€“ Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, â€¦ â€“ 1950s and 1960s
  - Tsetlin, Holland, Klopf â€“ 1970s
  - Sutton, Barto â€“ since 1980s

~~~
- Arthur Samuel â€“ first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro â€“ 1992, human-level Backgammon program trained solely by self-play

~~~
- IBM Watson in Jeopardy â€“ 2011

---
# History of Reinforcement Learning
## Recent successes

- Human-level video game playing (DQN) â€“ 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C â€“ 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow â€“ 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala â€“ Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala â€“ Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Reinforcement Learning
## Recent successes

![w=22%,f=right](r2d2-results.svgz)

- R2D2 â€“ Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training
~~~
- Agent57 - Mar 2020
  - super-human performance on all 57 Atari games
~~~
- Data-efficient Rainbow â€“ Jun 2019
  - learning from ~2 hours of game experience
![w=30%,mw=70%,h=center](der-progress.svgz)

---
# History of Reinforcement Learning
## Recent successes

- AlphaGo

  - Mar 2016 â€“ beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master â€“ Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero â€“ 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero â€“ Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=25%,h=center](a0_results.svgz)

---
# History of Reinforcement Learning
## Recent successes

- Dota2 â€“ Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN â€“ Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW â€“ Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games

~~~
- OpenAI Five â€“ Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs, 180 years of experience per day

~~~
- AlphaStar
  - Jan 2019: won 10 out of 11 StarCraft II games against two professional players
  - Oct 2019: ranked 99.8% on `Battle.net`, playing with full game rules

---
# AlphaStart

![w=71%,h=center](alphastar_results.svgz)

---
# History of Reinforcement Learning
## Recent successes

- Optimize non-differentiable loss

  - improved translation quality in 2016
  - better summarization performance

~~~
- Discovering discrete latent structures

~~~
- Effectively search in space of natural language policies

~~~
- TARDIS â€“ Jan 2017
  - allow using discrete external memory

~~~
- Neural architecture search (Nov 2016)
  - SoTA CNN architecture generated by another network
  - can search also for suitable RL architectures, new activation functions,
    optimizersâ€¦

---
section: Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, â€¦

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ğ”¼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) â‰ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t â‰ \argmax_a Q_t(a).$$

---
section: $Îµ$-greedy
# $Îµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$Îµ$-greedy_ method follows the greedy action with probability $1-Îµ$, and
chooses a uniformly random action with probability $Îµ$.

---
# $Îµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.svgz)

---
# $Îµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} âˆ‘_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} âˆ‘_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $Îµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.svgz)

---
# Fixed Learning Rate

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Î±(R_n - Q_n).$$

~~~
Converges to the true action values if
$$âˆ‘_{n=1}^âˆ Î±_n = âˆ \textrm{~~~~and~~~~}âˆ‘_{n=1}^âˆ Î±_n^2 < âˆ.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Î±)^n Q_1 + âˆ‘_{i=1}^n Î±(1-Î±)^{n-i} R_i.$$

~~~
The bias can be utilized to support exploration at the start of the episode by
setting the initial values to more than the expected value of the optimal
solution.

---
# Optimistic Initial Values and Fixed Learning Rate

![w=85%,h=center,v=middle](optimistic_values.svgz)

---
# Method Comparison

![w=85%,h=center,v=middle](bandits_comparison.svgz)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.svgz)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.svgz)

A **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizont tasks** then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_Ï€[q_Ï€(s, a)], \\
  q_Ï€(s, a) &= ğ”¼_Ï€[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizont tasks or if $Î³ < 1$, there always exists a unique optimal
state-value function, unique optimal action-value function, and (not necessarily
unique) optimal policy.

---
section: Monte Carlo Methods
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

~~~
Keeping estimated returns for the action-value function, we evaluate
the current policy by sampling one episode while using it. We then update
the action-value function by averaging over the observed returns, including
the currently sampled episode.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume **exploring starts**, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
~~~
- **first visit**: only the first occurence of a state-action pair in an episode is
  considered
- **every visit**: all occurences of a state-action pair are considered.

~~~
Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.svgz)


---
# Monte Carlo and $Îµ$-soft Policies

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$
and we call it $Îµ$-greedy, if one action has a maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
For $Îµ$-soft policy, Monte Carlo policy evaluation also converges, without the need
of exploring starts.

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{t+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: POMDP
# Partially Observable MDPs

Recall that a MDP is a quadruple $(ğ“¢, ğ“, p, Î³)$, where:
- $ğ“¢$ is a set of states,
- $ğ“$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a reward $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a discount factor.

~~~
**Partially observable Markov decision process** (POMDP) extends the Markov decision
process to a sextuple $(ğ“¢, ğ“, p, Î³, ğ“, o)$, where in addition to an MDP:
- $ğ“$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model.

and agents are provided only with an observation $O_t$ instead of state $S_t$.

~~~
In robotics (out of the domain of this course), several approaches are used to
handle POMDPs, to model uncertainty, imprecise mechanisms and inaccurate
sensors.
