title: NPFL122, Lecture 1
class: title, langtech, cc-by-nc-sa
# Introduction to Reinforcement Learning

## Milan Straka

### October 3, 2022

---
# Reinforcement Learning

**Reinforcement learning** is a machine learning paradigm, different from
_supervised_ and _unsupervised learning_.

~~~
The essence of reinforcement learning is to learn from _interactions_ with the
environment to maximize a numeric _reward_ signal.
~~~
The learner is not told which actions to take, and the actions may affect not
just the immediate reward, but also all following rewards.

~~~
![w=50%,h=center](robots.png)

---
# Deep Reinforcement Learning

In the last decade, reinforcement learning has been successfully combined with
_deep neural networks_.

~~~
![w=30%](atari_games.png)
~~~
![w=37%](a0_results.svgz)
~~~
![w=31%](alphastar.png)

~~~
![w=78%,mw=17%,h=center](robot_navigation.jpg)
~~~
![w=53%,mw=26%,h=center](data_center.jpg)
~~~
![w=30%](muzero_rc.png)
~~~
![w=24%](sparrow.svgz)

---
section: Organization

# Organization

**Course Website:** https://ufal.mff.cuni.cz/courses/npfl122
~~~
  - Slides, recordings, assignments, exam questions
~~~

**Course Repository:** https://github.com/ufal/npfl122
- Templates for the assignments, slide sources.

~~~

## Piazza

- Piazza will be used as a communication platform.

  You can post questions or notes,
  - privately to the instructors, or
~~~
  - to everyone (signed or anonymously).
~~~

  Students can answer other student's questions too, which allows you to get
  faster response. However, please do not send even parts of your solutions to
  other students.

~~~
- Please use Piazza for **all communication** with the instructors.
~~~
- You will get the invite link after the first lecture.

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you should be able to create an account
  using your CAS credentials and should automatically see the right group.
~~~
- Otherwise, there will be **instructions** on **Piazza** how to get
  ReCodEx account (generally you will need to send me a message with several
  pieces of information and I will send it to ReCodEx administrators in
  batches).

---
# Course Requirements

## Practicals
~~~

- There will be 1-4 assignments a week, each with a 2-week deadline.
~~~
  - There is also another week-long second deadline, but for less points.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the practicals, you need to get 80 non-bonus points. There will be
  assignments for at least 120 non-bonus points.
~~~
- If you get more than 80 points (be it bonus or non-bonus), they will be
  all transferred to the exam. Additionally, if you solve all the assignments,
  you pass the exam with grade 1.

~~~
## Lecture

You need to pass a written exam (or solve all the assignments).
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus the surplus
  points from the practicals and plus at most 10 surplus points for **community
  work** (improving slides, ‚Ä¶).
~~~
- You need 60/75/90 points to pass with grade 3/2/1.

---
section: History
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s ‚Äì Richard Bellman

~~~
- Trial and error learning ‚Äì since 1850s
  - Law and effect ‚Äì Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, ‚Ä¶ ‚Äì 1950s and 1960s
  - Tsetlin, Holland, Klopf ‚Äì 1970s
  - Sutton, Barto ‚Äì since 1980s

~~~
- Arthur Samuel ‚Äì first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro ‚Äì 1992, human-level Backgammon program trained solely by self-play

~~~
- IBM Watson in Jeopardy ‚Äì 2011

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Atari Games

- Human-level video game playing (DQN) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C ‚Äì 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow ‚Äì 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala ‚Äì Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala ‚Äì Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Atari Games

![w=22%,f=right](r2d2_results.svgz)

- R2D2 ‚Äì Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training
~~~
- Agent57 - Mar 2020
  - super-human performance on all 57 Atari games
~~~
- Data-efficient Rainbow ‚Äì Jun 2019
  - learning from ~2 hours of game experience
![w=30%,mw=70%,h=center](der-progress.svgz)

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Board Games

- AlphaGo

  - Mar 2016 ‚Äì beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master ‚Äì Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero ‚Äì 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero ‚Äì Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=24%,h=center](a0_results.svgz)

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì 3D Games

- Dota2 ‚Äì Aug 2017

  - OpenAI bot won Dota2 1v1 matches against a professional player

~~~
- MERLIN ‚Äì Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW ‚Äì Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games

~~~
- OpenAI Five ‚Äì Aug 2018
  - won Dota2 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs, 180 years of experience per day

~~~
- AlphaStar
  - Jan 2019: won 10 out of 11 StarCraft II games against two professional players
  - Oct 2019: ranked 99.8% on `Battle.net`, playing with full game rules

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Other Applications

- Optimize non-differentiable loss

  - improved translation quality in 2016
  - better summarization performance

~~~
- Discovering discrete latent structures

~~~
- Effectively search in space of natural language policies

~~~
- TARDIS ‚Äì Jan 2017
  - allow using discrete external memory

~~~
- Neural architecture search (Nov 2016)
  - SoTA CNN architecture generated by another network
  - can search also for suitable RL architectures, new activation functions,
    optimizers‚Ä¶

~~~
- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction

~~~
- Improving efficiency of VP9 codec (2022; 4% in bandwith with no loss in
  quality)

---
# History of Deep Reinforcement Learning

Note that the machines learn just to obtain a reward we have defined,
they do not learn what we want them to.

- [Hide and seek](https://openai.com/blog/emergent-tool-use/#surprisingbehaviors)

~~~
![w=49%,mh=70%,v=bottom](driving.gif)
~~~
![w=49%,mh=70%,v=bottom](human_evaluation.gif)

---
section: Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting an action $A_1$ (the index of the arm to use), and we
obtain a reward $R_1$. We then repeat the process by selecting an action $A_2$,
obtaining $R_2$, selecting $A_3$, ‚Ä¶, with the indices denoting the time step
when the actions and rewards occurred.

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ùîº[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t ‚âù \argmax_a Q_t(a).$$

---
section: $Œµ$-greedy
# $Œµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# $Œµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.svgz)

---
# $Œµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} ‚àë_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ‚àë_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $Œµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.svgz)

---
# Fixed Learning Rate

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Œ±(R_n - Q_n).$$

~~~
Converges to the true action values if
$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$

~~~
The bias can be utilized to support exploration at the start of the episode by
setting the initial values to more than the expected value of the optimal
solution.

---
# Optimistic Initial Values and Fixed Learning Rate

![w=85%,h=center,v=middle](optimistic_values.svgz)

---
# Method Comparison

![w=85%,h=center,v=middle](bandits_comparison.svgz)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(ùì¢, ùìê, p, Œ≥)$,
where:
- $ùì¢$ is a set of states,
~~~
- $ùìê$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a **reward** $r ‚àà ‚Ñù$,
~~~
- $Œ≥ ‚àà [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t ‚âù ‚àë_{k=0}^‚àû Œ≥^k R_{t + 1 + k}$. The goal is to optimize $ùîº[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ùì¢=\{S\}$;
~~~
- an action for every arm, $ùìê=\{a_1, a_2, ‚Ä¶, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ùìù(Œº_i, œÉ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ùìù(r | Œº_i, œÉ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ùìù(r | Œº_{i,s}, œÉ_{i,s}^2) ‚ãÖ \textit{next}(s'|s).$$

---
section: POMDP
# Partially Observable MDPs

Recall that the Markov decision process is a quadruple $(ùì¢, ùìê, p, Œ≥)$,
where:
- $ùì¢$ is a set of states,
- $ùìê$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a reward $r ‚àà ‚Ñù$,
- $Œ≥ ‚àà [0, 1]$ is a discount factor.

~~~
![w=46%,f=right](pomdp.svgz)

**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(ùì¢, ùìê, p, Œ≥, ùìû, o)$, where in addition to an MDP,
- $ùìû$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$ is used as agent input
  instead of the state $S_t$.

---
section: POMDP
# Partially Observable MDPs

Planning in a general POMDP is in theory undecidable.
~~~
- Nevertheless, several approaches are used to handle POMDPs in robotics
  - to model uncertainty, imprecise mechanisms and inaccurate sensors, ‚Ä¶
  - consider for example robotic vacuum cleaners

~~~

Partially observable MDPs are needed to model many environments
(maze navigation, FPS games, ‚Ä¶).
~~~
- We will initially assume all environments are fully observable, even if some
  of them will not.
~~~
- Later we will mention solutions, where partially observable MDPs are handled
  using recurrent networks (or networks with external memory), which model the
  latent states $S_t$.

---
section: Monte Carlo Methods
# Monte Carlo Methods

We now present the first algorithm for computing optimal behavior without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ùì¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Specifically, they try to estimate
$$Q(s, a) ‚âà ùîº[G_t | S_t = s, A_t = a].$$

~~~
With such estimates, a greedy action in state $S_t$ can be computed as
$$A_t = \argmax_a Q(S_t, a).$$

~~~
To hope for convergence, we need to visit each state-action pair infinitely many times.
One of the simplest way to achieve that is to assume **exploring starts**, where
we randomly select the first state and first action, and behave greedily
afterwards.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.svgz)


---
# Monte Carlo and $Œµ$-soft Behavior

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
Instead of choosing random state at the beginning, we can consider adding
‚Äúrandomness‚Äù gradually ‚Äì for a given $Œµ$, we set the probability of choosing any
action to be at least
$$\frac{Œµ}{|ùìê(s)|}$$
in each step. Such behavior is called _$Œµ$-soft_.

~~~
In an $Œµ$-soft behaviour, selecting and action greedily (the $Œµ$-greedy
behavior) means one action has a maximum probability of
$$1-Œµ+\frac{Œµ}{|A(s)|}.$$

~~~
We now present Monte Carlo algorithm with $Œµ$-greedy action selection.

---
# Monte Carlo for $Œµ$-soft Behavior

### On-policy every-visit Monte Carlo for $Œµ$-soft Policies
Algorithm parameter: small $Œµ>0$

Initialize $Q(s, a) ‚àà ‚Ñù$ arbitrarily (usually to 0), for all $s ‚àà ùì¢, a ‚àà ùìê$<br>
Initialize $C(s, a) ‚àà ‚Ñ§$ to 0, for all $s ‚àà ùì¢, a ‚àà ùìê$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, ‚Ä¶, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Œµ$, generate a random uniform action
  - Otherwise, set $A_t ‚âù \argmax\nolimits_a Q(S_t, a)$
- $G ‚Üê 0$
- For each $t=T-1, T-2, ‚Ä¶, 0$:
  - $G ‚Üê Œ≥G + R_{t+1}$
  - $C(S_t, A_t) ‚Üê C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ‚Üê Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$
