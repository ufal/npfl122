title: NPFL122, Lecture 1
class: title
# Introduction to Reinforcement Learning

## Milan Straka

### October 9, 2018

---
section: History
# History of Reinforcement Learning

_Develop goal-seaking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s ‚Äì Richard Bellmann

~~~
- Trial and error learning ‚Äì since 1850s
  - Law and effect ‚Äì Edward Thorndike, 1911
  - Shannon, Minsky, Clark&Farley, ‚Ä¶ ‚Äì 1950s and 1960s
  - Tsetlin, Holland, Klopf ‚Äì 1970s
  - Sutton, Barto ‚Äì since 1980s

~~~
- Arthur Samuel ‚Äì first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro ‚Äì 1992, human-level Backgammon playing program trained solely by self-play

~~~
- IBM Watson in Jeopardy ‚Äì 2011

---
# History of Reinforcement Learning
## Recent successes

- Human-level video game playing (DQN) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C ‚Äì 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow ‚Äì 2017
  - human-normalized media: 153%

~~~
- Impala ‚Äì Feb 2018
  - one network to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala ‚Äì Sep 2018
  - human-normalized mean: 176.9%, median: 110.7% on 57 games

---
# History of Reinforcement Learning
## Recent successes

- AlphaGo

  - Mar 2016 ‚Äì beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master ‚Äì Dec 2016
  - beat 60 professionals
  - beat Ke Jie in May 2017
~~~
- AlphaGo Zero ‚Äì 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero ‚Äì Dec 2017
  - self-play only
  - defeated AlphaGo Zero after 34 hours of training (21 million games)
~~~
  - impressive chess and shogi performance after 9h and 12h, respectively

---
# History of Reinforcement Learning
## Recent successes

- Dota2 ‚Äì Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN ‚Äì Mar 2018
  - unsupervised representation of states using external memory
  - partial observations
  - beat human in unknown maze navigation

~~~
- FTW ‚Äì Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play
  - trained on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)

~~~
- OpenAI Five ‚Äì Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs
    - 180 years of experience per day

---
# History of Reinforcement Learning
## Recent successes

- Improved translation quality in 2016

~~~
- Discovering discrete latent structures

~~~
- TARDIS ‚Äì Jan 2017
  - allow using discrete external memory

‚Ä¶

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = ùîº[R_{t+1} | A_t = a].$$

~~~
Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$, we would like
$Q_t(a)$ to converge to $q_*(a)$.

~~~
A natural way to estimate $Q_t(a)$ is
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t(a) ‚âù \argmax_a Q_t(a).$$

---
# Multi-armed Bandits

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# Multi-armed Bandits

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# Multi-armed Bandits

## Incremental Implementation

Let $Q\_n$ be an estimate using $n$ rewards $R\_1, \ldots, R\_n$.

$$\begin{aligned}
Q_n &= \frac{1}{n} ‚àë_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ‚àë_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_{n-1}) \\
    &= \frac{1}{n} (R_n + n Q_{n-1} - Q_{n-1}) \\
    &= Q_{n-1} + \frac{1}{n}\Big(R_n - Q_{n-1}\Big)
\end{aligned}$$

---
# Multi-armed Bandits

## Non-stationary Problems

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Œ±(R_{n+1} - Q_n).$$

~~~
Converges to the true action values if
$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$

---
# Multi-armed Bandits

## Optimistic Initial Values

![w=85%,h=center,v=middle](optimistic_values.pdf)

---
# Multi-armed Bandits

## Upper Confidence Bound

$$A\_t ‚âù \argmax_a \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right].$$

~~~
![w=70%,h=center](ucb.pdf)

---
# Multi-armed Bandits

## Gradient Bandit Algorithms

Let $H_t(a)$ be a numerical _preference_ for an action $a$ at time $t$.

~~~
We could choose actions according to softmax distribution:
$$P(A_t = a) ‚âù \frac{e^{H_t(a)}}{‚àë_b e^{H_t(b)}}.$$

~~~
Using SGD and MLE loss, we can derive the following algorithm:
$$\begin{aligned}
  H_{t+1}(A_t) &‚Üê H_t(A_t) + Œ±(R_t - \bar R_t)(1 - P(A_t)), \\
  H_{t+1}(a) &‚Üê H_t(a) + Œ±(R_t - \bar R_t)P(A_t) \textrm{~~~~for~~}a‚â†A_t.
\end{aligned}$$

---
# Multi-armed Bandits

## Gradient Bandit Algorithms

![w=85%,h=center,v=middle](gradient_bandits.pdf)

---
# Multi-armed Bandits

## Method Comparsion

![w=80%,h=center,v=middle](bandits_comparison.pdf)

---
section: MDP Definition
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ is a quadruple $(\mathcal S, \mathcal A, P, Œ≥)$,
where:
- $ùì¢$ is a set of states,
~~~
- $ùìê$ is a set of actions,
~~~
- $P(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a _reward_ $r ‚àà ‚Ñù$,
~~~
- $Œ≥ ‚àà [0, 1]$ is a _discount factor_.

~~~
Let a _return_ $G_t$ be $G_t ‚âù ‚àë_{k=0}^‚àû Œ≥^k R_{t + 1 + k}$.

---
# (State-)Value and Action-Value Functions

A _policy_ $œÄ$ computes a distribution of actions in a given state, i.e.,
$œÄ(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of policy, we define _value function_ $v\_œÄ(s)$, or
_state-value function_, as
$$v_œÄ(s) ‚âù ùîº_œÄ\left[G_t \middle| S_t = s\right] = ùîº_œÄ\left[‚àë_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for policy $œÄ$ is defined analogously as
$$q_œÄ(s, a) ‚âù ùîº_œÄ\left[G_t \middle| S_t = s, A_t = a\right] = ùîº_œÄ\left[‚àë_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
Evidently,
$$q_œÄ(s, a) = ùîº_œÄ[R_{t+1} + Œ≥v_œÄ(S_{t+1}) | S_t = s, A_t = a].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ‚âù \max_œÄ v_œÄ(s),$$
analogously
$$q_*(s, a) ‚âù \max_œÄ q_œÄ(s, a).$$

~~~
Any policy $œÄ$ with $v_œÄ = v_*$ is called an _optimal policy_.

~~~
## Existence
There always exists a unique optimal state-value function, unique optimal
action-value function, and not necessarily unique optimal policy.

---
section: Dynamic Programming
# Dynamic Programming

An approach devised by Richard Bellmann in 1950s.

~~~
To apply it do MDP, we now consider finite-horizon problems (i.e., with episodes
of bounded length) with finite number of states $ùì¢$ and actions $ùìê$.

~~~
The following recursion, which must obviously hold for optimal value function,
is usually called the _Bellmann equation_:
$$\begin{aligned}
  v_*(s) &= \max_a ùîº\left[R_{t+1} + Œ≥ v_*(S_{t+1}) \middle| S_t=s, A_t=a\right] \\
         &= \max_a ‚àë_{s', r} P(s', r | s, a) \left[r + Œ≥ v_*(s')\right].
\end{aligned}$$

~~~
To turn the Bellmann into equation, we change the equal signs to assignments:
$$v_{k+1}(s) ‚Üê \max_a ùîº\left[R_{t+1} + Œ≥ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

---
# Relations to Graph Algorithms

Searching for optimal value functions of deterministic problems is in fact
search for shortest path in a suitable graph.

~~~
![w=80%,mh=80%,h=center,v=middle](trellis.svg)

---
# Bellmann-Ford-Moore Algorithm

$$v_{k+1}(s) ‚Üê \max_a ùîº\left[R_{t+1} + Œ≥ v_k(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

Bellmann-Ford-Moore algorithm:
```python
# input: graph `g`, initial vertex `s`
for v in g.vertices: d[v] = 0 if v == s else ‚àû

for i in range(len(g.vertices) - 1):
  for e in g.edges:
    if d[e.source] + e.length < d[e.target]:
      d[e.target] = d[e.source] + e.length

```

---
section: Policy and Value Iterations
# Bellmann Backup Operator

Our goal is now to handle infinite horizon tasks.

For any value function $v‚àà‚Ñù^{|ùì¢|}$ we define _Bellmann backup operator_ $B : ‚Ñù^{|ùì¢|} ‚Üí ‚Ñù^{|ùì¢|}$ as
$$Bv(s) ‚âù \max_a ùîº\left[R_{t+1} + Œ≥ v(S_{t+1}) \middle| S_t=s, A_t=a\right].$$

~~~
It is not difficult to show that Bellman backup operator is a _contraction_:
$$\max_s \left|Bv_1(s) - Bv_2(s)\right| ‚â§ Œ≥ \max_s \left|v_1(s) - v_2(s)\right|.$$

~~~
Using Banach fixed-point theorem, it follows that there exist a _unique value function_
$v_*$ such that
$$Bv_* = v_*.$$

---
# Value Iteration Algorithm

![w=100%,v=middle](value_iteration.pdf)

---
# Policy Iteration Algorithm

![w=70%,h=center](policy_iteration.pdf)
