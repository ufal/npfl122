title: NPFL122, Lecture 1
class: title, langtech, cc-by-sa
# Introduction to Reinforcement Learning

## Milan Straka

### October 7, 2019

---
section: History
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s ‚Äì Richard Bellman

~~~
- Trial and error learning ‚Äì since 1850s
  - Law and effect ‚Äì Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, ‚Ä¶ ‚Äì 1950s and 1960s
  - Tsetlin, Holland, Klopf ‚Äì 1970s
  - Sutton, Barto ‚Äì since 1980s

~~~
- Arthur Samuel ‚Äì first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro ‚Äì 1992, human-level Backgammon playing program trained solely by self-play

~~~
- IBM Watson in Jeopardy ‚Äì 2011

---
# History of Reinforcement Learning
## Recent successes

- Human-level video game playing (DQN) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C ‚Äì 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow ‚Äì 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala ‚Äì Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala ‚Äì Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Reinforcement Learning
## Recent successes

![w=22%,f=right](r2d2-results.pdf)

- R2D2 ‚Äì Jan 2019
  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training

~~~
- Data-efficient Rainbow ‚Äì Jun 2019
  - learning from ~2 hours of game experience
![w=38%,mw=70%,h=center](der-progress.pdf)

---
# History of Reinforcement Learning
## Recent successes

- AlphaGo

  - Mar 2016 ‚Äì beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master ‚Äì Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero ‚Äì 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero ‚Äì Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=25%,h=center](a0_results.pdf)

---
# History of Reinforcement Learning
## Recent successes

- Dota2 ‚Äì Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN ‚Äì Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW ‚Äì Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)

~~~
- OpenAI Five ‚Äì Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs
    - 180 years of experience per day

~~~
- AlphaStar ‚Äì Jan 2019
  - won 10 out of 11 StarCraft II games against two professional players

---
# History of Reinforcement Learning
## Recent successes

- Optimize non-differentiable loss

  - improved translation quality in 2016
  - better summarization performance

~~~
- Discovering discrete latent structures

~~~
- Effectively search in space of natural language policies

~~~
- TARDIS ‚Äì Jan 2017
  - allow using discrete external memory

~~~
- Neural architecture search (Nov 2016)
  - SoTA CNN architecture generated by another network
  - can search also for suitable RL architectures, new activation functions,
    optimizers‚Ä¶

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, ‚Ä¶

~~~
Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = ùîº[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t ‚âù \argmax_a Q_t(a).$$

---
section: $Œµ$-greedy
# $Œµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# $Œµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# $Œµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} ‚àë_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ‚àë_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $Œµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.pdf)

---
section: Non-stationary Problems
# Fixed Learning Rate

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Œ±(R_n - Q_n).$$

~~~
Converges to the true action values if
$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$

~~~
The bias can be utilized to support exploration at the start of the episode by
setting the initial values to more than the expected value of the optimal
solution.

---
# Optimistic Initial Values and Fixed Learning Rate

![w=85%,h=center,v=middle](optimistic_values.pdf)

---
section: UCB
# Upper Confidence Bound

Using same epsilon for all actions in $Œµ$-greedy method seems inefficient. One
possible improvement is to select action according to upper confidence bound
(instead of choosing a random action with probability $Œµ$):
$$A_t ‚âù \argmax_a \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right].$$

~~~
The updates are then performed as before (e.g., using averaging, or fixed
learning rate $Œ±$).

~~~
Note that if $N_t(a) = 0$, the right expression is assumed to have a value of
$‚àû$.

---
# Motivation Behind Upper Confidence Bound

Actions with little average reward are probably selected too often.

~~~
Instead of simple $Œµ$-greedy approach, we might try selecting an action as
little as possible, but still enough to converge.

~~~
Assuming random variables $X_i$ bounded by $[0, 1]$ and $XÃÑ = ‚àë_{i=1}^N
X_i$, (Chernoff-)Hoeffding's inequality states that
$$P(XÃÑ - ùîº[XÃÑ] ‚â• Œ¥) ‚â§ e^{-2nŒ¥^2}.$$

~~~
Our goal is to choose $Œ¥$ such that for every action,
$$P(Q_t(a) - q_*(a) ‚â• Œ¥) ‚â§ \left(\frac{1}{t}\right)^Œ±.$$

~~~
We can achieve the required inequality (with $Œ±=2$) by setting
$$Œ¥ ‚â• \sqrt{(\ln t)/N_t(a)}.$$

---
# Asymptotical Optimality of UCB

We define _regret_ as a difference of maximum of what we could get
(i.e., repeatedly using action with maximum expectation) and
what a strategy yields, i.e.,
$$\textit{regret}_N ‚âù N \max_a q_*(a) - ‚àë_{i=1}^N ùîº[R_i].$$

~~~
It can be shown that regret of UCB is asymptotically optimal,
see Lai and Robbins (1985), Asymptotically Efficient Adaptive Allocation Rules.

---
# Upper Confidence Bound Results

![w=80%,h=center,v=middle](ucb.pdf)

---
section: Gradient
# Gradient Bandit Algorithms

Let $H_t(a)$ be a numerical _preference_ for an action $a$ at time $t$.

~~~
We could choose actions according to softmax distribution:
$$œÄ(A_t = a) ‚âù \softmax(a) = \frac{e^{H_t(a)}}{‚àë_b e^{H_t(b)}}.$$

~~~
In other words, we model a _distribution_ of the most rewarding action.

~~~
Usually, all $H_1(a)$ are set to zero, which corresponds to random uniform
initial policy.

~~~
Using SGD and MLE loss, we can (and later we will) derive the following algorithm:
$$H_{t+1}(a) ‚Üê H_t(a) + Œ±R_t([a = A_t] - œÄ(a)).$$
The $[a = A_t]$ is a one-hot vector with the element corresponding to action
$A_t$ set to 1.

---
# Gradient Bandit Algorithms

![w=90%,h=center,v=middle](gradient_bandits.pdf)

---
section: Comparison
# Method Comparison

![w=85%,h=center,v=middle](bandits_comparison.pdf)
