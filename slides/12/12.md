title: NPFL122, Lecture 12
class: title, langtech, cc-by-sa
# ST and Gumbel-Softmax, DreamerV2, MERLIN, FTW

## Milan Straka

### December 19, 2022

---
section: DiscreteLatentVars
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation – $\softmax$. This way, we can
compute the derivatives and also predict the most probable class.
(It is possible to derive $\softmax$ as an entropy-regularized $\argmax$.)

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
section: ST
# Stochastic Gradient Estimators

Consider a model with a discrete categorical latent variable $→z$ sampled from
$p(→z; →θ)$, with a loss $L(→z; →ω)$.
~~~
Several gradient estimators have been proposed:

- A REINFORCE-like gradient estimation.

~~~
  Using the identity $∇_{→θ} p(→z; →θ) = p(→z; →θ) ∇_{→θ} \log p(→z; →θ)$, we
  obtain that
~~~
  $$∇_{→θ} 𝔼_{→z} \big[L(→z; →ω)\big] = 𝔼_{→z} \big[L(→z; →ω) ∇_{→θ} \log p(→z; →θ)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$∇_{→θ} 𝔼_{→z} \big[L(→z; →ω)\big] = 𝔼_{→z} \big[(L(→z; →ω) - b) ∇_{→θ} \log p(→z; →θ)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $∇_{→θ} →z ≈ ∇_{→θ} p(→z; →θ)$, which implies
  $∇_{p(→z; →θ)} →z ≈ 1$. Even if the bias can be considerable, it seems to work
  quite well in practice.

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $→p = (p_1, p_2, …, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z ∼ →p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u ∼ U(0, 1)$ and then compute $g = -\log(-\log u)$.


---
class: dbend
# Gumbel Distribution

First recall that exponential distribution $\operatorname{Exp}(λ)$ has
$$\operatorname{PDF}(x; λ) = λ e^{-λx},~~~\operatorname{CDF}(x; λ) = 1 - e^{-λx}.$$

~~~
The standard $\operatorname{Gumbel}(0, 1)$ distribution has
$$\operatorname{PDF}(x) = e^{-x - e^{-x}},~~~\operatorname{CDF}(x) = e^{-e^{-x}}.$$

~~~
The Gumbel distribution can be used to model the distribution of maximum
of a number of samples from the exponential distribution:
~~~
if $x̃$ is a maximum of $N$ samples from the $\operatorname{Exp}(1)$
distribution, we get that
$$P(x̃ - \log N ≤ x)
  = P(x̃ ≤ x + \log N)
  = \operatorname{CDF}_{\operatorname{Exp}(1)}\big(x + \log N\big)^N
  = \Big(1 - \frac{e^{-x}}{N}\Big)^N,$$
~~~
which converges to $e^{-e^{-x}} = \operatorname{CDF}_{\operatorname{Gumbel}(0, 1)}(x)$ for $N → ∞$.

---
class: dbend
# Gumbel-Max Trick Proof

To prove the Gumbel-Max trick, we first reformulate it slightly.

Let $l_i$ be logits of a categorical distribution (so that the class probabilities
$π_i ∝ e^{l_i}$), and let $g_i ∼ \operatorname{Gumbel}(0, 1)$. Then
$$π_k = P\big(k = \argmax\nolimits_i (g_i + l_i)\big).$$

~~~
We first observe that the theorem is invariant to a scalar shift of logits,
so we can without loss of generality assume that $∑_i e^{l_i} = 1$ and $π_i
= e^{l_i}$.

~~~
For convenience, denote $u_i ≝ g_i + l_i$.

~~~
We will use both the $\operatorname{PDF}$ and $\operatorname{CDF}$ of
a $\operatorname{Gumbel}(0, 1)$ distribution:
$$\begin{aligned}
  \operatorname{PDF}(g_i) &= e^{-g_i - e^{-g_i}}, \\
  \operatorname{CDF}(g_i) &= e^{-e^{-g_i}}.
\end{aligned}$$

---
class: dbend
# Gumbel-Max Trick Proof

To finish the proof, we compute

$\displaystyle\kern12em{}\mathllap{P\big(k = \argmax\nolimits_i (g_i + l_i)\big)} = P(u_k ≥ u_i, ∀_{i≠k})$

~~~
$\displaystyle\kern12em{} = ∫ P(u_k) ∏\nolimits_{i≠k} P(u_k ≥ u_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ P(g_k | g_k = u_k - l_k) ∏\nolimits_{i≠k} P(g_i ≤ u_k - l_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ e^{\textcolor{blue}{l_k}-u_k-e^{\textcolor{darkgreen}{l_k}-u_k}} ∏\nolimits_{i≠k} e^{-e^{\textcolor{magenta}{l_i}-u_k}} \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ \textcolor{blue}{π_k}e^{-u_k-\textcolor{darkgreen}{π_k}\textcolor{red}{e^{-u_k}}} ∏\nolimits_{i≠k} e^{-\textcolor{magenta}{π_i}\textcolor{red}{e^{-u_k}}} \d u_k$

~~~
$\displaystyle\kern12em{} = π_k ∫ e^{-u_k-\textcolor{red}{e^{-u_k}} ∑_i π_i} \d u_k$

~~~
$\displaystyle\kern12em{} = π_k ∫ e^{-g_k-e^{-g_k}} \d g_k = π_k ⋅ 1 = π_k.$

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{∑_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot, and
therefore the Gumbel-softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-softmax with a straight-through estimator,
obtaining ST Gumbel-softmax, where we:
- discretize $→y$ as $→z = \argmax →y$,
~~~
- assume $∇_{→θ}→z ≈ ∇_{→θ}→y$, or in other words, $\frac{∂→z}{∂→y} ≈ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)

---
# Applications of Discrete Latent Variables

The discrete latent variables can be used among others to:
- allow the SAC algorithm to be used on **discrete** actions,
  using either Gumbel-softmax relaxation (if the critic takes
  the actions as binary indicators, it is possible to pass not
  just one-hot encoding, but the result of Gumbel-softmax directly),
  or a straight-through estimator;

~~~
- model images using discrete latent variables
  - VQ-VAE, VQ-VAE-2 use “codebook loss” with a straight-through estimator

    ![w=100%](vqvae.png)

---
# Applications of Discrete Latent Variables

- VQ-GAN combines the VQ-VAE and Transformers, where the latter is used
  to generate a sequence of the _discrete_ latents.

![w=100%](vqgan_architecture.png)

---
class: center
# Applications of Discrete Latent Variables – VQ-GAN

<video controls style="width: 90%">
  <source src="https://github.com/CompVis/taming-transformers/raw/9539a92f08ebea816ec6ddecb2dedd6c8664ef08/images/taming.mp4" type="video/mp4">
</video>

---
# Applications of Discrete Latent Variables – DALL-E

- In DALL-E, Transformer is used to model a sequence of words followed by
  a sequence of the discrete image latent variables.

  The Gumbel-softmax relaxation is used to train the discrete latent states,
  with temperature annealed with a cosine decay from 1 to 1/16 over the first
  150k (out of 3M) updates.

![w=100%,h=center](dalle.png)

---
section: DreamerV2
# DreamerV2

![w=40%,f=right](dreamerv2_performance.svgz)

The PlaNet model was followed by Dreamer (Dec 2019) and DreamerV2 (Oct 2020),
which train an agent using reinforcement learning using the model alone.
After 200M environment steps, it surpasses Rainbow on a collection of 55
Atari games (the authors do not mention why they do not use all 57 games)
when training on a single GPU for 10 days per game.

~~~
During training, a policy is learned from 486B compact states “dreamed”
by the model, which is 10,000 times more than the 50M observations from
the real environment (with action repeat 4).

~~~
Interestingly, the latent states are represented as a vector of several
**categorical** variables – 32 variables with 32 classes each are utilized in
the paper.

---
section: 😴₂Model
# DreamerV2 – Model Learning

The model in DreamerV2 is learned using the RSSM, collecting agent experiences
of observations, actions, rewards, and discount factors (0.995 within episode
and 0 at an episode end). Training is performed on batches of 50 sequences
of length at most 50 each.

~~~
![w=45%,f=right](dreamerv2_model_learning.svgz)

$$\begin{aligned}
\textrm{recurrent model:}      && h_t &= f_φ(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{representation model:} && s_t &∼ q_φ(s_t | h_t,x_t), \\
\textrm{transition predictor:} && s̄_t &∼ p_φ(s̄_t | h_t), \\
\textrm{image predictor:}      && x̄_t &∼ p_φ(x̄_t | h_t,s_t), \\
\textrm{reward predictor:}     && r̄_t &∼ p_φ(r̄_t | h_t,s_t), \\
\text{discount predictor:}     && γ̄_t &∼ p_φ(γ̄_t | h_t,s_t).
\end{aligned}$$

~~~
![w=75%,h=center](dreamerv2_st_gradients.svgz)

---
# DreamerV2 – Model Learning

The following loss function is used:

$$\begin{aligned}
𝓛(φ) = 𝔼_{q_φ(s_{1:T} | a_{1:T}, x_{1:T})}\Big[∑\nolimits_{t=1}^T &
  \underbrace{-\log p_φ(x_t | h_t,s_t)}_\textrm{image log loss}
  \underbrace{-\log p_φ(r_t | h_t,s_t)}_\textrm{reward log loss}
  \underbrace{-\log p_φ(γ_t | h_t,s_t)}_\textrm{discount log loss} \\
 &\underbrace{+β D_\textrm{KL}\big[q_φ(s_t | h_t,x_t) \| p_φ(s_t | h_t)\big]}_\textrm{KL loss}
\Big].
\end{aligned}$$

~~~
In the KL term, we train both the prior and the encoder. However, regularizing
the encoder towards the prior makes training harder (especially at the
beginning), so the authors propose **KL balancing**, minimizing the KL term
faster for the prior ($α=0.8$) than for the posterior.

![w=100%](dreamerv2_kl_balancing.svgz)

---
section: 😴₂Policy
# DreamerV2 – Policy Learning

![w=50%,f=right](dreamerv2_policy_learning.svgz)

The policy is trained solely from the model, starting from the encountered
posterior states and then considering $H=15$ actions simulated in the compact
latent state.

~~~
We train an actor predicting $π_ψ(a_t | s_t)$ and a critic predicting
$$\textstyle v_ξ(s_t) = 𝔼_{p_φ, π_ψ} \big[∑_{r ≥ t} (∏_{r'=t+1}^r γ_{r'}) r_t\big].$$

~~~
The critic is trained by estimating the truncated $λ$-return as
$$V_t^λ = r_t + \gamma_t\begin{cases}
  (1 - λ) v_ξ(\hat{z}_{t+1}) + λ V_{t+1}^λ & \textrm{if~~}t<H, \\
  v_ξ(\hat{z}_H) & \textrm{if~~}t=H. \\
\end{cases}$$
and then minimizing the MSE.

---
# DreamerV2 – Policy Learning

The actor is trained using two approaches:
- the REINFORCE-like loss (with a baseline), which is unbiased, but has a high
  variance (even with the baseline);
~~~
- the reparametrization of discrete actions using a straight-through gradient
  estimation, which is biased, but has lower variance.

~~~
$$\begin{aligned}
𝓛(ψ) = 𝔼_{p_φ,π_ψ}\Big[∑\nolimits_{t=1}^{H-1} \big(&
    \underbrace{-ρ \log π_ψ(a_t|s_t)\operatorname{stop\char`_gradient}(V_t^λ-v_ξ({s_t}))}_\textrm{reinforce} \\
   &\underbrace{-(1-ρ) V_t^λ}_\textrm{dynamics backprop}\,\,
    \underbrace{-η H(a_t|s_t)}_\textrm{entropy regularizer}\big)\Big]
\end{aligned}$$

For Atari domains, authors use $ρ = 1$ and $η=10^{-3}$, while for continuous
actions, $ρ = 1$ works better (presumably because of the bias in case of
discrete actions) and $η=10^{-4}$ is used.

---
# DreamerV2 – Results

The authors evaluate on 55 Atari games. They argue that the commonly used
metrics have various flaws:
- **gamer-normalized median** ignores scores on half of the games,
- **gamer-normalized mean** is dominated by several games where the agent
  achieves super-human performance by several orders.

~~~
They therefore propose two additional ones:
- **record-normalized mean** normalizes with respect to any registered human
  world record for each game; however, in some games the agents still achieve
  super-human-record performance;
- **clipped record-normalized mean** additionally clips each score to 1;
  this measure is used as the primary metric in the paper.

---
# DreamerV2 – Results

![w=100%](dreamerv2_results_graph.svgz)

![w=67%,h=center](dreamerv2_results_table.svgz)

Scheduling anneals actor gradient mixing $ρ$ (from 0.1 to 0), entropy loss scale,
KL, lr.

---
# DreamerV2 – Ablations

![w=100%](dreamerv2_ablations.svgz)

![w=67%,h=center](dreamerv2_ablations_table.svgz)

---
# DreamerV2 – Discrete Latent Variables

Categorical latent variables outperform Gaussian latent variables on 42 games,
tie on 5 games and decrease performance on 8 games (where a tie is defined as
being within 5\%).

The authors provide several hypotheses why could the categorical latent
variables be better:
- Categorical prior can perfectly match aggregated posterior, because mixture of
  categoricals is categorical, which is not true for Gaussians.

~~~
- Sparsity achieved by the 32 categorical variables with 32 classes each could
  be beneficial for generalization.

~~~
- Contrary to intuition, optimizing categorical variables might be easier than
  optimizing Gaussians, because the straight-through estimator ignores a term
  which would otherwise scale the gradient, which could reduce
  exploding/vanishing gradient problem.

~~~
- Categorical variables could be a better match for modeling discrete aspect
  of the Atari games (defeating an enemy, collecting reward, entering a room, …).

---
# DreamerV2 – Comparison, Hyperparametres

![w=100%](dreamerv2_comparison.svgz)

~~~
![w=100%](dreamerv2_hyperparameters.svgz)

---
section: MERLIN
# MERLIN

In a partially-observable environment, keeping all information in the RNN state
is substantially limiting. Therefore, _memory-augmented_ networks can be used to
store suitable information in external memory (in the lines of NTM, DNC, or MANN
models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=95.75%,mw=50%,h=center](merlin_rl-lstm.svgz)![w=95%,mw=50%,h=center](merlin_rl-mem.svgz)

---
# MERLIN – Memory Module

![w=30%,f=right](merlin_rl-mem.svgz)

Let $→M$ be a memory matrix of size $N_\textit{mem} × 2|→e|$.

~~~
Assume we have already encoded observations as $→e_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
them by a deep LSTM (two layers are used in the paper) to compute $→h_t$.

~~~
Then, we apply a linear layer to $→h_t$, computing $K$ key vectors
$→k_1, …, →k_K$ of length $2|→e|$ and $K$ positive scalars $β_1, …, β_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $→k_i$ and all memory
rows $→M_j$, multiply the similarities by $β_i$ and pass them through a $\softmax$
to obtain weights $→ω_i$. The read vector is then computed as $⇉M →ω_i$.

~~~
**Writing:** We find one-hot write index $→v_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $→v_\textit{ret} ← γ →v_\textit{ret} + (1 - γ) →v_\textit{wr}$,
and retroactively update the memory matrix using
$⇉M ← ⇉M + →v_\textit{wr}[→e_t, 0] + →v_\textit{ret}[0, →e_t]$.

---
# MERLIN — Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therefore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $→z$ and storing them in memory.

~~~
We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a (Gaussian diagonal) _prior_ distribution over $→z_t$ predicts next state variable
conditioned on history of state variables and actions
$p(→z_t^\textrm{prior} | →z_{t-1}, a_{t-1}, …, →z_1, a_1)$,
and _posterior_ corrects the prior using the new observation $→o_t$, forming
a better estimate $q(→z_t | →o_t, →z_t^\textrm{prior}, →z_{t-1}, a_{t-1}, …, →z_1, a_1) + →z_t^\textrm{prior}$.

---
# MERLIN — Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and the posterior to the loss, to ensure
  consistency between the prior and the posterior.

~~~
![w=85%,h=center](merlin_diagram.svgz)

---
# MERLIN — Algorithm

![w=100%](merlin_algorithm.svgz)

---
# MERLIN

![w=70%,h=center](merlin_tasks.svgz)

---
# MERLIN

![w=50%,h=center](merlin_analysis.svgz)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.svgz)

---
section: CTF-FTW
# For the Win agent for Capture The Flag

![w=100%](ctf_overview.svgz)

---
# For the Win agent for Capture The Flag

- Extension of the MERLIN architecture.

~~~
- Hierarchical RNN with two timescales.

~~~
- Population based training controlling KL divergence penalty weights,
  internal dense rewards, slow ticking RNN speed, and gradient flow factor from
  fast to slow RNN.

  In every game, teams of similarly skilled agents were selected, and the
  authors state it is crucial to employ several agents instead of just one
  (30 simultaneously trained agents are used).

---
# For the Win agent for Capture The Flag

![w=47%,h=center](ctf_architecture.svgz)

---
# For the Win agent for Capture The Flag

![w=80%,h=center](ctf_curves.svgz)

