title: NPFL122, Lecture 12
class: title, langtech, cc-by-nc-sa
# ST and Gumbel-Softmax, DreamerV2, MERLIN, FTW

## Milan Straka

### December 20, 2021

---
section: DiscreteLatentVars
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation â€“ $\softmax$. This way, we can
compute the derivatives, and also predict the most probable class.

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
section: ST
# Stochastic Gradient Estimators

Consider a model with a discrete categorical latent variable $â†’z$ sampled from
$p(â†’z; â†’Î¸)$, with a loss $L(â†’z; â†’Ï‰)$.
~~~
Several gradient estimators have been proposed:

- A REINFORCE-like gradient estimation.

  Using the identity $âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸) = p(â†’z; â†’Î¸) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)$, we
  obtain that
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[(L(â†’z; â†’Ï‰) - b) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $âˆ‡_{â†’Î¸} â†’z â‰ˆ âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸)$, which implies
  $âˆ‡_{p(â†’z; â†’Î¸)} â†’z â‰ˆ 1$. Even if the bias can be considerable, it seems to work
  quite well in practice.

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $â†’p = (p_1, p_2, â€¦, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z âˆ¼ â†’p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u âˆ¼ U(0, 1)$ and then compute $g = -\log(-\log u)$.

---
class: dbend
# Gumbel-Max Trick Proof

To prove the Gumbel-Max trick, we first reformulate it slightly.

Let $l_i$ be logits of a categorical distribution (so that the class probabilities
$Ï€_i âˆ e^{l_i}$), and let $g_i âˆ¼ \operatorname{Gumbel}(0, 1)$. Then
$$Ï€_k = P\big(k = \argmax\nolimits_i (g_i + l_i)\big).$$

~~~
We first observe that the theorem is invariant to a scalar shift of logits,
so we can without loss of generality assume that $âˆ‘_i e^{l_i} = 1$ and $Ï€_i
= e^{l_i}$.

~~~
For convenience, denote $u_i â‰ g_i + l_i$.

~~~
Recall that the $\operatorname{PDF}$ and $\operatorname{CDF}$ of
a $\operatorname{Gumbel}(0, 1)$ distribution are:
$$\begin{aligned}
  \operatorname{PDF}(g_i) &= e^{-g_i - e^{-g_i}}, \\
  \operatorname{CDF}(g_i) &= e^{-e^{-g_i}}.
\end{aligned}$$

---
class: dbend
# Gumbel-Max Trick Proof

To finish the proof, we compute
$$\begin{aligned}
P\big(k = \argmax\nolimits_i (g_i + l_i)\big)
 &= P(u_k â‰¥ u_i, âˆ€_{iâ‰ k}) \\
 &= âˆ« P(u_k) âˆ\nolimits_{iâ‰ k} P(u_k â‰¥ u_i | u_k) \d u_k \\
 &= âˆ« P(g_k | g_k = u_k - l_k) âˆ\nolimits_{iâ‰ k} P(g_i â‰¤ u_k - l_i | u_k) \d u_k \\
 &= âˆ« e^{\textcolor{blue}{l_k}-u_k-e^{\textcolor{darkgreen}{l_k}-u_k}} âˆ\nolimits_{iâ‰ k} e^{-e^{\textcolor{magenta}{l_i}-u_k}} \d u_k \\
 &= âˆ« \textcolor{blue}{Ï€_k}e^{-u_k-\textcolor{darkgreen}{Ï€_k}\textcolor{red}{e^{-u_k}}}
    âˆ\nolimits_{iâ‰ k} e^{-\textcolor{magenta}{Ï€_i}\textcolor{red}{e^{-u_k}}} \d u_k \\
 &= Ï€_k âˆ« e^{-u_k-\textcolor{red}{e^{-u_k}} âˆ‘_i Ï€_i} \d u_k \\
 &= Ï€_k âˆ« e^{-g_k-e^{-g_k}} \d g_k = Ï€_k â‹… 1 = Ï€_k.
\end{aligned}$$

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{âˆ‘_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot and
therefore the Gumbel-softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-softmax with a straight-through estimator,
obtaining ST Gumbel-softmax, where we:
- discretize $â†’y$ as $â†’z = \argmax â†’y$,
~~~
- assume $âˆ‡_{â†’Î¸}â†’z â‰ˆ âˆ‡_{â†’Î¸}â†’y$, or in other words, $\frac{âˆ‚â†’z}{âˆ‚â†’y} â‰ˆ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)

---
# Applications of Discrete Latent Variables

The discrete latent variables can be used among others to:
- allow the SAC algorithm to be used on **discrete** actions,
  using either Gumbel-softmax relaxation (if the critic takes
  the actions as binary indicators, it is possible to pass not
  just one-hot encoding, but the result of Gumbel-softmax directly),
  or a straight-through estimator;

~~~
- model images using discrete latent variables
  - VQ-VAE, VQ-VAE-2 use â€œcodebook lossâ€ with a straight-through estimator

    ![w=100%](vqvae.png)

---
# Applications of Discrete Latent Variables

- VQ-GAN combines the VQ-VAE and Transformers, where the latter is used
  to generate a sequence of the _discrete_ latents.

![w=100%](vqgan_architecture.png)

---
class: center
# Applications of Discrete Latent Variables â€“ VQ-GAN

<video controls style="width: 90%">
  <source src="https://github.com/CompVis/taming-transformers/raw/9539a92f08ebea816ec6ddecb2dedd6c8664ef08/images/taming.mp4" type="video/mp4">
</video>

---
# Applications of Discrete Latent Variables â€“ DALL-E

- In DALL-E, Transformer is used to model a sequence of words followed by
  a sequence of the discrete image latent variables.

  The Gumbel-softmax relaxation is used to train the discrete latent states,
  with temperature annealed with a cosine decay from 1 to 1/16 over the first
  150k (out of 3M) updates.

![w=100%,h=center](dalle.png)

---
section: DreamerV2
# DreamerV2

![w=40%,f=right](dreamerv2_performance.svgz)

The PlaNet model was followed by Dreamer (Dec 2019) and DreamerV2 (Oct 2020),
which train an agent using reinforcement learning using the model alone.
After 200M environment steps, it surpasses Rainbow on a collection of 55
Atari games (the authors do not mention why they do not use all 57 games)
when training on a single GPU for 10 days per game.

~~~
During training, a policy is learned from 486B compact states â€œdreamedâ€
by the model, which is 10,000 times more than the 50M observations from
the real environment (with action repeat 4).

~~~
Interestingly, the latent states are represented as a vector of several
**categorical** variables â€“ 32 variables with 32 classes each are utilized in
the paper.

---
section: ğŸ˜´â‚‚Model
# DreamerV2 â€“ Model Learning

The model in DreamerV2 is learned using the RSSM, collecting agent experiences
of observations, actions, rewards and discount factors (0.999 within episode
and 0 at an episode end). Training is performed on batches of 50 sequences
of length at most 50 each.

~~~
![w=45%,f=right](dreamerv2_model_learning.svgz)

$$\begin{aligned}
\textrm{recurrent model:}      && h_t &= f_Ï†(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{representation model:} && s_t &âˆ¼ q_Ï†(s_t | h_t,x_t), \\
\textrm{transition predictor:} && sÌ„_t &âˆ¼ p_Ï†(sÌ„_t | h_t), \\
\textrm{image predictor:}      && xÌ„_t &âˆ¼ p_Ï†(xÌ„_t | h_t,s_t), \\
\textrm{reward predictor:}     && rÌ„_t &âˆ¼ p_Ï†(rÌ„_t | h_t,s_t), \\
\text{discount predictor:}     && Î³Ì„_t &âˆ¼ p_Ï†(Î³Ì„_t | h_t,s_t).
\end{aligned}$$

~~~
![w=75%,h=center](dreamerv2_st_gradients.svgz)

---
# DreamerV2 â€“ Model Learning

The following loss function is used:

$$\begin{aligned}
ğ“›(Ï†) = ğ”¼_{q_Ï†(s_{1:T} | a_{1:T}, x_{1:T})}\Big[âˆ‘\nolimits_{t=1}^T &
  \underbrace{-\log p_Ï†(x_t | h_t,s_t)}_\textrm{image log loss}
  \underbrace{-\log p_Ï†(r_t | h_t,s_t)}_\textrm{reward log loss}
  \underbrace{-\log p_Ï†(Î³_t | h_t,s_t)}_\textrm{discount log loss} \\
 &\underbrace{+Î² D_\textrm{KL}\big[q_Ï†(s_t | h_t,x_t) \| p_Ï†(s_t | h_t)\big]}_\textrm{KL loss}
\Big].
\end{aligned}$$

~~~
In the KL term, we train both the prior and the encoder. However, regularizing
the encoder towards the prior makes training harder (especially at the
beginning), so the authors propose **KL balancing**, minimizing the KL term
faster for the prior ($Î±=0.8$) than for the posterior.

![w=100%](dreamerv2_kl_balancing.svgz)

---
section: ğŸ˜´â‚‚Policy
# DreamerV2 â€“ Policy Learning

![w=50%,f=right](dreamerv2_policy_learning.svgz)

The policy is trained solely from the model, starting from the encountered
posterior states and then considering $H=15$ actions simulated in the compact
latent state.

~~~
We train an actor predicting $Ï€_Ïˆ(a_t | s_t)$ and a critic predicting
$$\textstyle v_Î¾(s_t) = ğ”¼_{p_Ï†, Ï€_Ïˆ} \big[âˆ‘_{r â‰¥ t} (âˆ_{r'=t+1}^r Î³_{r'}) r_t\big].$$

~~~
The critic is trained by estimating the truncated $Î»$-return as
$$V_t^Î» = r_t + \gamma_t\begin{cases}
  (1 - Î») v_Î¾(\hat{z}_{t+1}) + Î» V_{t+1}^Î» & \textrm{if~~}t<H, \\
  v_Î¾(\hat{z}_H) & \textrm{if~~}t=H. \\
\end{cases}$$
and then minimizing the MSE.

---
# DreamerV2 â€“ Policy Learning

The actor is trained using two approaches:
- the REINFORCE-like loss (with a baseline), which is unbiased, but has a high
  variance (even with the baseline);
~~~
- the reparametrization of discrete actions using a straight-through gradient
  estimation, which is biased, but has lower variance.

~~~
$$\begin{aligned}
ğ“›(Ïˆ) = ğ”¼_{p_Ï†,Ï€_Ïˆ}\Big[âˆ‘\nolimits_{t=1}^{H-1} \big(&
    \underbrace{-Ï \log Ï€_Ïˆ(a_t|s_t)\operatorname{stop gradient}(V_t^Î»-v_Î¾({s_t}))}_\textrm{reinforce} \\
   &\underbrace{-(1-Ï) V_t^Î»}_\textrm{dynamics backprop}\,\,
    \underbrace{-Î· H(a_t|s_t)}_\textrm{entropy regularizer}\big)\Big]
\end{aligned}$$

For Atari domains, the authors use $Ï = 0$, while for continuous actions, $Ï = 1$ works better
(presumably because of the bias in case of discrete actions).

---
# DreamerV2 â€“ Results

The authors evaluate on 55 Atari games. They argue that the commonly used
metrics have various flaws:
- **gamer-normalized median** ignores scores on half of the games,
- **gamer-normalized mean** is dominated by several games where the agent
  achieves super-human performance by several orders.

~~~
They therefore propose two additional ones:
- **record-normalized mean** normalizes with respect to any registered human
  world record for each game; however, in some games the agents still achieve
  super-human-record performance;
- **clipped record-normalized mean** additionally clips each score to 1;
  this measure is used as the primary metric in the paper.

---
# DreamerV2 â€“ Results

![w=100%](dreamerv2_results_graph.svgz)

![w=67%,h=center](dreamerv2_results_table.svgz)

Scheduling anneals actor gradient mixing $Ï$ from 0.1 to 0, entropy loss scale,
KL, lr.

---
# DreamerV2 â€“ Ablations

![w=100%](dreamerv2_ablations.svgz)

![w=67%,h=center](dreamerv2_ablations_table.svgz)

---
# DreamerV2 â€“ Discrete Latent Variables

Categorical latent variables outperform Gaussian latent variables on 42 games,
tie on 5 games and decrease performance on 8 games (where a tie is defined as
being within 5\%).

The authors provide several hypotheses why could the categorical latent
variables be better:
- Categorical prior can perfectly math aggregated posterior, because mixture of
  categoricals is categorical, which is not true for Gaussians.

~~~
- Sparsity achieved by the 32 categorical variables with 32 classes each could
  be beneficial for generalization.

~~~
- Contrary to intuition, optimizing categorical variables might be easier than
  optimizing Gaussians, because the straight-through estimator ignores a term
  which would otherwise scale the gradient, which could reduce
  exploding/vanishing gradient problem.

~~~
- Categorical variables could be better match for modeling discrete aspect
  of the Atari games (defeating an enemy, collecting reward, entering a room, â€¦).

---
# DreamerV2 â€“ Comparison, Hyperparametres

![w=100%](dreamerv2_comparison.svgz)

~~~
![w=100%](dreamerv2_hyperparameters.svgz)

---
section: MERLIN
# MERLIN

However, keeping all information in the RNN state is substantially limiting.
Therefore, _memory-augmented_ networks can be used to store suitable information
in external memory (in the lines of NTM, DNC or MANN models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=50%,h=center](merlin_rl-mem.svgz)

---
# MERLIN â€“ Memory Module

![w=30%,f=right](merlin_rl-mem.svgz)

Let $â†’M$ be a memory matrix of size $N_\textit{mem} Ã— 2|â†’e|$.

~~~
Assume we have already encoded observations as $â†’e_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
then by a deep LSTM (two layers are used in the paper) to compute $â†’h_t$.

~~~
Then, we apply a linear layer to $â†’h_t$, computing $K$ key vectors
$â†’k_1, â€¦ â†’k_K$ of length $2|â†’e|$ and $K$ positive scalars $Î²_1, â€¦, Î²_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $â†’k_i$ and all memory
rows $M_j$, multiply the similarities by $Î²_i$ and pass them through a $\softmax$
to obtain weights $â†’Ï‰_i$. The read vector is then computed as $â‡‰M â†’w_i$.

~~~
**Writing:** We find one-hot write index $â†’v_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $â†’v_\textit{ret} â† Î³ â†’v_\textit{ret} + (1 - Î³) â†’v_\textit{wr}$, and update
the memory matrix using $â‡‰M â† â‡‰M + â†’v_\textit{wr}[â†’e_t, 0] + â†’v_\textit{ret}[0, â†’e_t]$.

---
# MERLIN â€” Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therefore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $â†’z$ and storing them in memory.

~~~
According to the paper, the idea of unsupervised and predictive modeling has
been entertained for decades, and recent discussions have proposed such modeling
to be connected to hippocampal memory.

We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a _prior_ distribution over $â†’z_t$ predicts next state variable
conditioned on history of state variables and actions $p(â†’z_t | â†’z_{t-1}, a_{t-1}, â€¦, â†’z_1, a_1)$,
and _posterior_ corrects the prior using the new observation $â†’o_t$, forming
a better estimate $q(â†’z_t | â†’o_t, â†’z_{t-1}, a_{t-1}, â€¦, â†’z_1, a_1)$.

---
# MERLIN â€” Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and posterior to the loss, to ensure
  consistency between the prior and posterior.

~~~
![w=85%,h=center](merlin_diagram.svgz)

---
# MERLIN â€” Algorithm

![w=100%](merlin_algorithm.svgz)

---
# MERLIN

![w=70%,h=center](merlin_tasks.svgz)

---
# MERLIN

![w=50%,h=center](merlin_analysis.svgz)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.svgz)

---
section: CTF-FTW
# For the Win agent for Capture The Flag

![w=100%](ctf_overview.svgz)

---
# For the Win agent for Capture The Flag

- Extension of the MERLIN architecture.

~~~
- Hierarchical RNN with two timescales.

~~~
- Population based training controlling KL divergence penalty weights,
  slow ticking RNN speed and gradient flow factor from fast to slow RNN.

---
# For the Win agent for Capture The Flag

![w=47%,h=center](ctf_architecture.svgz)

---
# For the Win agent for Capture The Flag

![w=80%,h=center](ctf_curves.svgz)

