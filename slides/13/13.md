title: NPFL122, Lecture 13
class: title, langtech, cc-by-nc-sa
# Multi-Agent RL, PPO, MAPPO

## Milan Straka

### January 02, 2023

---
section: MARL
# Multi-Agent Reinforcement Learning

We use the thesis
> _Cooperative Multi-Agent Reinforcement Learning_ https://dspace.cuni.cz/handle/20.500.11956/127431

as an introduction text.

---
section: NPG
# Natural Policy Gradient

The following approach has been introduced by Kakade (2002).

~~~
Using policy gradient theorem, we are able to compute $âˆ‡ v_Ï€$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $â†’d$ which maximizes $v_Ï€(s; â†’Î¸ + â†’d)$ under
the constraint that $\|â†’d\|^2$ is bounded by a small constant is exactly
the gradient $âˆ‡ v_Ï€$.

~~~
Normally, the length $|â†’d|^2$ is computed using Euclidean metric. But in general,
any metric could be used. Representing a metric using a positive-definite matrix
$â‡‰G$ (identity matrix for Euclidean metric), we can compute the distance as
$\|â†’d\|_{â‡‰G}^2 = âˆ‘_{ij} G_{ij} d_i d_j = â†’d^T â‡‰G â†’d$. The steepest ascent direction is
then given by $â‡‰G^{-1} âˆ‡ v_Ï€$.

~~~
Note that when $â‡‰G$ is the Hessian $â‡‰H v_Ï€$, the above process is exactly
Newton's method.

---
# Natural Policy Gradient

![w=100%,v=middle](npg.svgz)

---
# Natural Policy Gradient

A suitable choice for the metric is _Fisher information matrix_,
which is defined as a _covariance matrix_ of the **score function**
$âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸)$.
~~~
The expectaton of the score function is 0:
$$ğ”¼_{Ï€(a|s; â†’Î¸)} âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸) = âˆ« Ï€(a|s; â†’Î¸) \frac{âˆ‡_{â†’Î¸} Ï€(a|s; â†’Î¸)}{Ï€(a|s; â†’Î¸)} \d a
  = âˆ‡_{â†’Î¸} âˆ« Ï€(a|s; â†’Î¸) \d a = âˆ‡_{â†’Î¸} 1 = 0.$$

~~~
The Fisher information matrix is therefore
$$F_s(â†’Î¸) â‰ ğ”¼_{Ï€(a | s; â†’Î¸)} \Big[\big(âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸)\big) \big(âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸)\big)^T \Big].$$

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

~~~
The Fisher information matrix is also a Hessian of the
$D_\textrm{KL}(Ï€(a | s; â†’Î¸) \| Ï€(a | s; â†’Î¸')$:
$$F_s(â†’Î¸) = \frac{âˆ‚^2}{âˆ‚Î¸_i' âˆ‚Î¸_j'} D_\textrm{KL}\big(Ï€(a | s; â†’Î¸) \| Ï€(a | s; â†’Î¸')\big)\Big|_{â†’Î¸' = â†’Î¸}.$$

---
# Natural Policy Gradient

Using the metric
$$F(â†’Î¸) = ğ”¼_{s âˆ¼ Î¼_{â†’Î¸}} F_s(â†’Î¸)$$
we want to update the parameters using $â†’d_F â‰ F(â†’Î¸)^{-1} âˆ‡ v_Ï€$.

~~~
An interesting property of using the $â†’d_F$ to update the parameters is that
- updating $â†’Î¸$ using $âˆ‡ v_Ï€$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $â†’Î¸$ using $F(â†’Î¸)^{-1} âˆ‡ v_Ï€$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $â†’d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$â†’d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $â‡‰Aâ†’x = â†’b$
in an iterative manner, by using $â‡‰A$ only to compute products $â‡‰Aâ†’v$ for
a suitable $â†’v$.

~~~
Therefore, $â†’d_F$ is found as a solution of
$$F(â†’Î¸)â†’d_F = âˆ‡ v_Ï€$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{Î±}{\sqrt{(âˆ‡ v_Ï€)^T F(â†’Î¸)^{-1} âˆ‡ v_Ï€}}.$$

---
section: TRPO
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $Ï€, Ï€Ìƒ$, we can write
$$v_Ï€Ìƒ = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€Ìƒ)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s),$$
where $a_Ï€(a | s)$ is the advantage function $q_Ï€(a | s) - v_Ï€(s)$ and
$Î¼(Ï€Ìƒ)$ is the on-policy distribution of the policy $Ï€Ìƒ$.

~~~
Analogously to policy improvement, we see that if $a_Ï€(a | s) â‰¥0$, policy
$Ï€Ìƒ$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s âˆ¼ Î¼(Ï€Ìƒ)$ is costly. Therefore, we instead
consider
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s).$$

---
# Trust Region Policy Optimization
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(a | s)$$

It can be shown that for parametrized $Ï€(a | s; â†’Î¸)$ the $L_Ï€(Ï€Ìƒ)$ matches
$v_{Ï€Ìƒ}$ to the first order.

~~~
Schulman et al. additionally proves that if we denote
$Î± = D_\textrm{KL}^\textrm{max}(Ï€_\textrm{old} \| Ï€_\textrm{new})
   = \max_s D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) \| Ï€_\textrm{new}(â‹…|s)\big)$, then
$$v_{Ï€_\textrm{new}} â‰¥ L_{Ï€_\textrm{old}}(Ï€_\textrm{new}) - \frac{4ÎµÎ³}{(1-Î³)^2}Î±\textrm{~~~where~~~}Îµ = \max_{s, a} |a_Ï€(s, a)|.$$

~~~
Therefore, TRPO maximizes $L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})$ subject to
$D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) < Î´$, where
- $D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) = ğ”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0})} [D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) \| Ï€_\textrm{new}(â‹…|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $Î´$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $Ï€$.

---
# Trust Region Policy Optimization

$$\textrm{maximize}~~L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})
 = ğ”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0}), a âˆ¼ Ï€_{â†’Î¸_0}(a | s)} \Big[\tfrac{Ï€_{â†’Î¸}(a|s)}{Ï€_{â†’Î¸_0}(a|s)}a_{Ï€_{â†’Î¸_0}}(a | s)\Big]
 ~~\textrm{subject to}~~D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) < Î´$$

The parameters are updated using $â†’d_F = F(â†’Î¸)^{-1} âˆ‡ L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{Î´/(â†’d_F^T F(â†’Î¸)^{-1} â†’d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
# Proximal Policy Optimization

A simplification of TRPO which can be implemented using a few lines of code.

Let $r_t(â†’Î¸) â‰ \frac{Ï€(A_t|S_t; â†’Î¸)}{Ï€(A_t|S_t; â†’Î¸_\textrm{old})}$. PPO
maximizes the objective (i.e., you should minimize its negation)
$$L^\textrm{CLIP}(â†’Î¸) â‰ ğ”¼_t\Big[\min\big(r_t(â†’Î¸) AÌ‚_t, \operatorname{clip}(r_t(â†’Î¸), 1-Îµ, 1+Îµ) AÌ‚_t)\big)\Big].$$

Such a $L^\textrm{CLIP}(â†’Î¸)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $AÌ‚_t$ are additionally estimated using the so-called
_generalized advantage estimation_, which is just an analogue
of the truncated n-step lambda-return:
$$AÌ‚_t = âˆ‘_{i=0}^{n-1} Î³^i Î»^i \big(R_{t+1+i} + Î³ V(S_{t+i+1}) - V(S_{t + i})\big).$$

~~~
![w=80%,h=center](ppo_algorithm.svgz)

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)

---
section: HideAndSeek
# Multi-Agent Hide-and-Seek

As another example, consider https://openai.com/blog/emergent-tool-use/.
