title: NPFL122, Lecture 13
class: title, langtech, cc-by-sa
# Explicit Memory, MERLIN, FTW

## Milan Straka

### January 5, 2021

---
section: POMDP
# MDPs and Partially Observable MDPs

Recall that a **Markov decision process** (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
- $𝓐$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a **reward** $r ∈ ℝ$,
- $γ ∈ [0, 1]$ is a **discount factor**.

~~~
**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(𝓢, 𝓐, p, γ, 𝓞, o)$, where in addition to an MDP
- $𝓞$ is a set of observations,
- $o(O_t | S_t, A_{t-1})$ is an observation model, which is used as agent input
  instead of $S_t$.

---
# Partially Observable MDPs

In Deep RL, partially observable MDPs are usually handled using recurrent
networks. After suitable encoding of input observation $O_t$ and previous
action $A_{t-1}$, a RNN (usually LSTM) unit is used to model the current $S_t$
(or its suitable latent representation), which is in turn utilized to produce
$A_t$.

![w=50%,h=center](merlin_rl-lstm.svgz)

---
section: MERLIN
# MERLIN

However, keeping all information in the RNN state is substantially limiting.
Therefore, _memory-augmented_ networks can be used to store suitable information
in external memory (in the lines of NTM, DNC or MANN models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=50%,h=center](merlin_rl-mem.svgz)

---
# MERLIN – Memory Module

![w=30%,f=right](merlin_rl-mem.svgz)

Let $→M$ be a memory matrix of size $N_\textit{mem} × 2|→e|$.

~~~
Assume we have already encoded observations as $→e_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
then by a deep LSTM (two layers are used in the paper) to compute $→h_t$.

~~~
Then, we apply a linear layer to $→h_t$, computing $K$ key vectors
$→k_1, … →k_K$ of length $2|→e|$ and $K$ positive scalars $β_1, …, β_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $→k_i$ and all memory
rows $M_j$, multiply the similarities by $β_i$ and pass them through a $\softmax$
to obtain weights $→ω_i$. The read vector is then computed as $⇉M →w_i$.

~~~
**Writing:** We find one-hot write index $→v_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $→v_\textit{ret} ← γ →v_\textit{ret} + (1 - γ) →v_\textit{wr}$, and update
the memory matrix using $⇉M ← ⇉M + →v_\textit{wr}[→e_t, 0] + →v_\textit{ret}[0, →e_t]$.

---
# MERLIN — Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therefore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $→z$ and storing them in memory.

~~~
According to the paper, the idea of unsupervised and predictive modeling has
been entertained for decades, and recent discussions have proposed such modeling
to be connected to hippocampal memory.

We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a _prior_ distribution over $→z_t$ predicts next state variable
conditioned on history of state variables and actions $p(→z_t | →z_{t-1}, a_{t-1}, …, →z_1, a_1)$,
and _posterior_ corrects the prior using the new observation $→o_t$, forming
a better estimate $q(→z_t | →o_t, →z_{t-1}, a_{t-1}, …, →z_1, a_1)$.

---
# MERLIN — Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and posterior to the loss, to ensure
  consistency between the prior and posterior.

~~~
![w=85%,h=center](merlin_diagram.svgz)

---
# MERLIN — Algorithm

![w=37%,h=center](merlin_algorithm.svgz)

---
# MERLIN

![w=70%,h=center](merlin_tasks.svgz)

---
# MERLIN

![w=50%,h=center](merlin_analysis.svgz)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.svgz)

---
section: CTF-FTW
# For the Win agent for Capture The Flag

![w=100%](ctf_overview.svgz)

---
# For the Win agent for Capture The Flag

- Extension of the MERLIN architecture.

~~~
- Hierarchical RNN with two timescales.

~~~
- Population based training controlling KL divergence penalty weights,
  slow ticking RNN speed and gradient flow factor from fast to slow RNN.

---
# For the Win agent for Capture The Flag

![w=47%,h=center](ctf_architecture.svgz)

---
# For the Win agent for Capture The Flag

![w=80%,h=center](ctf_curves.svgz)

