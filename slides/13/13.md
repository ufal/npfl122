title: NPFL122, Lecture 13
class: title, langtech, cc-by-nc-sa
# Multi-Agent RL, PPO, MAPPO

## Milan Straka

### January 02, 2023

---
section: MARL
# Multi-Agent Reinforcement Learning

We use the thesis
> _Cooperative Multi-Agent Reinforcement Learning_ https://dspace.cuni.cz/handle/20.500.11956/127431

as an introduction text.

---
section: NPG
# Natural Policy Gradient

The following approach has been introduced by Kakade (2002).

~~~
Using policy gradient theorem, we are able to compute $∇ v_π$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $→d$ which maximizes $v_π(s; →θ + →d)$ under
the constraint that $\|→d\|^2$ is bounded by a small constant is exactly
the gradient $∇ v_π$.

~~~
Normally, the length $|→d|^2$ is computed using Euclidean metric. But in general,
any metric could be used. Representing a metric using a positive-definite matrix
$⇉G$ (identity matrix for Euclidean metric), we can compute the distance as
$\|→d\|_{⇉G}^2 = ∑_{ij} G_{ij} d_i d_j = →d^T ⇉G →d$. The steepest ascent direction is
then given by $⇉G^{-1} ∇ v_π$.

~~~
Note that when $⇉G$ is the Hessian $⇉H v_π$, the above process is exactly
Newton's method.

---
# Natural Policy Gradient

![w=100%,v=middle](npg.svgz)

---
# Natural Policy Gradient

A suitable choice for the metric is _Fisher information matrix_,
which is defined as a _covariance matrix_ of the **score function**
$∇_{→θ} \log π(a|s; →θ)$.
~~~
The expectaton of the score function is 0:
$$𝔼_{π(a|s; →θ)} ∇_{→θ} \log π(a|s; →θ) = ∫ π(a|s; →θ) \frac{∇_{→θ} π(a|s; →θ)}{π(a|s; →θ)} \d a
  = ∇_{→θ} ∫ π(a|s; →θ) \d a = ∇_{→θ} 1 = 0.$$

~~~
The Fisher information matrix is therefore
$$F_s(→θ) ≝ 𝔼_{π(a | s; →θ)} \Big[\big(∇_{→θ} \log π(a|s; →θ)\big) \big(∇_{→θ} \log π(a|s; →θ)\big)^T \Big].$$

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

~~~
The Fisher information matrix is also a Hessian of the
$D_\textrm{KL}(π(a | s; →θ) \| π(a | s; →θ')$:
$$F_s(→θ) = \frac{∂^2}{∂θ_i' ∂θ_j'} D_\textrm{KL}\big(π(a | s; →θ) \| π(a | s; →θ')\big)\Big|_{→θ' = →θ}.$$

---
# Natural Policy Gradient

Using the metric
$$F(→θ) = 𝔼_{s ∼ μ_{→θ}} F_s(→θ)$$
we want to update the parameters using $→d_F ≝ F(→θ)^{-1} ∇ v_π$.

~~~
An interesting property of using the $→d_F$ to update the parameters is that
- updating $→θ$ using $∇ v_π$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $→θ$ using $F(→θ)^{-1} ∇ v_π$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $→d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$→d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $⇉A→x = →b$
in an iterative manner, by using $⇉A$ only to compute products $⇉A→v$ for
a suitable $→v$.

~~~
Therefore, $→d_F$ is found as a solution of
$$F(→θ)→d_F = ∇ v_π$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{α}{\sqrt{(∇ v_π)^T F(→θ)^{-1} ∇ v_π}}.$$

---
section: TRPO
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $π, π̃$, we can write
$$v_π̃ = v_π + 𝔼_{s ∼ μ(π̃)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s),$$
where $a_π(a | s)$ is the advantage function $q_π(a | s) - v_π(s)$ and
$μ(π̃)$ is the on-policy distribution of the policy $π̃$.

~~~
Analogously to policy improvement, we see that if $a_π(a | s) ≥0$, policy
$π̃$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s ∼ μ(π̃)$ is costly. Therefore, we instead
consider
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s).$$

---
# Trust Region Policy Optimization
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s)$$

It can be shown that for parametrized $π(a | s; →θ)$ the $L_π(π̃)$ matches
$v_{π̃}$ to the first order.

~~~
Schulman et al. additionally proves that if we denote
$α = D_\textrm{KL}^\textrm{max}(π_\textrm{old} \| π_\textrm{new})
   = \max_s D_\textrm{KL}\big(π_\textrm{old}(⋅|s) \| π_\textrm{new}(⋅|s)\big)$, then
$$v_{π_\textrm{new}} ≥ L_{π_\textrm{old}}(π_\textrm{new}) - \frac{4εγ}{(1-γ)^2}α\textrm{~~~where~~~}ε = \max_{s, a} |a_π(s, a)|.$$

~~~
Therefore, TRPO maximizes $L_{π_{→θ_0}}(π_{→θ})$ subject to
$D_\textrm{KL}^{→θ_0}(π_{→θ_0} \| π_{→θ}) < δ$, where
- $D_\textrm{KL}^{→θ_0}(π_{→θ_0} \| π_{→θ}) = 𝔼_{s ∼ μ(π_{→θ_0})} [D_\textrm{KL}\big(π_\textrm{old}(⋅|s) \| π_\textrm{new}(⋅|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $δ$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $π$.

---
# Trust Region Policy Optimization

$$\textrm{maximize}~~L_{π_{→θ_0}}(π_{→θ})
 = 𝔼_{s ∼ μ(π_{→θ_0}), a ∼ π_{→θ_0}(a | s)} \Big[\tfrac{π_{→θ}(a|s)}{π_{→θ_0}(a|s)}a_{π_{→θ_0}}(a | s)\Big]
 ~~\textrm{subject to}~~D_\textrm{KL}^{→θ_0}(π_{→θ_0} \| π_{→θ}) < δ$$

The parameters are updated using $→d_F = F(→θ)^{-1} ∇ L_{π_{→θ_0}}(π_{→θ})$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{δ/(→d_F^T F(→θ)^{-1} →d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
# Proximal Policy Optimization

A simplification of TRPO which can be implemented using a few lines of code.

Let $r_t(→θ) ≝ \frac{π(A_t|S_t; →θ)}{π(A_t|S_t; →θ_\textrm{old})}$. PPO
maximizes the objective (i.e., you should minimize its negation)
$$L^\textrm{CLIP}(→θ) ≝ 𝔼_t\Big[\min\big(r_t(→θ) Â_t, \operatorname{clip}(r_t(→θ), 1-ε, 1+ε) Â_t)\big)\Big].$$

Such a $L^\textrm{CLIP}(→θ)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $Â_t$ are additionally estimated using the so-called
_generalized advantage estimation_, which is just an analogue
of the truncated n-step lambda-return:
$$Â_t = ∑_{i=0}^{n-1} γ^i λ^i \big(R_{t+1+i} + γ V(S_{t+i+1}) - V(S_{t + i})\big).$$

~~~
![w=80%,h=center](ppo_algorithm.svgz)

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)

---
section: HideAndSeek
# Multi-Agent Hide-and-Seek

As another example, consider https://openai.com/blog/emergent-tool-use/.
