title: NPFL122, Lecture 7
class: title, langtech, cc-by-sa
# Continuous Actions, DDPG, TD3

## Milan Straka

### November 14, 2022

---
section: PAAC
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workers to only generate episodes. Such approach
was described in May 2017 by [Clemente et al.](https://arxiv.org/abs/1705.04862),
who named their agent **parallel advantage actor-critic** (PAAC).

![w=70%,h=center](paac_framework.svgz)

---
# Parallel Advantage Actor Critic

![w=85%,h=center](paac_algorithm.svgz)

---
# Parallel Advantage Actor Critic

![w=70%,h=center](paac_performance.svgz)

The authors use $8$ workers, $n_e=32$ parallel environments, $5$-step returns,
$γ=0.99$, $ε=0.1$, $β=0.01$, and a learning rate of $α=0.0007⋅n_e=0.0224$.

The $\textrm{arch}_\textrm{nips}$ is from A3C: 16 filters $8×8$ stride 4, 32
filters $4×4$ stride 2, a dense layer with 256 units. The
$\textrm{arch}_\textrm{nature}$ is from DQN: 32 filters $8×8$ stride 4, 64
filters $4×4$ stride 2, 64 filters $3×3$ stride 1 and 512-unit fully connected
layer. All nonlinearities are ReLU.

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_epochs.svgz)

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_time.svgz)

---
# Parallel Advantage Actor Critic

![w=100%,v=middle](paac_time_usage.svgz)

---
section: Continuous Action Space
# Continuous Action Space

Until now, the actions were discrete. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product
of several such ranges:
$$∏_i [a_i, b_i].$$

~~~
![w=40%,f=right](normal_distribution.svgz)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$
is
$$p(x) ≝ \frac{1}{\sqrt{2 π σ^2}} e^{\large-\frac{(x - μ)^2}{2σ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution, we suitably parametrize the action value,
usually using the normal distribution.

~~~
Considering only one real-valued action, we therefore have
$$π(a | s; →θ) ≝ P\Big(a ∼ 𝓝\big(μ(s; →θ), σ(s; →θ)^2\big)\Big),$$
where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard
deviation of the action distribution.

~~~
The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a common regression (i.e., one output neuron
  without activation);
~~~
- the standard deviation (which must be positive) being computed again as
  a single neuron, but with either $\exp$ or $\operatorname{softplus}$, where
  $\operatorname{softplus}(x) ≝ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

~~~
```python
  mus = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.math.exp(sds)   # or sds = tf.math.softplus(sds)

  action_dist = tfp.distributions.Normal(mus, sds)

  # Loss computed as - log π(a|s) * returns - entropy_regularization
  loss = - action_dist.log_prob(actions) * returns \
         - args.entropy_regularization * action_dist.entropy()
```

---
# Continuous Action Space

When the action consists of several real values, i.e., action is a suitable
subregion of $ℝ^n$ for $n>1$, we can:
- either use multivariate Gaussian distribution;
- or factorize the probability into a product of univariate normal
  distributions.

~~~
Modeling the action distribution using a single normal distribution might be
insufficient, in which case a mixture of normal distributions is usually used.

~~~
Sometimes, the continuous action space is used even for discrete output – when
modeling pixels intensities (256 values) or sound amplitude (2$^{16}$ values),
instead of a softmax we use discretized mixture of distributions,
usually $\operatorname{logistic}$ (a distribution with a sigmoid CDF). Then,
$$π(a) = ∑_i p_i\Big(σ\big((a + 0.5 - μ_i) / σ_i\big) - σ\big((a - 0.5 - μ_i) / σ_i\big) \Big).$$
However, such mixtures are usually used in generative modeling, not in
reinforcement learning.

---
section: DPG
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$∇_{→θ} J(→θ) ∝ 𝔼_{s ∼ μ} \Big[∑\nolimits_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ)\Big].$$

~~~
## Deterministic Policy Gradient Theorem
Assume that the policy $π(s; →θ)$ is deterministic and computes
an action $a∈ℝ$. Further, assume the reward $r(s, a)$ is actually
a deterministic function of the given state-action pair.
Then, under several assumptions about continuousness, the following holds:
$$∇_{→θ} J(→θ) ∝ 𝔼_{s ∼ μ} \Big[∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al in 2014.

---
# Deterministic Policy Gradient Theorem – Proof

The proof is very similar to the original (stochastic) policy gradient theorem.

~~~
However, we will be exchanging derivatives and integrals, for which we need
several assumptions:
~~~
- we assume that $h(s), p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a r(s, a),
  π(s; →θ), ∇_{→θ} π(s; →θ)$ are continuous in all parameters and variables;
~~~
- we further assume that $h(s), p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a
  r(s, a)$ are bounded.

Details (which assumptions are required and when) can be found in Appendix B
of the paper _Deterministic Policy Gradient Algorithms: Supplementary Material_ by
David Silver et al.

---
# Deterministic Policy Gradient Theorem – Proof

$\displaystyle ∇_{→θ} v_π(s) = ∇_{→θ} q_π(s, π(s; →θ))$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ}\Big(r\big(s, π(s; →θ)\big) + ∫_{s'} γp\big(s'| s, π(s; →θ)\big)\big(v_π(s')\big) \d s'\Big)$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a r(s, a) \big|_{a=π(s; →θ)} + ∇_{→θ} ∫_{s'} γp\big(s' | s, π(s; →θ)\big) v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a \Big(r(s, a) + ∫_{s'} γp\big(s' | s, a\big) v_π(s') \d s' \Big) \Big|_{a=π(s; →θ)}\\
                    \qquad\qquad\qquad + ∫_{s'} γp\big(s' | s, π(s; →θ)\big) ∇_{→θ} v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s; →θ)} + ∫_{s'} γp\big(s' | s, π(s; →θ)\big) ∇_{→θ} v_π(s') \d s'$

~~~
We finish the proof as in the gradient theorem by continually expanding $∇_{→θ} v_π(s')$, getting
$∇_{→θ} v_π(s) = ∫_{s'} ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) \big[∇_{→θ} π(s'; →θ) ∇_a q_π(s', a)\big|_{a=π(s';→θ)}\big] \d s'$
~~~
and then $∇_{→θ} J(→θ) = 𝔼_{s ∼ h} ∇_{→θ} v_π(s) ∝ 𝔼_{s ∼ μ} \big[∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}\big]$.

---
section: DDPG
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
(similarly to how expected Sarsa is also an off-policy algorithm).

~~~
We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,
training $q(s, a; →θ)$ using a deterministic variant of the Bellman equation:
$$q(S_t, A_t; →θ) = 𝔼_{S_{t+1}} \big[r(S_t, A_t) + γ q(S_{t+1}, π(S_{t+1}; →θ))\big]$$
and $π(s; →θ)$ according to the deterministic policy gradient theorem.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

~~~
The authors utilize a replay buffer, a target network (updated by exponential
moving average with $τ=0.001$), batch normalization for CNNs, and perform
exploration by adding a Ornstein-Uhlenbeck noise to the predicted actions.
Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the
policy and critic network, respectively.

---
# Deep Deterministic Policy Gradients

![w=65%,h=center](ddpg.svgz)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.svgz)

---
# Deep Deterministic Policy Gradients

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

![w=57%,h=center](ddpg_results.svgz)

---
section: OrnsteinUhlenbeck
# Ornstein-Uhlenbeck Exploration

While the exploration policy could just use Gaussian noise, the authors claim
that temporarily-correlated noise is more effective for physical control
problems with inertia.

~~~
They therefore generate noise using Ornstein-Uhlenbeck process, by computing
$$n_t ← n_{t-1} + θ ⋅ (μ - n_{t-1}) + ε∼𝓝(0, σ^2),$$
utilizing hyperparameter values $θ=0.15$ and $σ=0.2$.

---
section: MuJoCo
# MuJoCo

![w=100%,v=middle](mujoco_environments.svgz)

---
section: TD3
# Twin Delayed Deep Deterministic Policy Gradient

The paper Addressing Function Approximation Error in Actor-Critic Methods by
Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which

~~~
- decrease maximization bias by training two critics and choosing the minimum of
  their predictions;

~~~
- introduce several variance-lowering optimizations:
  - delayed policy updates;
  - target policy smoothing.

~~~

The TD3 algorithm has been together with SAC one of the best
algorithms for off-policy continuous-actions RL training (as of 2022).

---
# TD3 – Maximization Bias

Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.
In Q-learning, the maximization bias was caused by the explicit $\max$ operator.
For DDPG methods, it can be caused by the gradient descent itself. Let
$→θ_\textit{approx}$ be the parameters maximizing the $q_{→θ}$ and let
$→θ_\textit{true}$ be the hypothetical parameters which maximise true $q_π$,
and let $π_\textit{approx}$ and $π_\textit{true}$ denote the corresponding
policies.

~~~
Because the gradient direction is a local maximizer, for sufficiently small
$α<ε_1$ we have
$$𝔼\big[q_{→θ}(s, π_\textit{approx})\big] ≥ 𝔼\big[q_{→θ}(s, π_\textit{true})\big].$$

~~~
However, for real $q_π$ and for sufficiently small $α<ε_2$, it holds that
$$𝔼\big[q_π(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

~~~
Therefore, if $𝔼\big[q_{→θ}(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{true})\big]$,
for $α < \min(ε_1, ε_2)$
$$𝔼\big[q_{→θ}(s, π_\textit{approx})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

---
# TD3 – Maximization Bias

![w=50%](td3_bias.svgz)![w=50%](td3_bias_dqac.svgz)

~~~
Analogously to Double DQN we could compute the learning targets using
the current policy and the target critic, i.e., $r + γ q_{→θ'}(s', π_{→φ(s')})$
(instead of using the target policy and the target critic as in DDPG), obtaining DDQN-AC algorithm.
However, the authors found out that the policy changes too slowly and the target
and current networks are too similar.

~~~
Using the original Double Q-learning, two pairs of actors and critics could be
used, with the learning targets computed by the opposite critic, i.e.,
$r + γ q_{→θ_2}(s', π_{→φ_1}(s'))$ for updating $q_{→θ_1}$. The resulting DQ-AC
algorithm is slightly better, but still suffering from overestimation.

---
# TD3 – Algorithm

The authors instead suggest to employ two critics and one actor. The actor is
trained using one of the critics, and both critics are trained using the same
target computed using the _minimum_ value of both critics as
$$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s')).$$
The resulting algorithm is called CDQ – Clipped Double Q-learning.

~~~
Furthermore, the authors suggest two additional improvements for variance
reduction.
- For obtaining higher quality target values, the authors propose to train the
  critics more often. Therefore, critics are updated each step, but the actor
  and the target networks are updated only every $d$-th step ($d=2$ is used in
  the paper).

~~~
- To explicitly model that similar actions should lead to similar results,
  a small random noise is added to the performed actions when computing the
  target value:
  $$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s') + ε)~~~\textrm{for}~~~
    ε ∼ \operatorname{clip}(𝓝(0, σ), -c, c).$$

---
# TD3 – Algorithm

![w=43%,h=center](td3_algorithm.svgz)

---
# TD3 – Algorithm

![w=80%,h=center](td3_hyperparameters.svgz)

---
# TD3 – Results

![w=70%,h=center](td3_results_curves.svgz)
![w=70%,h=center](td3_results.svgz)

---
# TD3 – Ablations

![w=85%,h=center](td3_ablations.svgz)
![w=85%,h=center](td3_ablations_dqac.svgz)

The AHE is the authors' reimplementation of DDPG using updated architecture,
hyperparameters, and exploration. TPS is Target Policy Smoothing, DP is Delayed
Policy update, and CDQ is Clipped Double Q-learning.

---
# TD3 – Ablations

![w=65%,h=center](td3_ablations_results.svgz)
