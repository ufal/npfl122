title: NPFL122, Lecture 7
class: title, langtech, cc-by-nc-sa
# Policy Gradient Methods

## Milan Straka

### November 26, 2018

---
section: Policy Gradient Methods
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$Ï€(a | s; â†’Î¸).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $Ï€$ instead of just $Îµ$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_Ï€(s)$ and to
that account we need to compute its _gradient_ $âˆ‡_â†’Î¸ v_Ï€(s)$.

---
# Policy Gradient Methods

In addition to discarding $Îµ$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# Policy Gradient Theorem

Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.
Let also $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$.

~~~
Then
$$âˆ‡_â†’Î¸ v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸)$$
and
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$

~~~
where $P(s â†’ â€¦ â†’ s'|Ï€)$ is probability of transitioning from state $s$ to $s'$
using 0, 1, â€¦ steps.



---
# Proof of Policy Gradient Theorem

$\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \Big[ âˆ‘\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ \big(âˆ‘\nolimits_{s'} p(s'|s, a)(r + v_Ï€(s'))\big) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \big(âˆ‘\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\big) \Big]$

~~~
_We now expand $v_Ï€(s')$._

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \Big(âˆ‘\nolimits_{s'} p(s'|s, a)\Big(\\
                \qquad\qquad\qquad âˆ‘\nolimits_{a'} \Big[ âˆ‡ Ï€(a'|s'; â†’Î¸) q_Ï€(s', a') + Ï€(a'|s'; â†’Î¸) \Big(âˆ‘\nolimits_{s''} p(s''|s', a') âˆ‡ v_Ï€(s'')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_Ï€(s'')$, we obtain the following:_

$\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸).$

---
# Proof of Policy Gradient Theorem

Recall that the initial state distribution is $h(s)$ and the on-policy
distribution under $Ï€$ is $Î¼(s)$. If we let $Î·(s)$ denote the number
of time steps spent, on average, in state $s$ in a single episode,
we have
$$Î·(s) = h(s) + âˆ‘_{s'}Î·(s') âˆ‘_a Ï€(a|s') p(s|s',a).$$

~~~
The on-policy distribution is then the normalization of $Î·(s)$:
$$Î¼(s) â‰ \frac{Î·(s)}{âˆ‘_{s'} Î·(s')}.$$

~~~
The last part of the policy gradient theorem follows from the fact that $Î¼(s)$ is
$$Î¼(s) = ğ”¼_{s_0 âˆ¼ h(s)} P(s_0 â†’ â€¦ â†’ s | Ï€).$$

---
section: REINFORCE
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$. The loss is defined as
$$\begin{aligned}
  -âˆ‡_â†’Î¸ J(â†’Î¸) &âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) \\
              &= ğ”¼_{s âˆ¼ Î¼} âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).
\end{aligned}$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$-âˆ‡_â†’Î¸ J(â†’Î¸) âˆ ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸),$$
where we used the fact that
$$âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸) = \frac{1}{Ï€(a | s; â†’Î¸)} âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

To compute the gradient
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$-ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸),$$
estimating the $q_Ï€(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.pdf)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$âˆ‘_a b(s) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‘_a âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‡1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_Ï€(s, a) - v_Ï€(s)$ function is also called an _advantage function_
$$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s).$$

~~~
Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.pdf)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.pdf)

---
section: Actor-Critic
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called _actor-critic_ methods.

~~~
The idea is straightforward â€“ instead of estimating the episode return using the
whole episode rewards, we can use $n$-step temporal difference estimation.

---
# Actor-Critic

![w=85%,h=center](actor_critic.pdf)
