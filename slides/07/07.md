title: NPFL122, Lecture 7
class: title, langtech, cc-by-sa
# Policy Gradient Methods

## Milan Straka

### November 26, 2018

---
section: Policy Gradient Methods
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.

~~~
Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is probability of transitioning from state $s$ to $s'$
using 0, 1, … steps.



---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \qquad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \Big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

Recall that the initial state distribution is $h(s)$ and the on-policy
distribution under $π$ is $μ(s)$. If we let $η(s)$ denote the number
of time steps spent, on average, in state $s$ in a single episode,
we have
$$η(s) = h(s) + ∑_{s'}η(s') ∑_a π(a|s') p(s|s',a).$$

~~~
The on-policy distribution is then the normalization of $η(s)$:
$$μ(s) ≝ \frac{η(s)}{∑_{s'} η(s')}.$$

~~~
The last part of the policy gradient theorem follows from the fact that $μ(s)$ is
$$μ(s) = 𝔼_{s_0 ∼ h(s)} P(s_0 → … → s | π).$$

---
section: REINFORCE
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. The loss is defined as
$$\begin{aligned}
  -∇_→θ J(→θ) &∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) \\
              &= 𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).
\end{aligned}$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$-∇_→θ J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \ln π(a | s; →θ),$$
where we used the fact that
$$∇_→θ \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$

To compute the gradient
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$-𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.pdf)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called an _advantage function_
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.pdf)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.pdf)

---
section: Actor-Critic
# Actor-Critic

It is possible to combine the policy gradient methods and temporal difference
methods, creating a family of algorithms usually called _actor-critic_ methods.

~~~
The idea is straightforward – instead of estimating the episode return using the
whole episode rewards, we can use $n$-step temporal difference estimation.

---
# Actor-Critic

![w=85%,h=center](actor_critic.pdf)
