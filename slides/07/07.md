title: NPFL122, Lecture 7
class: title, langtech, cc-by-nc-sa
# PAAC, Continuous Actions, DDPG

## Milan Straka

### November 16, 2020

---
section: PAAC
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workes to only generate episodes. Such approach
was described in May 2017 by Clemente et al., who named their agent
_parallel advantage actor-critic_ (PAAC).

![w=70%,h=center](paac_framework.svgz)

---
# Parallel Advantage Actor Critic

![w=85%,h=center](paac_algorithm.svgz)

---
# Parallel Advantage Actor Critic

![w=70%,h=center](paac_performance.svgz)

The authors use $8$ workers, $n_e=32$ parallel environments, $5$-step returns,
$Î³=0.99$, $Îµ=0.1$, $Î²=0.01$ and a learning rate of $Î±=0.0007â‹…n_e=0.0224$.

The $\textrm{arch}_\textrm{nips}$ is from A3C: 16 filters $8Ã—8$ stride 4, 32
filters $4Ã—4$ stride 2, a dense layer with 256 units. The
$\textrm{arch}_\textrm{nature}$ is from DQN: 32 filters $8Ã—8$ stride 4, 64
filters $4Ã—4$ stride 2, 64 filters $3Ã—3$ stride 1 and 512-unit fully connected
layer. All nonlinearities are ReLU.

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_epochs.svgz)

---
# Parallel Advantage Actor Critic

![w=100%](paac_workers_time.svgz)

---
# Parallel Advantage Actor Critic

![w=100%,v=middle](paac_time_usage.svgz)


---
section: Continuous Action Space
# Continuous Action Space

Until now, the actions were discrete. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b âˆˆ â„$, or more generally from a Cartesian product
of several such ranges:
$$âˆ_i [a_i, b_i].$$

~~~
![w=40%,f=right](normal_distribution.svgz)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $Î¼$ and variance $Ïƒ^2$, probability density function of $ğ“(Î¼, Ïƒ^2)$
is
$$p(x) â‰ \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} e^{\large-\frac{(x - Î¼)^2}{2Ïƒ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution we suitably parametrize the action value,
usually using the normal distribution. Considering only one real-valued action,
we therefore have
$$Ï€(a | s; â†’Î¸) â‰ P\Big(a âˆ¼ ğ“\big(Î¼(s; â†’Î¸), Ïƒ(s; â†’Î¸)^2\big)\Big),$$
where $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ are function approximation of mean and standard
deviation of the action distribution.

The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a regular regression (i.e., one output neuron
  without activation);
- the standard deviation (which must be positive) being computed again as
  a single neuron, but with either $\exp$ or $\operatorname{softplus}$, where
  $\operatorname{softplus}(x) â‰ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $Î¼(s; â†’Î¸)$ and $Ïƒ(s; â†’Î¸)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

~~~
```python
  mus = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.keras.layers.Dense(actions)(hidden_layer)
  sds = tf.math.exp(sds)   # or sds = tf.math.softplus(sds)

  action_dist = tfp.distributions.Normal(mus, sds)

  # Loss computed as - log Ï€(a|s) - entropy_regularization
  loss = - action_dist.log_prob(actions) * returns \
         - args.entropy_regularization * action_dist.entropy()
```

---
# Continuous Action Space

When the action consists of several real values, i.e., action is a suitable
subregion of $â„^n$ for $n>1$, we can:
- either use multivariate Gaussian distribution;
- or factorize the probability into a product of univariate normal
  distributions.

~~~
Modeling the action distribution using a single normal distribution might be
insufficient, in which case a mixture of normal distributions is usually used.

~~~
Sometimes, the continuous action space is used even for discrete output -- when
modeling pixels intensities (256 values) or sound amplitude (2$^{16}$ values),
instead of a softmax we use discretized mixture of distributions,
usually $\operatorname{logistic}$ (a distribution with a sigmoid cdf). Then,
$$Ï€(a) = âˆ‘_i p_i\Big(Ïƒ\big((a + 0.5 - Î¼_i) / Ïƒ_i\big) - Ïƒ\big((a - 0.5 - Î¼_i) / Ïƒ_i\big) \Big).$$
However, such mixtures are usually used in generative modeling, not in
reinforcement learning.

---
section: DPG
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

~~~
## Deterministic Policy Gradient Theorem
Assume that the policy $Ï€(s; â†’Î¸)$ is deterministic and computes
an action $aâˆˆâ„$. Further, assume the reward $r(s, a)$ is actually
a deterministic function of the given state-action pair.
Then, under several assumptions about continuousness, the following holds:
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ ğ”¼_{sâˆ¼Î¼(s)} \Big[âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s;â†’Î¸)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al in 2014.

---
# Deterministic Policy Gradient Theorem â€“ Proof

The proof is very similar to the original (stochastic) policy gradient theorem.

~~~
However, we will be exchanging derivatives and integrals, for which we need
several assumptions:
~~~
- we assume that $h(s), p(s' | s, a), âˆ‡_a p(s' | s, a), r(s, a), âˆ‡_a r(s, a),
  Ï€(s; â†’Î¸), âˆ‡_â†’Î¸ Ï€(s; â†’Î¸)$ are continuous in all parameters and variables;
~~~
- we further assume that $h(s), p(s' | s, a), âˆ‡_a p(s' | s, a), r(s, a), âˆ‡_a
  r(s, a)$ are bounded.

Details about which assumptions are required when can be found in Appendix B
of _Deterministic Policy Gradient Algorithms: Supplementary Material_ by
David Silver et al.

---
# Deterministic Policy Gradient Theorem â€“ Proof

$\displaystyle âˆ‡_â†’Î¸ v_Ï€(s) = âˆ‡_â†’Î¸ q_Ï€(s, Ï€(s; â†’Î¸))$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸\Big(r\big(s, Ï€(s; â†’Î¸)\big) + âˆ«_{s'} p\big(s' | s, Ï€(s; â†’Î¸)\big) Î³v_Ï€(s') \d s'\Big)$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a r(s, a) \big|_{a=Ï€(s; â†’Î¸)} + âˆ‡_â†’Î¸ âˆ«_{s'} Î³p\big(s' | s, Ï€(s; â†’Î¸)\big) v_Ï€(s') \d s'$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a \Big(r(s, a) + âˆ«_{s'} Î³p\big(s' | s, a\big) v_Ï€(s') \d s' \Big) \Big|_{a=Ï€(s; â†’Î¸)}\\
                    \qquad\qquad\qquad + âˆ«_{s'} Î³p\big(s' | s, Ï€(s; â†’Î¸)\big) âˆ‡_â†’Î¸ v_Ï€(s') \d s'$

~~~
$\displaystyle \phantom{âˆ‡_â†’Î¸ v_Ï€(s)} = âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s; â†’Î¸)} + âˆ«_{s'} Î³p\big(s' | s, Ï€(s; â†’Î¸)\big) âˆ‡_â†’Î¸ v_Ï€(s') \d s'$

~~~
We finish the proof as in the gradient theorem by continually expanding $âˆ‡_â†’Î¸ v_Ï€(s')$, getting
$âˆ‡_â†’Î¸ v_Ï€(s) = âˆ«_{s'} âˆ‘_{k=0}^âˆ Î³^k P(s â†’ s'\textrm{~in~}k\textrm{~steps~}|Ï€) \big[âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s;â†’Î¸)}\big] \d s'$
~~~
and then $âˆ‡_â†’Î¸ J(â†’Î¸) = ğ”¼_{s âˆ¼ h} âˆ‡_â†’Î¸ v_Ï€(s) âˆ ğ”¼_{sâˆ¼Î¼(s)} \big[âˆ‡_â†’Î¸ Ï€(s; â†’Î¸) âˆ‡_a q_Ï€(s, a)\big|_{a=Ï€(s;â†’Î¸)}\big]$.

---
section: DDPG
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
(similarly to how expected Sarsa is also an off-policy algorithm).

~~~
We therefore train function approximation for both $Ï€(s; â†’Î¸)$ and $q(s, a; â†’Î¸)$,
training $q(s, a; â†’Î¸)$ using a deterministic variant of the Bellman equation:
$$q(S_t, A_t; â†’Î¸) = ğ”¼_{R_{t+1}, S_{t+1}} \big[R_{t+1} + Î³ q(S_{t+1}, Ï€(S_{t+1}; â†’Î¸))\big]$$
and $Ï€(s; â†’Î¸)$ according to the deterministic policy gradient theorem.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

The authors utilize a replay buffer, a target network (updated by exponential
moving average with $Ï„=0.001$), batch normalization for CNNs, and perform
exploration by adding a Ornstein-Uhlenbeck noise to predicted actions.
Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the
policy and critic network, respectively.

---
# Deep Deterministic Policy Gradients

![w=65%,h=center](ddpg.svgz)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.svgz)

---
# Deep Deterministic Policy Gradients

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

![w=57%,h=center](ddpg_results.svgz)

---
section: OrnsteinUhlenbeck
# Ornstein-Uhlenbeck Exploration

While the exploration policy could just use Gaussian noise, the authors claim
that temporarily-correlated noise is more effective for physical control
problems with innertia.

~~~
They therefore generate noise using Ornstein-Uhlenbeck process, by computing
$$n_t â† n_{t-1} + Î¸ â‹… (Î¼ - n_{t-1}) + Îµâˆ¼ğ“(0, Ïƒ^2),$$
utilizing hyperparameter values $Ï„=0.15$ and $Ïƒ=0.2$.
