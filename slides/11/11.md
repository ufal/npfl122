title: NPFL122, Lecture 11
class: title, langtech, cc-by-sa
# PopArt Normalization, R2D2, MuZero

## Milan Straka

### December 14, 2020

---
section: PopArt Normalization
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; →θ, σ, μ)$ is computed using a normalized value
predictor $n(s; →θ)$
$$v(s; →θ, σ, μ) ≝ σ n(s; →θ) + μ$$
and further assume that $n(s; →θ)$ is an output of a linear function
$$n(s; →θ) ≝ →ω^T f(s; →θ-\{→ω, b\}) + b.$$

~~~
We can update the $σ$ and $μ$ using exponentially moving average with decay rate
$β$ (in the paper, first moment $μ$ and second moment $υ$ is tracked, and
the standard deviation is computed as $σ=\sqrt{υ-μ^2}$; decay rate $β=3 ⋅ 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $μ$ and $σ$, we can normalize the observed (unnormalized) returns as
$(G - μ) / σ$ and use an actor-critic algorithm with advantage $(G - μ)/σ - n(S; →θ)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $→ω, b$ used to compute the
value estimate
$$v(s; →θ, σ, μ) ≝ σ \Big(→ω^T f(s; →θ-\{→ω, b\}) + b\Big) + μ$$
are updated under any change $μ → μ'$ and $σ → σ'$ as
$$\begin{aligned}
  →ω' &← \frac{σ}{σ'}→ω,\\
  b' &← \frac{σb + μ - μ'}{σ'}.
\end{aligned}$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $→μ$, $→σ$ and $→n(s; →θ)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.svgz)

~~~
![w=100%](popart_atari_curves.svgz)

---
# PopArt Results

![w=85%,h=center](popart_atari_statistics.svgz)

Normalization statistics on chosen environments.

---
# PopArt Results

![w=100%,v=middle](popart_dmlab_curves.svgz)

---
section: TransformedRewards
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $𝓣$
$$(𝓣q)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[r + γ \max_{a'} q(s', a')\Big].$$

~~~
Instead of reducing the magnitude of rewards, we might use a function
$h: ℝ → ℝ$ to reduce their scale. We define a transformed Bellman operator
$𝓣_h$ as
$$(𝓣_hq)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[h\Big(r + γ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = α z$ for $α > 0$, then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_* = α q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $α$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ 𝓣 q_* = h \circ 𝓣(h^{-1} \circ h \circ q_*) = 𝓣_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

The authors use the following transformation for the Atari environments
$$h(x) ≝ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + εx$$
with $ε = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4ε (|x| + 1 + ε)} - 1}{2ε} \right)^2 - 1\right).$$

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2_recurrent_staleness.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $γ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2_memory_size.svgz)

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics.

~~~
At each time-step $t$, for each of $1 ≤ k ≤ K$ steps, a model $μ_θ$, with parameters $θ$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$, predicts three future quantities:
- the policy $→p^k_t ≈ π(a_{t+k+1} | o_1, …, o_t, a_{t+1}, …, a_{t+k})$,
~~~
- the value function $v^k_t ≈ 𝔼\big[u_{t+k+1} + γ u_{t+k+2} + … | o_1, …, o_t, a_{t+1}, …, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t ≈ u_{t+k}$,

where $u_i$ are the observed rewards and $π$ is the behaviour policy.

---
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components,
a _representation_ function, a _dynamics_ function and a _prediction_ function.

~~~
- The dynamics function, $r^k, s^k = g_θ(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values and policies.

~~~
- The prediction function $→p^k, v^k = f_θ(s^k)$, computes the policy and value
  function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 = h_θ(o_1, …, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
# MuZero

To select a move, we employ a MCTS algorithm similar to the AlphaZero. It
produces a policy $π_t$ and value estimate $ν_t$, and an action is then
sampled from the policy $a_{t+1} ∼ π_t$.

~~~
During training, we utilize a sequence of $k$ moves. We estimate the return
using bootstrapping $z_t = u_{t+1} + γ u_{t+2} + … + γ^{n-1} u_{t+n} + γ^n ν_{t+n}$.
The values $k=5$ and $n=10$ are used in the paper.

~~~
The loss is then composed of the following components:
$$𝓛_t(θ) = ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(π_{t+k}, →p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + εx$ for $ε=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, …, 300$, which they claim
to be more stable.

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left.
\begin{array}{ll}
s^0 &= h_\theta(o_1, ..., o_t) \\
r^k, s^k &= g_\theta(s^{k-1}, a^k) \\
%
\mathbf{p}^k, v^k &= f_\theta(s^k)
\end{array}
\right\} \;\;
\mathbf{p}^k, v^k, r^k = \mu_\theta(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
\nu_t, \pi_t &= MCTS(s^0_t, \mu_\theta) \\
a_t &\sim \pi_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
\mathbf{p}^k_t, v^k_t, r^k_t &= \mu_\theta(o_1, …, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{
\begin{array}{lr}
u_T
& \text{ for games } \\
u_{t+1} + \gamma u_{t+2} + ... + \gamma^{n-1} u_{t+n} + \gamma^n \nu_{t+n}
& \text{ for general MDPs }
\end{array}
\right. \\
𝓛_t(\theta) &= \sum_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(\pi_{t+k}, →p^k_t)  + c \|\theta\|^2 \\
&\text{Losses} \\
𝓛^r(u, r) &= \left\{
\begin{array}{lr}
0 & \text{ for games } \\
\boldsymbol{\phi}(u)^T \log \mathbf{r} & \text{ for general MDPs }
\end{array}
\right. \\
𝓛^v(z, q) &= \left\{
\begin{array}{lr}
(z - q)^2 & \text{ for games } \\
\boldsymbol{\phi}(z)^T \log \mathbf{q} & \text{ for general MDPs }
\end{array}
\right. \\
𝓛^p(\pi, p) &= \boldsymbol{\pi}^T \log \mathbf{p}
\end{aligned}$$

---
# MuZero – Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero – Atari Results

![w=100%](muzero_atari.svgz)

---
# MuZero – Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero – Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)
