title: NPFL122, Lecture 11
class: title, langtech, cc-by-sa
# MuZero, PlaNet

## Milan Straka

### December 12, 2022

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics. It is evaluated both on board games
and on the Atari domain.

~~~
At each time-step $t$, for each of $1 ≤ k ≤ K$ steps, a model $μ_θ$ with parameters $θ$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$, predicts three future quantities:
- the policy $→p^k_t ≈ π(a_{t+k+1} | o_1, …, o_t, a_{t+1}, …, a_{t+k})$,
~~~
- the value function $v^k_t ≈ 𝔼\big[u_{t+k+1} + γ u_{t+k+2} + … | o_1, …, o_t, a_{t+1}, …, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t ≈ u_{t+k}$,

where $u_i$ are the observed rewards and $π$ is the behaviour policy.

---
section: μ0Model
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components:
a **representation** function, a **dynamics** function, and a **prediction** function.

~~~
- The dynamics function, $(r^k, s^k) ← g_θ(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values, and policies.

~~~
- The prediction function, $(→p^k, v^k) ← f_θ(s^k)$, computes the policy and the
  value function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 ← h_θ(o_1, …, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
section: μ0MCTS
# MuZero – MCTS

The MCTS algorithm is very similar to the one used in AlphaZero, only the
trained model is used. It produces a policy $→π_t$ and a value estimate $ν_t$.

~~~
- All actions, including the invalid ones, are allowed at any time, except at
  the root, where the invalid actions (available from the current state) are
  disallowed.

~~~
- No states are considered terminal during the search.

~~~
- During the backup phase, we consider a general discounted bootstrapped return
  $$G_k = ∑\nolimits_{t=0}^{l-k-1} γ^t r_{k+1+t} + γ^{l-k} v_l.$$

~~~
- Furthermore, the expected return is generally unbounded. Therefore, MuZero
  normalize the Q-value estimates to $[0, 1]$ range by using the minimum and
  maximum the values observed in the search tree until now:
  $$Q̄(s, a) = \frac{Q(s, a) - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}{\max_{s',a' ∈ \mathrm{Tree}} Q(s', a') - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}.$$

---
# MuZero – Action Selection

To select a move, we employ a MCTS algorithm and then sample
an action from the obtained policy, $a_{t+1} ∼ →π_t$.

~~~
For games, the same strategy of sampling the actions $a_t$ as in AlphaZero is used.
~~~
In the Atari domain, the actions are sampled according to the visit counts for the
whole episode, but with a given temperature $T$:
$$π(a|s) = \frac{N(s, a)^{1/T}}{∑_b N(s, b)^{1/T}},$$
where $T$ is decayed during training – for first 500k steps it is 1, for the
next 250k steps it is 0.5 and for the last 250k steps it is 0.25.

~~~
While for the board games 800 simulations are used during MCTS, only 50 are used
for Atari.

~~~
In case of Atari, the replay buffer consists of 125k sequences of 200 actions.

---
section: μ0Training
# MuZero – Training

During training, we utilize a sequence of $K$ moves. We estimate the return
using bootstrapping as $z_t = u_{t+1} + γ u_{t+2} + … + γ^{n-1} u_{t+n} + γ^n ν_{t+n}$.
The values $K=5$ and $n=10$ are used in the paper, with batch size 2048 for the
board games and 1024 for Atari.

~~~
The loss is then composed of the following components:
$$𝓛_t(θ) = ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(→π_{t+k}, →p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + εx$ for $ε=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, …, 300$, which they claim
to be more stable.

~~~
Furthermore, in Atari the discount factor $γ=0.997$ is used, and the replay buffer elements
are sampled according to prioritized replay with priority $∝ |ν-z|^α$, and importance sampling
with exponent $β$ is used to account for changing the sampling distribution
($α=β=1$ is used).

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left. \begin{array}{ll}
  s^0 &= h_θ(o_1, ..., o_t) \\
  r^k, s^k &= g_θ(s^{k-1}, a^k) \\
  →p^k, v^k &= f_θ(s^k)
\end{array} \right\} \;\; →p^k, v^k, r^k = μ_θ(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
ν_t, →π_t &= MCTS(s^0_t, μ_θ) \\
a_t &∼ →π_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
→p^k_t, v^k_t, r^k_t &= μ_θ(o_1, …, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{\begin{array}{lr}
  u_T & \text{ for games } \\
  u_{t+1} + γ u_{t+2} + ... + γ^{n-1} u_{t+n} + γ^n ν_{t+n} & \text{ for general MDPs }
\end{array}\right. \\
𝓛_t(θ) &= ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(→π_{t+k}, →p^k_t)  + c \|θ\|^2 \\

&\text{Losses} \\
𝓛^r(u, r) &= \left\{ \begin{array}{lr} 0 & \text{ for games } \\ -→φ(u)^T \log →φ(r) & \text{ for general MDPs } \end{array} \right. \\
𝓛^v(z, q) &= \left\{ \begin{array}{lr} (z - q)^2 & \text{ for games } \\ -→φ(z)^T \log →φ(q) & \text{ for general MDPs } \end{array} \right. \\
𝓛^p(→π, p) &= -→π^T \log →p
\end{aligned}$$

---
# MuZero – Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero – Atari Results

![w=100%](muzero_atari.svgz)

~~~
MuZero Reanalyze is optimized for greater sample efficiency. It revisits
past trajectories by re-running the MCTS using the network with the latest
parameters, notably
~~~
- using the fresh policy as target in 80\% of the training updates, and
~~~
- always using the fresh $v^k ← f_θ(s^k)$ in the bootstrapped target $z_t$.

~~~
Some hyperparameters were changed too – 2.0 samples were drawn per state
instead of 0.1, the value loss was weighted down to 0.25, and the $n$-step
return was reduced to $n=5$.

---
# MuZero – Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero – Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)

---
section: PlaNet
# PlaNet

In Nov 2018, an interesting paper from D. Hafner et al. proposed
a **Deep Planning Network (PlaNet)**, which is a model-based agent
that learns the MDP dynamics from pixels, and then chooses actions
using a CEM planner utilizing the learned compact latent space.

~~~
The PlaNet is evaluated on selected tasks from the DeepMind control suite
![w=100%,mh=50%,v=middle](planet_environments.svgz)

---
# PlaNet

In PlaNet, partially observable MDPs following the stochastic dynamics are
considered:
$$\begin{aligned}
\textrm{transition function:} && s_t &∼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &∼p(o_t|s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|s_t), \\
\textrm{policy:} && a_t &∼p(a_t|o_{≤t},a_{< t}).
\end{aligned}$$

~~~
The main goal is to train the first three – the transition function, the
observation function, and the reward function.

---
section: DataCollect&Plan
# PlaNet – Data Collection

![w=32%,f=left](planet_algorithm.svgz)

Because an untrained agent will most likely not cover all needed environment
states, we need to iteratively collect new experience and train the model.
The authors propose $S=5$, $C=100$, $B=50$, $L=50$, $R$ between 2 and 8.

~~~
For planning, CEM algorithm (capable of solving all tasks with a true model)
is used; $H=12$, $I=10$, $J=1000$, $K=100$.

![w=85%,mw=66%,h=center](planet_planner.svgz)

---
section: LatentModel
# PlaNet – Latent Dynamics

![w=47%,f=right](planet_rssm.svgz)

First let us consider a typical latent-space model, consisting of
$$\begin{aligned}
\textrm{transition function:} && s_t &∼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &∼p(o_t|s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|s_t).
\end{aligned}$$

~~~
The transition model is Gaussian with mean and variance predicted by a network,
the observation model is Gaussian with identity covariance and mean predicted by
a deconvolutional network, and the reward model is a scalar Gaussian with unit
variance and mean predicted by a neural network.

~~~
To train such a model, we turn to variational inference, and use an
encoder $q(s_{1:T}|o_{1:T},a_{1:T-1})=∏_{t=1}^T q(s_t|s_{t-1},a_{t-1},o_t)$,
which is a Gaussian with mean and variance predicted by a convolutional neural
network.

---
# PlaNet – Training Objective

Using the encoder, we obtain the following variational lower bound on the
log-likelihood of the observations (for rewards the bound is analogous):
$$\begin{aligned}
  &\log p(o_{1:T}|a_{1:T}) \\
  &\quad = \log ∫ ∏_t p(s_t|s_{t-1},a_{t-1}) p(o_t|s_t)\d s_{1:T} \\
  &\quad ≥ ∑_{t=1}^T \Big(
    \underbrace{𝔼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{𝔼_{q(s_{t-1}|o_{≤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{≤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

~~~
We evaluate the expectations using a single sample, and use the reparametrization
trick to allow backpropagation through the sampling.

---
# PlaNet – Training Objective Derivation

To derive the training objective, we employ importance sampling and the Jensen’s
inequality:

$$\begin{aligned}
 &\log p(o_{1:T}|a_{1:T}) \\
 &\quad= \log 𝔼_{p(s_{1:T}|a_{1:T})} ∏_{t=1}^T p(o_t|s_t) \\
 &\quad= \log 𝔼_{q(s_{1:T}|o_{1:T},a_{1:T})} ∏_{t=1}^T p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) / q(s_t|o_{≤ t},a_{< t}) \\
 &\quad≥ 𝔼_{q(s_{1:T}|o_{1:T},a_{1:T})} ∑_{t=1}^T \log p(o_t|s_t) + \log p(s_t|s_{t-1},a_{t-1}) - \log q(s_t|o_{≤ t},a_{< t}) \\
 &\quad= ∑_{t=1}^T \Big(
    \underbrace{𝔼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{𝔼_{q(s_{t-1}|o_{≤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{≤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

---
section: RSSM
# PlaNet – Recurrent State-Space Model

The purely stochastic transitions struggle to store information for multiple
timesteps. Therefore, the authors propose to include a deterministic path to the
model (providing access to all previous states), obtaining the
**recurrent state-space model (RSSM)**:
![w=55%,h=center](planet_rssm.svgz)

~~~
$$\begin{aligned}
\textrm{deterministic state model:} && h_t &= f(h_{t-1}, s_{t-1}, a_{t-1}), \\
\textrm{stochastic state function:} && s_t &∼p(s_t|h_t), \\
\textrm{observation function:} && o_t &∼p(o_t|h_t, s_t), \\
\textrm{reward function:} && r_t &∼p(r_t|h_t, s_t), \\
\textrm{encoder:} && q_t &∼q(s_t|h_t, o_t).
\end{aligned}$$

---
# PlaNet – Results

![w=100%,v=middle](planet_results.svgz)

---
# PlaNet – Ablations

![w=85%,h=center](planet_ablations1.svgz)

---
# PlaNet – Ablations

![w=85%,h=center](planet_ablations2.svgz)
