title: NPFL122, Lecture 11
class: title, langtech, cc-by-sa
# PopArt Normalization, R2D2, MuZero

## Milan Straka

### December 14, 2020

---
section: PopArt Normalization
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; â†’Î¸, Ïƒ, Î¼)$ is computed using a normalized value
predictor $n(s; â†’Î¸)$
$$v(s; â†’Î¸, Ïƒ, Î¼) â‰ Ïƒ n(s; â†’Î¸) + Î¼$$
and further assume that $n(s; â†’Î¸)$ is an output of a linear function
$$n(s; â†’Î¸) â‰ â†’Ï‰^T f(s; â†’Î¸-\{â†’Ï‰, b\}) + b.$$

~~~
We can update the $Ïƒ$ and $Î¼$ using exponentially moving average with decay rate
$Î²$ (in the paper, first moment $Î¼$ and second moment $Ï…$ is tracked, and
the standard deviation is computed as $Ïƒ=\sqrt{Ï…-Î¼^2}$; decay rate $Î²=3 â‹… 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $Î¼$ and $Ïƒ$, we can normalize the observed (unnormalized) returns as
$(G - Î¼) / Ïƒ$ and use an actor-critic algorithm with advantage $(G - Î¼)/Ïƒ - n(S; â†’Î¸)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $â†’Ï‰, b$ used to compute the
value estimate
$$v(s; â†’Î¸, Ïƒ, Î¼) â‰ Ïƒ \Big(â†’Ï‰^T f(s; â†’Î¸-\{â†’Ï‰, b\}) + b\Big) + Î¼$$
are updated under any change $Î¼ â†’ Î¼'$ and $Ïƒ â†’ Ïƒ'$ as
$$\begin{aligned}
  â†’Ï‰' &â† \frac{Ïƒ}{Ïƒ'}â†’Ï‰,\\
  b' &â† \frac{Ïƒb + Î¼ - Î¼'}{Ïƒ'}.
\end{aligned}$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $â†’Î¼$, $â†’Ïƒ$ and $â†’n(s; â†’Î¸)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.svgz)

~~~
![w=100%](popart_atari_curves.svgz)

---
# PopArt Results

![w=85%,h=center](popart_atari_statistics.svgz)

Normalization statistics on chosen environments.

---
# PopArt Results

![w=100%,v=middle](popart_dmlab_curves.svgz)

---
section: TransformedRewards
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $ğ“£$
$$(ğ“£q)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[r + Î³ \max_{a'} q(s', a')\Big].$$

~~~
Instead of reducing the magnitude of rewards, we might use a function
$h: â„ â†’ â„$ to reduce their scale. We define a transformed Bellman operator
$ğ“£_h$ as
$$(ğ“£_hq)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[h\Big(r + Î³ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = Î± z$ for $Î± > 0$, then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_* = Î± q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $Î±$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ ğ“£ q_* = h \circ ğ“£(h^{-1} \circ h \circ q_*) = ğ“£_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

The authors use the following transformation for the Atari environments
$$h(x) â‰ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + Îµx$$
with $Îµ = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4Îµ (|x| + 1 + Îµ)} - 1}{2Îµ} \right)^2 - 1\right).$$

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2_recurrent_staleness.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $Î³ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2_memory_size.svgz)

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics.

~~~
At each time-step $t$, for each of $1 â‰¤ k â‰¤ K$ steps, a model $Î¼_Î¸$, with parameters $Î¸$, conditioned on past observations $o_1, â€¦, o_t$ and future actions $a_{t+1}, â€¦, a_{t+k}$, predicts three future quantities:
- the policy $â†’p^k_t â‰ˆ Ï€(a_{t+k+1} | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k})$,
~~~
- the value function $v^k_t â‰ˆ ğ”¼\big[u_{t+k+1} + Î³ u_{t+k+2} + â€¦ | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t â‰ˆ u_{t+k}$,

where $u_i$ are the observed rewards and $Ï€$ is the behaviour policy.

---
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components,
a _representation_ function, a _dynamics_ function and a _prediction_ function.

~~~
- The dynamics function, $r^k, s^k = g_Î¸(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values and policies.

~~~
- The prediction function $â†’p^k, v^k = f_Î¸(s^k)$, computes the policy and value
  function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 = h_Î¸(o_1, â€¦, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
# MuZero

To select a move, we employ a MCTS algorithm similar to the AlphaZero. It
produces a policy $Ï€_t$ and value estimate $Î½_t$, and an action is then
sampled from the policy $a_{t+1} âˆ¼ Ï€_t$.

~~~
During training, we utilize a sequence of $k$ moves. We estimate the return
using bootstrapping $z_t = u_{t+1} + Î³ u_{t+2} + â€¦ + Î³^{n-1} u_{t+n} + Î³^n Î½_{t+n}$.
The values $k=5$ and $n=10$ are used in the paper.

~~~
The loss is then composed of the following components:
$$ğ“›_t(Î¸) = âˆ‘_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(Ï€_{t+k}, â†’p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + Îµx$ for $Îµ=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, â€¦, 300$, which they claim
to be more stable.

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left.
\begin{array}{ll}
s^0 &= h_\theta(o_1, ..., o_t) \\
r^k, s^k &= g_\theta(s^{k-1}, a^k) \\
%
\mathbf{p}^k, v^k &= f_\theta(s^k)
\end{array}
\right\} \;\;
\mathbf{p}^k, v^k, r^k = \mu_\theta(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
\nu_t, \pi_t &= MCTS(s^0_t, \mu_\theta) \\
a_t &\sim \pi_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
\mathbf{p}^k_t, v^k_t, r^k_t &= \mu_\theta(o_1, â€¦, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{
\begin{array}{lr}
u_T
& \text{ for games } \\
u_{t+1} + \gamma u_{t+2} + ... + \gamma^{n-1} u_{t+n} + \gamma^n \nu_{t+n}
& \text{ for general MDPs }
\end{array}
\right. \\
ğ“›_t(\theta) &= \sum_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(\pi_{t+k}, â†’p^k_t)  + c \|\theta\|^2 \\
&\text{Losses} \\
ğ“›^r(u, r) &= \left\{
\begin{array}{lr}
0 & \text{ for games } \\
\boldsymbol{\phi}(u)^T \log \mathbf{r} & \text{ for general MDPs }
\end{array}
\right. \\
ğ“›^v(z, q) &= \left\{
\begin{array}{lr}
(z - q)^2 & \text{ for games } \\
\boldsymbol{\phi}(z)^T \log \mathbf{q} & \text{ for general MDPs }
\end{array}
\right. \\
ğ“›^p(\pi, p) &= \boldsymbol{\pi}^T \log \mathbf{p}
\end{aligned}$$

---
# MuZero â€“ Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero â€“ Atari Results

![w=100%](muzero_atari.svgz)

---
# MuZero â€“ Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero â€“ Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)
