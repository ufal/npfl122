title: NPFL122, Lecture 11
class: title, langtech, cc-by-nc-sa
# MuZero, PlaNet

## Milan Straka

### December 12, 2022

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics. It is evaluated both on board games
and on the Atari domain.

~~~
At each time-step $t$, for each of $1 â‰¤ k â‰¤ K$ steps, a model $Î¼_Î¸$ with parameters $Î¸$, conditioned on past observations $o_1, â€¦, o_t$ and future actions $a_{t+1}, â€¦, a_{t+k}$, predicts three future quantities:
- the policy $â†’p^k_t â‰ˆ Ï€(a_{t+k+1} | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k})$,
~~~
- the value function $v^k_t â‰ˆ ğ”¼\big[u_{t+k+1} + Î³ u_{t+k+2} + â€¦ | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t â‰ˆ u_{t+k}$,

where $u_i$ are the observed rewards and $Ï€$ is the behaviour policy.

---
section: Î¼0Model
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components:
a **representation** function, a **dynamics** function, and a **prediction** function.

~~~
- The dynamics function, $(r^k, s^k) â† g_Î¸(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values, and policies.

~~~
- The prediction function, $(â†’p^k, v^k) â† f_Î¸(s^k)$, computes the policy and the
  value function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 â† h_Î¸(o_1, â€¦, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
section: Î¼0MCTS
# MuZero â€“ MCTS

The MCTS algorithm is very similar to the one used in AlphaZero, only the
trained model is used. It produces a policy $â†’Ï€_t$ and a value estimate $Î½_t$.

~~~
- All actions, including the invalid ones, are allowed at any time, except at
  the root, where the invalid actions (available from the current state) are
  disallowed.

~~~
- No states are considered terminal during the search.

~~~
- During the backup phase, we consider a general discounted bootstrapped return
  $$G_k = âˆ‘\nolimits_{t=0}^{l-k-1} Î³^t r_{k+1+t} + Î³^{l-k} v_l.$$

~~~
- Furthermore, the expected return is generally unbounded. Therefore, MuZero
  normalize the Q-value estimates to $[0, 1]$ range by using the minimum and
  maximum the values observed in the search tree until now:
  $$QÌ„(s, a) = \frac{Q(s, a) - \min_{s',a' âˆˆ \mathrm{Tree}} Q(s', a')}{\max_{s',a' âˆˆ \mathrm{Tree}} Q(s', a') - \min_{s',a' âˆˆ \mathrm{Tree}} Q(s', a')}.$$

---
# MuZero â€“ Action Selection

To select a move, we employ a MCTS algorithm and then sample
an action from the obtained policy, $a_{t+1} âˆ¼ â†’Ï€_t$.

~~~
For games, the same strategy of sampling the actions $a_t$ as in AlphaZero is used.
~~~
In the Atari domain, the actions are sampled according to the visit counts for the
whole episode, but with a given temperature $T$:
$$Ï€(a|s) = \frac{N(s, a)^{1/T}}{âˆ‘_b N(s, b)^{1/T}},$$
where $T$ is decayed during training â€“ for first 500k steps it is 1, for the
next 250k steps it is 0.5 and for the last 250k steps it is 0.25.

~~~
While for the board games 800 simulations are used during MCTS, only 50 are used
for Atari.

~~~
In case of Atari, the replay buffer consists of 125k sequences of 200 actions.

---
section: Î¼0Training
# MuZero â€“ Training

During training, we utilize a sequence of $K$ moves. We estimate the return
using bootstrapping as $z_t = u_{t+1} + Î³ u_{t+2} + â€¦ + Î³^{n-1} u_{t+n} + Î³^n Î½_{t+n}$.
The values $K=5$ and $n=10$ are used in the paper, with batch size 2048 for the
board games and 1024 for Atari.

~~~
The loss is then composed of the following components:
$$ğ“›_t(Î¸) = âˆ‘_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(â†’Ï€_{t+k}, â†’p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + Îµx$ for $Îµ=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, â€¦, 300$, which they claim
to be more stable.

~~~
Furthermore, in Atari the discount factor $Î³=0.997$ is used, and the replay buffer elements
are sampled according to prioritized replay with priority $âˆ |Î½-z|^Î±$, and importance sampling
with exponent $Î²$ is used to account for changing the sampling distribution
($Î±=Î²=1$ is used).

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left. \begin{array}{ll}
  s^0 &= h_Î¸(o_1, ..., o_t) \\
  r^k, s^k &= g_Î¸(s^{k-1}, a^k) \\
  â†’p^k, v^k &= f_Î¸(s^k)
\end{array} \right\} \;\; â†’p^k, v^k, r^k = Î¼_Î¸(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
Î½_t, â†’Ï€_t &= MCTS(s^0_t, Î¼_Î¸) \\
a_t &âˆ¼ â†’Ï€_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
â†’p^k_t, v^k_t, r^k_t &= Î¼_Î¸(o_1, â€¦, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{\begin{array}{lr}
  u_T & \text{ for games } \\
  u_{t+1} + Î³ u_{t+2} + ... + Î³^{n-1} u_{t+n} + Î³^n Î½_{t+n} & \text{ for general MDPs }
\end{array}\right. \\
ğ“›_t(Î¸) &= âˆ‘_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(â†’Ï€_{t+k}, â†’p^k_t)  + c \|Î¸\|^2 \\

&\text{Losses} \\
ğ“›^r(u, r) &= \left\{ \begin{array}{lr} 0 & \text{ for games } \\ -â†’Ï†(u)^T \log â†’Ï†(r) & \text{ for general MDPs } \end{array} \right. \\
ğ“›^v(z, q) &= \left\{ \begin{array}{lr} (z - q)^2 & \text{ for games } \\ -â†’Ï†(z)^T \log â†’Ï†(q) & \text{ for general MDPs } \end{array} \right. \\
ğ“›^p(â†’Ï€, p) &= -â†’Ï€^T \log â†’p
\end{aligned}$$

---
# MuZero â€“ Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero â€“ Atari Results

![w=100%](muzero_atari.svgz)

~~~
MuZero Reanalyze is optimized for greater sample efficiency. It revisits
past trajectories by re-running the MCTS using the network with the latest
parameters, notably
~~~
- using the fresh policy as target in 80\% of the training updates, and
~~~
- always using the fresh $v^k â† f_Î¸(s^k)$ in the bootstrapped target $z_t$.

~~~
Some hyperparameters were changed too â€“ 2.0 samples were drawn per state
instead of 0.1, the value loss was weighted down to 0.25, and the $n$-step
return was reduced to $n=5$.

---
# MuZero â€“ Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero â€“ Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)

---
section: PlaNet
# PlaNet

In Nov 2018, an interesting paper from D. Hafner et al. proposed
a **Deep Planning Network (PlaNet)**, which is a model-based agent
that learns the MDP dynamics from pixels, and then chooses actions
using a CEM planner utilizing the learned compact latent space.

~~~
The PlaNet is evaluated on selected tasks from the DeepMind control suite
![w=100%,mh=50%,v=middle](planet_environments.svgz)

---
# PlaNet

In PlaNet, partially observable MDPs following the stochastic dynamics are
considered:
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t), \\
\textrm{policy:} && a_t &âˆ¼p(a_t|o_{â‰¤t},a_{< t}).
\end{aligned}$$

~~~
The main goal is to train the first three â€“ the transition function, the
observation function, and the reward function.

---
section: DataCollect&Plan
# PlaNet â€“ Data Collection

![w=32%,f=left](planet_algorithm.svgz)

Because an untrained agent will most likely not cover all needed environment
states, we need to iteratively collect new experience and train the model.
The authors propose $S=5$, $C=100$, $B=50$, $L=50$, $R$ between 2 and 8.

~~~
For planning, CEM algorithm (capable of solving all tasks with a true model)
is used; $H=12$, $I=10$, $J=1000$, $K=100$.

![w=85%,mw=66%,h=center](planet_planner.svgz)

---
section: LatentModel
# PlaNet â€“ Latent Dynamics

![w=47%,f=right](planet_rssm.svgz)

First let us consider a typical latent-space model, consisting of
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t).
\end{aligned}$$

~~~
The transition model is Gaussian with mean and variance predicted by a network,
the observation model is Gaussian with identity covariance and mean predicted by
a deconvolutional network, and the reward model is a scalar Gaussian with unit
variance and mean predicted by a neural network.

~~~
To train such a model, we turn to variational inference, and use an
encoder $q(s_{1:T}|o_{1:T},a_{1:T-1})=âˆ_{t=1}^T q(s_t|s_{t-1},a_{t-1},o_t)$,
which is a Gaussian with mean and variance predicted by a convolutional neural
network.

---
# PlaNet â€“ Training Objective

Using the encoder, we obtain the following variational lower bound on the
log-likelihood of the observations (for rewards the bound is analogous):
$$\begin{aligned}
  &\log p(o_{1:T}|a_{1:T}) \\
  &\quad = \log âˆ« âˆ_t p(s_t|s_{t-1},a_{t-1}) p(o_t|s_t)\d s_{1:T} \\
  &\quad â‰¥ âˆ‘_{t=1}^T \Big(
    \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

~~~
We evaluate the expectations using a single sample, and use the reparametrization
trick to allow backpropagation through the sampling.

---
# PlaNet â€“ Training Objective Derivation

To derive the training objective, we employ importance sampling and the Jensenâ€™s
inequality:

$$\begin{aligned}
 &\log p(o_{1:T}|a_{1:T}) \\
 &\quad= \log ğ”¼_{p(s_{1:T}|a_{1:T})} âˆ_{t=1}^T p(o_t|s_t) \\
 &\quad= \log ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ_{t=1}^T p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) / q(s_t|o_{â‰¤ t},a_{< t}) \\
 &\quadâ‰¥ ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ‘_{t=1}^T \log p(o_t|s_t) + \log p(s_t|s_{t-1},a_{t-1}) - \log q(s_t|o_{â‰¤ t},a_{< t}) \\
 &\quad= âˆ‘_{t=1}^T \Big(
    \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

---
section: RSSM
# PlaNet â€“ Recurrent State-Space Model

The purely stochastic transitions struggle to store information for multiple
timesteps. Therefore, the authors propose to include a deterministic path to the
model (providing access to all previous states), obtaining the
**recurrent state-space model (RSSM)**:
![w=55%,h=center](planet_rssm.svgz)

~~~
$$\begin{aligned}
\textrm{deterministic state model:} && h_t &= f(h_{t-1}, s_{t-1}, a_{t-1}), \\
\textrm{stochastic state function:} && s_t &âˆ¼p(s_t|h_t), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|h_t, s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|h_t, s_t), \\
\textrm{encoder:} && q_t &âˆ¼q(s_t|h_t, o_t).
\end{aligned}$$

---
# PlaNet â€“ Results

![w=100%,v=middle](planet_results.svgz)

---
# PlaNet â€“ Ablations

![w=85%,h=center](planet_ablations1.svgz)

---
# PlaNet â€“ Ablations

![w=85%,h=center](planet_ablations2.svgz)
