title: NPFL122, Lecture 5
class: title, langtech, cc-by-nc-sa
# Function Approximation,<br>Eligibility Traces

## Milan Straka

### November 12, 2018

---
section: Refresh
# $n$-step Methods

![w=40%,f=right](../04/nstep_td.pdf)

Full return is
$$G_t = âˆ‘_{k=t}^âˆ R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + Î³ V_t(S_{t+1}).$$

We can generalize both into $n$-step returns:
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n V_{t+n-1}(S_{t+n}).$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$.

---
# $n$-step Sarsa

![w=30%,f=right](../04/nstep_td.pdf)

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n Q_{t+n-1}(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$, we get the following straightforward
update rule:
$$Q_{t+n}(S_t, A_t) â‰ Q_{t+n-1}(S_t, A_t) + Î±\left[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\right].$$

![w=55%,h=center](../04/nstep_sarsa_example.pdf)

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](../04/tree_backup_example.pdf)

We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} â‰ R_{t+1} + âˆ‘\nolimits_a Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q_{t}(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$

---
# Function Approximation

We will approximate value function $v$ and/or state-value function $q$, choosing
from a family of functions parametrized by a weight vector $â†’wâˆˆâ„^d$.

We denote the approximations as
$$\begin{gathered}
  \hat v(s, â†’w),\\
  \hat q(s, a, â†’w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(â†’w) â‰ âˆ‘_{sâˆˆğ“¢} Î¼(s) \left[v_Ï€(s) - \hat v(s, â†’w)\right]^2,$$
where the state distribution $Î¼(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $â†’w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \frac{1}{2} Î± âˆ‡ \left[v_Ï€(S_t) - \hat v(S_t, â†’w_t)\right]^2\\
           &â† â†’w_t - Î±\left[v_Ï€(S_t) - \hat v(S_t, â†’w_t)\right] âˆ‡ \hat v(S_t, â†’w_t).\\
\end{aligned}$$

As usual, the $v_Ï€(S_t)$ is estimated by a suitable sample. For example in Monte
Carlo methods, we use episodic return $G_t$, and in temporal difference methods,
we employ bootstrapping and use $R_{t+1} + Î³\hat v(S_{t+1}, â†’w).$

---
# Linear Methods

A simple special case of function approximation are linear methods, where
$$\hat v(â†’x(s), â†’w) â‰ â†’x(s)^T â†’w = âˆ‘x(s)_i w_i.$$

The $â†’x(s)$ is a representation of state $s$, which is a vector of the same size
as $â†’w$. It is sometimes called a _feature vector_.

The SGD update rule then becomes
$$â†’w_{t+1} â† â†’w_t - Î±\left[v_Ï€(S_t) - \hat v(â†’x(S_t), â†’w_t)\right] â†’x(S_t).$$

---
# Feature Construction for Linear Methods

Many methods developed in the past:

- state aggregation,

- polynomials

- Fourier basis

- tile coding

- radial basis functions

But of course, nowadays we use deep neural networks which construct a suitable
feature vector automatically as a latent variable (the last hidden layer).

---
section: Tile Coding
# Tile Coding

![w=100%,mh=90%,v=middle](../04/tile_coding.pdf)

If $t$ overlapping tiles are used, the learning rate is usually normalized as $Î±/t$.

---
# Tile Coding

For example, on the 1000-state random walk example, the performance of tile
coding surpasses state aggregation:

![w=60%,h=center](../04/tile_coding_performance.pdf)

---
# Asymmetrical Tile Coding

In higher dimensions, the tiles should have asymmetrical offsets, with
a sequence of $(1, 3, 5, â€¦, 2d-1)$ being a good choice.

![w=50%,h=center](../04/tile_coding_asymmetrical.pdf)

---
section: Semi-Gradient TD
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again use bootstrapping to estimate
$v_Ï€(S_t)$ as $R_{t+1} + Î³\hat v(S_{t+1}, â†’w).$

~~~
![w=70%,h=center](grad_td_estimation.pdf)

~~~
Note that such algorithm is called _semi-gradient_, because it does not
backpropagate through $\hat v(S', â†’w)$.

---
# Temporal Difference Semi-Gradient Policy Evaluation

An important fact is that linear semi-gradient TD methods do not converge to
$\overline{VE}$. Instead, they converge to a different _TD fixed point_
$â†’w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(â†’w_\mathrm{TD}) â‰¤ \frac{1}{1-Î³} \min_â†’w \overline{VE}(â†’w).$$

~~~
However, when $Î³$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=60%,h=center](grad_td_nstep_estimation.pdf)

---
# Temporal Difference Semi-Gradient Policy Evaluation

![w=100%,v=middle](grad_td_estimation_example.pdf)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.pdf)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.pdf)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.pdf)

The performances are for semi-gradient Sarsa($Î»$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.pdf)
![w=50%,h=center](mountain_car_performance_nstep.pdf)
